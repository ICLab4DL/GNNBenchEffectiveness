{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import index\n",
    "import torch\n",
    "from torch_sparse import SparseTensor\n",
    "from torch import tensor\n",
    "\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "\n",
    "\n",
    "row = torch.tensor([0, 1, 2, 2, 2, 3, 3, 4])\n",
    "col = torch.tensor([2, 2, 0, 1, 3, 2, 4, 3])\n",
    "c = SparseTensor(row=row, col=col)\n",
    "\n",
    "print(c)\n",
    "\n",
    "adjk = matmul(c, c)\n",
    "print(adjk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aa[0].unsqueeze(-1).dim())\n",
    "\n",
    "print(aa.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x338.2 with 13 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch_sparse import SparseTensor, matmul\n",
    "\n",
    "# plot results \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap\n",
    "import networkx as nx\n",
    "\n",
    "# here..\n",
    "cmaps = {}\n",
    "\n",
    "gradient = np.linspace(0, 1, 256)\n",
    "gradient = np.vstack((gradient, gradient))\n",
    "\n",
    "x_y_label_font = 20\n",
    "x_y_legend_font = 20\n",
    "\n",
    "plt.rc('font', family='Times New Roman')\n",
    "fig_dpi = 220\n",
    "fig_shape_squre = (6, 5)\n",
    "\n",
    "def plot_color_gradients(category, cmap_list):\n",
    "    # Create figure and adjust figure height to number of colormaps\n",
    "    nrows = len(cmap_list)\n",
    "    figh = 0.35 + 0.15 + (nrows + (nrows - 1) * 0.1) * 0.22\n",
    "    fig, axs = plt.subplots(nrows=nrows + 1, figsize=(6.4, figh), dpi=100)\n",
    "    fig.subplots_adjust(top=1 - 0.35 / figh, bottom=0.15 / figh,\n",
    "                        left=0.2, right=0.99)\n",
    "    axs[0].set_title(f'{category} colormaps', fontsize=14)\n",
    "\n",
    "    for ax, name in zip(axs, cmap_list):\n",
    "        ax.imshow(gradient, aspect='auto', cmap=plt.get_cmap(name))\n",
    "        ax.text(-0.01, 0.5, name, va='center', ha='right', fontsize=10,\n",
    "                transform=ax.transAxes)\n",
    "\n",
    "    # Turn off *all* ticks & spines, not just the ones with colormaps.\n",
    "    for ax in axs:\n",
    "        ax.set_axis_off()\n",
    "\n",
    "    # Save colormap list for later.\n",
    "    cmaps[category] = cmap_list\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "class MyColor(object):\n",
    "    def __init__(self, cmap_name, skip_idx=5, backup_name='Set1', \n",
    "                 backup_color=3, add_red=False, pre_defined=False):\n",
    "        if pre_defined:\n",
    "            # colors = ['#3682be','#45a776','#f05326','#eed777','#334f65','#b3974e','#38cb7d','#ddae33','#844bb3','#93c555','#5f6694','#df3881']\n",
    "            colors = '00a8e1 - 99cc00 - e30039 - 800080 - 00994e - ff6600 - 808000 - db00c2 - 008080 - 0000ff - c8cc00'\n",
    "            colors = [\"#\"+c.strip() for c in colors.split('-')]\n",
    "            print(colors)\n",
    "            cmap = ListedColormap(colors, name = 'mycmap')\n",
    "            self.color_set = list(cmap.colors)\n",
    "        else:\n",
    "            if isinstance(cmap_name, list):\n",
    "                #NOTE: combine all cmaps:\n",
    "                self.color_set = []\n",
    "                for cname in cmap_name:\n",
    "                    self.color_set.extend(list(plt.get_cmap(cname).colors))\n",
    "                print('color_set: ', self.color_set[15])\n",
    "            else:\n",
    "                self.color_set = list(plt.get_cmap(cmap_name).colors)\n",
    "                \n",
    "        # NOTE: always ignore light yellow in Set1\n",
    "\n",
    "        if add_red:\n",
    "            self.light_set = list(plt.get_cmap('Set1').colors)\n",
    "            self.color_set  = [v for i, v in enumerate(list(plt.get_cmap(cmap_name).colors)[:-1]) if i!=5]\n",
    "            self.color_set.extend([self.light_set[0]])\n",
    "            self.color_set.extend([self.light_set[4]])\n",
    "\n",
    "        self.backup_set = plt.get_cmap(backup_name).colors\n",
    "        self.backup_color = backup_color\n",
    "        self.skip_idx=skip_idx\n",
    "        self.idx = 0\n",
    "        self.color_len = len(self.color_set)\n",
    "        \n",
    "    def get_color(self):\n",
    "        if self.idx == self.color_len - 1:\n",
    "            self.idx = 0\n",
    "        if self.idx == self.skip_idx:\n",
    "            self.idx += 1\n",
    "            return self.backup_set[self.backup_color]\n",
    "        color = self.color_set[self.idx]\n",
    "        self.idx += 1\n",
    "        return color\n",
    "    \n",
    "    def shuffle(self):\n",
    "        np.random.shuffle(self.color_set)\n",
    "    \n",
    "\n",
    "plot_color_gradients('Qualitative',\n",
    "                     ['Pastel1', 'Pastel2', 'Paired', 'Accent', 'Dark2',\n",
    "                      'Set1', 'Set2', 'Set3', 'tab10', 'tab20', 'tab20b',\n",
    "                      'tab20c'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_dir:  DATA/syn_degree/processed\n",
      "load dataset !\n",
      "SynDataset load data_path: DATA/syn_degree_0.9_class10.pkl\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'DATA/syn_degree_0.9_class10.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-99ca6f93507e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPrepareDatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDATASETS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m \u001b[0mdat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDATASETS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'syn_degree'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lizhengdao/github/GenerativeGNN/datasets/manager.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, kfold_class, outer_k, inner_k, seed, holdout_test_size, use_node_degree, use_node_attrs, use_one, use_shared, use_1hot, use_random_normal, use_pagerank, use_eigen, use_eigen_norm, use_deepwalk, precompute_kron_indices, additional_features, additional_graph_features, max_reductions, DATA_DIR, config)\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0mcorr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dataset_para'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSynDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'{self.name}_{corr}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'DATA'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dim_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_tasks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;31m# NOTE: fill __data_list__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lizhengdao/github/GenerativeGNN/datasets/manager.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, name, root, transform, pre_transform)\u001b[0m\n\u001b[1;32m   1393\u001b[0m             \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{name}.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SynDataset load data_path:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1395\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1396\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'DATA/syn_degree_0.9_class10.pkl'"
     ]
    }
   ],
   "source": [
    "import pickle as pk\n",
    "\n",
    "\n",
    "\n",
    "class StandardScaler():\n",
    "\n",
    "    def __init__(self, mean, std, fill_zeroes=False):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.fill_zeroes = fill_zeroes\n",
    "\n",
    "    def transform(self, data):\n",
    "        if self.fill_zeroes:\n",
    "            mask = (data == 0)\n",
    "            data[mask] = self.mean\n",
    "            \n",
    "        if isinstance(data, list):\n",
    "            if self.std == 0:\n",
    "                return [0.01 for d in data]\n",
    "            \n",
    "            return [(d - self.mean)/self.std for d in data]\n",
    "        \n",
    "        if self.std == 0:\n",
    "            return np.zeros_like(data) + 0.01\n",
    "        \n",
    "        return (data - self.mean) / self.std\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return (data * self.std) + self.mean\n",
    "\n",
    "\n",
    "def normalize(data, along_axis=None, ignore_norm=[], same_data_shape=True):\n",
    "    '''\n",
    "        only norm numpy type data with last dimension.\n",
    "    '''\n",
    "    if isinstance(data, list):\n",
    "        if same_data_shape:\n",
    "            # hear also along each axis:\n",
    "            cur_data = np.array(data).reshape(-1, 1)\n",
    "            return normalize(cur_data, along_axis=along_axis, ignore_norm=ignore_norm)\n",
    "        else:\n",
    "        # NOTE: data shape: [(N, C), (N1, C),...], so cannot concatenate\n",
    "            normed_res = []\n",
    "            for each in data:\n",
    "                each_norm = normalize(each, along_axis=along_axis, ignore_norm=ignore_norm)\n",
    "                normed_res.append(each_norm)\n",
    "            return normed_res\n",
    "        \n",
    "    if not isinstance(data, np.ndarray):\n",
    "        data = data.cpu().numpy()\n",
    "        \n",
    "    if along_axis is not None:\n",
    "        if along_axis == -1:\n",
    "            # along all axis separately. data shape:(NxC) along each C_i\n",
    "            for ax in range(data.shape[-1]):\n",
    "                if ax in ignore_norm or (ax - data.shape[-1]) in ignore_norm:\n",
    "                    continue\n",
    "                print('data shape: ', data.shape)\n",
    "                mean = np.mean(data[:, ax])\n",
    "                std = np.std(data[:, ax])\n",
    "                scaler = StandardScaler(mean=mean, std=std)\n",
    "                data[:, ax] = scaler.transform(data[:, ax])\n",
    "            return data\n",
    "        else:\n",
    "            print('norm along ', along_axis)\n",
    "            pass\n",
    "            \n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    scaler = StandardScaler(mean=mean, std=std)\n",
    "    normed_data = scaler.transform(data)\n",
    "                \n",
    "    return normed_data, scaler\n",
    "\n",
    "\n",
    "# Load all datasets:\n",
    "\n",
    "import sys,os\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "\n",
    "import my_utils\n",
    "\n",
    "from PrepareDatasets import DATASETS\n",
    "\n",
    "with open(f'DATA/syn_degree_0.9_class10.pkl')\n",
    "data = pk.load()\n",
    "    \n",
    "\n",
    "graph_path ='./DATA/syn_degree/processed/graphwise_syn_degree_add_avg_degree.pkl'\n",
    "# load as pickle:\n",
    "with open(graph_path, 'rb') as f:\n",
    "    a = pk.load(f)\n",
    "    plt.figure()\n",
    "    graph_features = normalize(a, along_axis=-1)\n",
    "    aa = [i.item() for i in graph_features]\n",
    "    \n",
    "    aa = sorted(aa)\n",
    "    plt.plot(aa)\n",
    "    # not normalized.\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
