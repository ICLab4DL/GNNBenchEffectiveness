{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "# here..\n",
    "cmaps = {}\n",
    "\n",
    "gradient = np.linspace(0, 1, 256)\n",
    "gradient = np.vstack((gradient, gradient))\n",
    "\n",
    "\n",
    "x_y_label_font = 20\n",
    "x_y_legend_font = 20\n",
    "\n",
    "plt.rc('font', family='Times New Roman')\n",
    "fig_dpi = 220\n",
    "fig_shape_squre = (6, 5)\n",
    "\n",
    "def plot_color_gradients(category, cmap_list):\n",
    "    # Create figure and adjust figure height to number of colormaps\n",
    "    nrows = len(cmap_list)\n",
    "    figh = 0.35 + 0.15 + (nrows + (nrows - 1) * 0.1) * 0.22\n",
    "    fig, axs = plt.subplots(nrows=nrows + 1, figsize=(6.4, figh), dpi=100)\n",
    "    fig.subplots_adjust(top=1 - 0.35 / figh, bottom=0.15 / figh,\n",
    "                        left=0.2, right=0.99)\n",
    "    axs[0].set_title(f'{category} colormaps', fontsize=14)\n",
    "\n",
    "    for ax, name in zip(axs, cmap_list):\n",
    "        ax.imshow(gradient, aspect='auto', cmap=plt.get_cmap(name))\n",
    "        ax.text(-0.01, 0.5, name, va='center', ha='right', fontsize=10,\n",
    "                transform=ax.transAxes)\n",
    "\n",
    "    # Turn off *all* ticks & spines, not just the ones with colormaps.\n",
    "    for ax in axs:\n",
    "        ax.set_axis_off()\n",
    "\n",
    "    # Save colormap list for later.\n",
    "    cmaps[category] = cmap_list\n",
    "    plt.show()\n",
    "\n",
    "class MyColor(object):\n",
    "    def __init__(self, cmap_name, skip_idx=5, backup_name='Set1', backup_color=3):\n",
    "        self.color_set  = plt.get_cmap(cmap_name).colors\n",
    "        self.backup_set = plt.get_cmap(backup_name).colors\n",
    "        self.backup_color = backup_color\n",
    "        self.skip_idx=skip_idx\n",
    "        self.idx = 0\n",
    "        self.color_len = len(self.color_set)\n",
    "        \n",
    "    def get_color(self):\n",
    "        if self.idx == self.color_len - 1:\n",
    "            self.idx = 0\n",
    "        if self.idx == self.skip_idx:\n",
    "            self.idx += 1\n",
    "            return self.backup_set[self.backup_color]\n",
    "        color = self.color_set[self.idx]\n",
    "        self.idx += 1\n",
    "        return color\n",
    "    \n",
    "def lighten_color(color, amount=0.3):\n",
    "    \"\"\"\n",
    "    Lightens the given color by multiplying (1-luminosity) by the given amount.\n",
    "    Input can be matplotlib color string, hex string, or RGB tuple.\n",
    "\n",
    "    Examples:\n",
    "    >> lighten_color('g', 0.3)\n",
    "    >> lighten_color('#F034A3', 0.6)\n",
    "    >> lighten_color((.3,.55,.1), 0.5)\n",
    "    \"\"\"\n",
    "    import matplotlib.colors as mc\n",
    "    import colorsys\n",
    "    try:\n",
    "        c = mc.cnames[color]\n",
    "    except:\n",
    "        c = color\n",
    "    c = colorsys.rgb_to_hls(*mc.to_rgb(c))\n",
    "    return colorsys.hls_to_rgb(c[0], 1 - amount * (1 - c[1]), c[2])\n",
    "\n",
    "\n",
    "plot_color_gradients('Qualitative',\n",
    "                     ['Pastel1', 'Pastel2', 'Paired', 'Accent', 'Dark2',\n",
    "                      'Set1', 'Set2', 'Set3', 'tab10', 'tab20', 'tab20b',\n",
    "                      'tab20c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load from file\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "dataset_name = 'MolecularFingerprint_REDDIT-BINARY_assessment'\n",
    "\n",
    "def load_logs(date=None, name=None) -> dict:\n",
    "    \n",
    "    def extract_data(data_path) -> list:\n",
    "        with open(data_path, 'r') as f:\n",
    "            data = []\n",
    "            for line in f.readlines():\n",
    "                if not line.startswith('Epoch'):\n",
    "                    continue\n",
    "                \n",
    "                line = line.strip().replace(',', '')\n",
    "                l = line.split(' ')\n",
    "                s_l = [l[4], l[7], l[10], l[13], l[16], l[19], l[-1]]\n",
    "                f_l = []\n",
    "                for i in s_l:\n",
    "                    try:\n",
    "                        fi = float(i)\n",
    "                    except Exception as e:\n",
    "                        fi = 0.0\n",
    "                    f_l.append(fi)\n",
    "                        \n",
    "                data.append(f_l)\n",
    "            return np.array(data)\n",
    "    \n",
    "    if date is not None:\n",
    "        root_dir = f'results/result_{date}/'\n",
    "        dirs = os.listdir(f'results/result_{date}/')\n",
    "        print(dirs)\n",
    "        dataset_log = {}\n",
    "        for d in dirs:\n",
    "            dd = os.path.join(root_dir, d, '10_NESTED_CV')\n",
    "            outer_dirs = os.listdir(dd)\n",
    "            log_folds = []\n",
    "            for i, id_name in enumerate(outer_dirs):\n",
    "                if not os.path.isdir(os.path.join(dd, id_name)):\n",
    "                    continue\n",
    "                \"\"\"\n",
    "                Epoch: 1, TR loss: 433.2928585476345 TR acc: 48.641975308641975, VL loss: 479.35484415690104 VL acc: 47.77777854071723 TE loss: 101.24097244262695 TE acc: 39.0\n",
    "                Epoch: 10, TR loss: 9.24662885548156 TR acc: 65.4320987654321, VL loss: 324.07152099609374 VL acc: 70.00000135633681 TE loss: 13.453640460968018 TE acc: 62.0\n",
    "                Epoch: 20, TR loss: 11.454432420377378 TR acc: 59.75308641975309, VL loss: 148.55850830078126 VL acc: 54.444445376926 TE loss: 30.56077377319336 TE acc: 53.0\n",
    "                Epoch: 30, TR l\n",
    "                \"\"\"\n",
    "                file_dir = os.path.join(dd, id_name, 'experiment.log')\n",
    "                data = extract_data(file_dir)\n",
    "                log_folds.append(data)\n",
    "            dataset_log[d] = log_folds\n",
    "        return dataset_log\n",
    "    \n",
    "    elif name is not None:\n",
    "        # TODO: search the best config:\n",
    "        root_path = f\"results/{name}/\"\n",
    "        dirs = os.listdir(root_path)\n",
    "        \n",
    "        folds_data = {} # TODO store data.\n",
    "        for d in dirs:\n",
    "            # search 10 folds:\n",
    "            \n",
    "            folds = []\n",
    "            for i in range(1, 11):\n",
    "                fold_dir = os.path.join(root_path, d, f'10_NESTED_CV/OUTER_FOLD_{i}/HOLDOUT_MS/')\n",
    "                configs = os.listdir(fold_dir)\n",
    "                # print winner config. TODO.\n",
    "                best_vl_acc = 0.0\n",
    "                best_conf = None\n",
    "                if len(configs) < 3:\n",
    "                    # only one config.\n",
    "                    best_conf = 'config_1'\n",
    "                else:\n",
    "                    for cf in configs:\n",
    "                        if not os.path.isdir(os.path.join(fold_dir,cf)):\n",
    "                            continue\n",
    "                        config_res = os.path.join(fold_dir, cf, 'experiment.log')\n",
    "                        with open(config_res,'r') as f:\n",
    "                            for l in f.readlines():\n",
    "                                if 'best' in l:\n",
    "                                    vlacc = float(l.split(' ')[9].split(',')[0])\n",
    "                                    if vlacc > best_vl_acc:\n",
    "                                        best_conf = cf\n",
    "                                        best_vl_acc = vlacc\n",
    "                                    \n",
    "                # print('best_conf:', best_conf, 'acc:', best_vl_acc)\n",
    "                # TODO: load the best again.\n",
    "                best_conf_path = os.path.join(fold_dir, best_conf, 'experiment.log')\n",
    "                best_data = extract_data(best_conf_path)\n",
    "                folds.append(best_data)\n",
    "            folds_data[d] = folds\n",
    "            \n",
    "        return folds_data\n",
    "                \n",
    "                \n",
    "        # with open()\n",
    "\"\"\"\n",
    "Epoch: 750, TR loss: 1.7823815012174378 TR acc: 13.374485612406161, VL loss: 1.8264499328754566 VL acc: 16.666666949236834 TE loss: None TE acc: None\n",
    "Stopping at epoch 751, best is (1.9384536331082567, 17.489711965553063, 1.841690769901982, 22.22222222222222, None, None, 251)\n",
    "TR Accuracy: 17.489711965553063 VL Accuracy: 22.22222222222222\n",
    "\"\"\"\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot best:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot\n",
    "\n",
    "# TODO: def histogram\n",
    "def plot_loss_curve(data_states:list=None, titles:list=None):\n",
    "\n",
    "    total = len(data_states)\n",
    "    \n",
    "    if total == 1:\n",
    "        folds = data_states[0]\n",
    "        name = titles[0]\n",
    "        \n",
    "        fig, axes = plt.subplots(4, 3, figsize=(10, 8), dpi=300)\n",
    "        for idx, fold in enumerate(folds):\n",
    "            axe = axes[int(idx/3)][idx%3]\n",
    "            \n",
    "            axe.plot(fold[:, 0]) # train loss\n",
    "            axe.plot(fold[:, 2]) # val loss\n",
    "            axe.set_ylim(0, 5)\n",
    "            # TODO: acc, right axis\n",
    "            \n",
    "            axe.set_title(f'fold: {idx}')\n",
    "        if len(folds)%3 > 0:\n",
    "            for d in range(3-total%3):\n",
    "                fig.delaxes(axes[-1][-d-1])\n",
    "        fig.suptitle(','.join(name.split('_')[:2]) +\",\" + name.split('_')[-1])\n",
    "        fig.legend(['train loss', 'val_loss'], loc='lower right')\n",
    "        # fig.title(','.join(name.split('_')[:2]))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "# plot gradient L2 norm:\n",
    "\n",
    "def plot_gradientnorm_curve(data_states:list=None, figure_title=None, titles:list=None, rolling=False):\n",
    "\n",
    "    total = len(data_states)\n",
    "    \n",
    "    if total == 1:\n",
    "        folds = data_states[0]\n",
    "        name = titles[0]\n",
    "        \n",
    "        fig, axes = plt.subplots(4, 3, figsize=(10, 12), dpi=300)\n",
    "        # plt.yscale(\"log\")  \n",
    "        for idx, fold in enumerate(folds):\n",
    "            axe = axes[int(idx/3)][idx%3]\n",
    "            axe.set_yscale('log', subs=[0, 1, 2, 3, 4, 5, 6, 7, 8])\n",
    "            if rolling:\n",
    "                df = pd.DataFrame(fold[:, -1])\n",
    "                axe.plot(df[0], 'lightblue', df[0].rolling(10).mean(), 'b') # gradient norm\n",
    "            else:\n",
    "                axe.plot(fold[:, -1]) # gradient norm\n",
    "            # TODO: acc, right axis\n",
    "            \n",
    "            axe.set_title(f'fold: {idx}')\n",
    "        if len(folds)%3 > 0:\n",
    "            for d in range(3-total%3):\n",
    "                fig.delaxes(axes[-1][-d-1])\n",
    "        fig.suptitle(','.join(name.split('_')[:2]) + \",\" +name.split('_')[-1])\n",
    "        fig.legend(['train loss', 'val_loss'], loc='lower right')\n",
    "        # fig.title(','.join(name.split('_')[:2]))\n",
    "    else: # multi datasets:\n",
    "        fig, axes = plt.subplots(4, 3, figsize=(10, 12), dpi=300)\n",
    "        last_leg = []\n",
    "        color = MyColor(cmap_name='Set1')\n",
    "        for i, folds in enumerate(data_states):\n",
    "            # plt.yscale(\"log\")  \n",
    "            cc = color.get_color()\n",
    "            for idx, fold in enumerate(folds):\n",
    "                axe = axes[int(idx/3)][idx%3]\n",
    "                axe.set_yscale('log', subs=[0, 1, 2, 3, 4, 5, 6, 7, 8])\n",
    "                if rolling:\n",
    "                    df = pd.DataFrame(fold[:, -1])\n",
    "                    # axe.plot(df[0], color=lighten_color(cc), alpha=0.4)\n",
    "                    axe.plot(df[0].rolling(12).mean(), color=cc, label=titles[i]) # gradient norm\n",
    "                else:\n",
    "                    axe.plot(fold[:, -1], label=titles[i])  # gradient norm\n",
    "                # TODO: acc, right axis\n",
    "                axe.set_title(f'fold: {idx}')\n",
    "                \n",
    "        if len(folds)%3 > 0:\n",
    "            for d in range(3-total%3):\n",
    "                fig.delaxes(axes[-1][-d-1])\n",
    "        handlers, labels = axes[0][0].get_legend_handles_labels()\n",
    "        fig.legend(handlers, labels, loc='lower right')\n",
    "        fig.suptitle(figure_title)\n",
    "        \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = load_logs(name='result_GIN_0202') # random\n",
    "\n",
    "# random, overfitting!!\n",
    "for k, v in stats.items():\n",
    "    loss_log, titles = [], []\n",
    "    loss_log.append(v)\n",
    "    titles.append(k + \"_Random\")\n",
    "    plot_loss_curve(loss_log, titles)\n",
    "    plot_gradientnorm_curve(loss_log, titles)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = load_logs(name='result_GIN_0202_attr')\n",
    "\n",
    "# attr, SOTA!!\n",
    "for k, v in stats.items():\n",
    "    loss_log, titles = [], []\n",
    "    loss_log.append(v)\n",
    "    titles.append(k)\n",
    "    plot_loss_curve(loss_log, titles)\n",
    "    plot_gradientnorm_curve(loss_log, titles)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = load_logs(name='result_GIN_0202_eigen')\n",
    "# stats = load_logs(name='result_GIN_0202_pagerank')\n",
    "# TODO: plot all config_17 or the best config?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in stats.items():\n",
    "    loss_log, titles = [], []\n",
    "    loss_log.append(v)\n",
    "    titles.append(k+\"_Eigenvector\")\n",
    "    plot_loss_curve(loss_log, titles)\n",
    "    plot_gradientnorm_curve(loss_log, titles)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = load_logs(name='result_GIN_0202_pagerank')\n",
    "for k, v in stats.items():\n",
    "    loss_log, titles = [], []\n",
    "    loss_log.append(v)\n",
    "    titles.append(k+\"_Pagerank\")\n",
    "    plot_loss_curve(loss_log, titles)\n",
    "    plot_gradientnorm_curve(loss_log, titles)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = load_logs(name='result_GIN_0202_degree')\n",
    "for k, v in stats.items():\n",
    "    loss_log, titles = [], []\n",
    "    loss_log.append(v)\n",
    "    titles.append(k+\"_Degree\")\n",
    "    plot_loss_curve(loss_log, titles)\n",
    "    plot_gradientnorm_curve(loss_log, titles, rolling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = load_logs(name='result_GIN_0202_degree_large_batchsize')\n",
    "for k, v in stats.items():\n",
    "    loss_log, titles = [], []\n",
    "    loss_log.append(v)\n",
    "    titles.append(k+\"_Degree_largeBatch\")\n",
    "    plot_loss_curve(loss_log, titles)\n",
    "    plot_gradientnorm_curve(loss_log, titles, rolling=True)\n",
    "    \n",
    "    # No difference !!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: using different node initializations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_1023 = load_logs(date=1023)\n",
    "\n",
    "for k, v in stats_1023.items():\n",
    "    print('dataset name: ', k, 'fold len:', len(v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in stats_1023.items():\n",
    "    loss_log, titles = [], []\n",
    "    loss_log.append(v)\n",
    "    titles.append(k)\n",
    "    plot_loss_curve(loss_log, titles)\n",
    "    \n",
    "# has N fold.\n",
    "# TODO: plot N fold first, then we may pick one best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_1023 = load_logs(date=1023)\n",
    "\n",
    "for k, v in stats_1023.items():\n",
    "    print('dataset name: ', k, 'fold len:', len(v))\n",
    "    \n",
    "\n",
    "loss_log, titles = [], []\n",
    "for k, v in stats_1023.items():\n",
    "    loss_log.append(v)\n",
    "    titles.append(k)\n",
    "    plot_loss_curve(loss_log, titles)\n",
    "    break\n",
    "    \n",
    "# has N fold.\n",
    "# TODO: plot N fold first, then we may pick one best.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Plot all gradient norm curves into one figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_states = load_logs(name='result_GIN_0202_degree') # random\n",
    "random_states = load_logs(name='result_GIN_0202') # random\n",
    "eigen_states = load_logs(name='result_GIN_0202_eigen') # random\n",
    "pagerank_states = load_logs(name='result_GIN_0202_pagerank') # random\n",
    "attr_states = load_logs(name='result_GIN_0202_attr') # random\n",
    "\n",
    "def extend_value(stat:dict):\n",
    "    return [v for v in stat.values()]\n",
    "\n",
    "all_stats = [degree_states, random_states, eigen_states, pagerank_states, attr_states]\n",
    "loss_logs = []\n",
    "[loss_logs.extend(extend_value(s)) for s in all_stats]\n",
    "\n",
    "titles = ['degree', 'random', 'eigen', 'pagerank', 'attr']\n",
    "\n",
    "plot_gradientnorm_curve(loss_logs,'GIN,mutag' ,titles, rolling=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: pick the best config from each fold?\n",
    "# what is the majority one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
