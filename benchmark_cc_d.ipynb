{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark on OGB, https://ogb.stanford.edu/docs/home/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import argparse\n",
    "import configparser\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_sparse\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch_geometric.utils import negative_sampling, to_networkx\n",
    "from typing import Union, Tuple\n",
    "from torch_geometric.typing import OptPairTensor, Adj, OptTensor, Size\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "\n",
    "from ogb.linkproppred import PygLinkPropPredDataset, Evaluator\n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "a = utils.DaoArgs()\n",
    "\n",
    "print(a.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A(object):\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "a = A()\n",
    "d = {'attr1':'value'}\n",
    "for k, v in d.items():\n",
    "    setattr(a, k, v)\n",
    "    \n",
    "print(a.attr1)\n",
    "\n",
    "for k, v in d.items():\n",
    "    print(getattr(a, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: plot graphs:\n",
    "\n",
    "# circulant graph:\n",
    "\n",
    "cg = nx.circulant_graph(11, [1, 2])\n",
    "\n",
    "plt.figure(figsize=(3,3))\n",
    "nx.draw_circular(cg)\n",
    "\n",
    "\n",
    "cg = nx.circulant_graph(11, [1, 3])\n",
    "\n",
    "plt.figure(figsize=(3,3))\n",
    "nx.draw_circular(cg)\n",
    "\n",
    "cg = nx.circulant_graph(11, [1, 4])\n",
    "\n",
    "plt.figure(figsize=(3,3))\n",
    "nx.draw_circular(cg)\n",
    "# circulant ladder graph:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spd_matrix(G, S, max_spd=5):\n",
    "    spd_matrix = np.zeros((G.number_of_nodes(), len(S)), dtype=np.float32)\n",
    "    for i, node_S in enumerate(S):\n",
    "        for node, length in nx.shortest_path_length(G, source=node_S).items():\n",
    "            spd_matrix[node, i] = min(length, max_spd)\n",
    "    return spd_matrix\n",
    "\n",
    "\n",
    "class Logger(object):\n",
    "    def __init__(self, runs, info=None):\n",
    "        self.info = info\n",
    "        self.results = [[] for _ in range(runs)]\n",
    "\n",
    "    def add_result(self, run, result):\n",
    "        assert len(result) == 2\n",
    "        assert run >= 0 and run < len(self.results)\n",
    "        self.results[run].append(result)\n",
    "\n",
    "    def print_statistics(self, run=None):\n",
    "        if run is not None:\n",
    "            result = 100 * torch.tensor(self.results[run])\n",
    "            argmax = result[:, 0].argmax().item()\n",
    "            print(f'Run {run + 1:02d}:')\n",
    "            print(f'Highest Valid: {result[:, 0].max():.2f}')\n",
    "            print(f'   Final Test: {result[argmax, 1]:.2f}')\n",
    "        else:\n",
    "            result = 100 * torch.tensor(self.results)\n",
    "            best_results = []\n",
    "            for r in result:\n",
    "                valid = r[:, 0].max().item()\n",
    "                test = r[r[:, 0].argmax(), 1].item()\n",
    "                best_results.append((valid, test))\n",
    "            best_result = torch.tensor(best_results)\n",
    "            print(f'All runs:')\n",
    "            r = best_result[:, 0]\n",
    "            print(f'Highest Valid: {r.mean():.4f} ± {r.std():.4f}')\n",
    "            r = best_result[:, 1]\n",
    "            print(f'   Final Test: {r.mean():.4f} ± {r.std():.4f}')\n",
    "\n",
    "\n",
    "class SAGEConv(MessagePassing):\n",
    "    r\"\"\"The GraphSAGE operator from the `\"Inductive Representation Learning on\n",
    "    Large Graphs\" <https://arxiv.org/abs/1706.02216>`_ paper\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{x}^{\\prime}_i = \\mathbf{W}_1 \\mathbf{x}_i + \\mathbf{W}_2 \\cdot\n",
    "        \\mathrm{mean}_{j \\in \\mathcal{N(i)}} \\mathbf{x}_j\n",
    "\n",
    "    Args:\n",
    "        in_channels (int or tuple): Size of each input sample. A tuple\n",
    "            corresponds to the sizes of source and target dimensionalities.\n",
    "        out_channels (int): Size of each output sample.\n",
    "        normalize (bool, optional): If set to :obj:`True`, output features\n",
    "            will be :math:`\\ell_2`-normalized, *i.e.*,\n",
    "            :math:`\\frac{\\mathbf{x}^{\\prime}_i}\n",
    "            {\\| \\mathbf{x}^{\\prime}_i \\|_2}`.\n",
    "            (default: :obj:`False`)\n",
    "        root_weight (bool, optional): If set to :obj:`False`, the layer will\n",
    "            not add transformed root node features to the output.\n",
    "            (default: :obj:`True`)\n",
    "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
    "            an additive bias. (default: :obj:`True`)\n",
    "        **kwargs (optional): Additional arguments of\n",
    "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: Union[int, Tuple[int, int]],\n",
    "                 out_channels: int, normalize: bool = False,\n",
    "                 root_weight: bool = True,\n",
    "                 bias: bool = True, **kwargs):  # yapf: disable\n",
    "        kwargs.setdefault('aggr', 'mean')\n",
    "        super(SAGEConv, self).__init__(**kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.normalize = normalize\n",
    "        self.root_weight = root_weight\n",
    "\n",
    "        if isinstance(in_channels, int):\n",
    "            in_channels = (in_channels, in_channels)\n",
    "\n",
    "        self.lin_l = Linear(in_channels[0], out_channels, bias=bias)\n",
    "        if self.root_weight:\n",
    "            self.lin_r = Linear(in_channels[1], out_channels, bias=False)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.lin_l.reset_parameters()\n",
    "        if self.root_weight:\n",
    "            self.lin_r.reset_parameters()\n",
    "\n",
    "\n",
    "    def forward(self, x: Union[Tensor, OptPairTensor], edge_index: Adj,\n",
    "                edge_attr: OptTensor = None, size: Size = None) -> Tensor:\n",
    "        \"\"\"\"\"\"\n",
    "        if isinstance(x, Tensor):\n",
    "            x: OptPairTensor = (x, x)\n",
    "\n",
    "        # Node and edge feature dimensionalites need to match.\n",
    "        if isinstance(edge_index, Tensor):\n",
    "            assert edge_attr is not None\n",
    "            assert x[0].size(-1) == edge_attr.size(-1)\n",
    "        elif isinstance(edge_index, SparseTensor):\n",
    "            assert x[0].size(-1) == edge_index.size(-1)\n",
    "\n",
    "        # propagate_type: (x: OptPairTensor, edge_attr: OptTensor)\n",
    "        out = self.propagate(edge_index, x=x, edge_attr=edge_attr, size=size)\n",
    "        out = self.lin_l(out)\n",
    "\n",
    "        x_r = x[1]\n",
    "        if self.root_weight and x_r is not None:\n",
    "            out += self.lin_r(x_r)\n",
    "\n",
    "        if self.normalize:\n",
    "            out = F.normalize(out, p=2., dim=-1)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def message(self, x_j: Tensor, edge_attr: Tensor) -> Tensor:\n",
    "        return F.relu(x_j + edge_attr)\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,\n",
    "                                   self.out_channels)\n",
    "\n",
    "\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout):\n",
    "        super(GraphSAGE,self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        \n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj_t, edge_attr, emb_ea):\n",
    "        edge_attr = torch.mm(edge_attr, emb_ea)\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = conv(x, adj_t, edge_attr)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, adj_t, edge_attr)  # no nonlinearity\n",
    "        return x\n",
    "\n",
    "\n",
    "class LinkPredictor(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(LinkPredictor, self).__init__()\n",
    "\n",
    "        self.lins = torch.nn.ModuleList()\n",
    "        self.lins.append(torch.nn.Linear(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.lins.append(torch.nn.Linear(hidden_channels, hidden_channels))\n",
    "        self.lins.append(torch.nn.Linear(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for lin in self.lins:\n",
    "            lin.reset_parameters()\n",
    "\n",
    "    def forward(self, x_i, x_j):\n",
    "        x = x_i * x_j\n",
    "        for lin in self.lins[:-1]:\n",
    "            x = lin(x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.lins[-1](x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Generation\n",
    "\n",
    "https://networkx.org/documentation/latest/tutorial.html#graph-generators-and-graph-operations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate ER-graph Dataset: 1. invariant to node; 2. variant to node.\n",
    "# Generate SBM-graph Dataset: 1. invariant to node; 2. variant to node.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# synthetic dataset generation\n",
    "---\n",
    "- a) `permutation invariant`\n",
    "- b) `permutation variant`\n",
    "- c) `build pipeline to construct node feautures.`\n",
    "- d) `build pipeline to construct graph feautures, e.g., laplacian matrix, wavelets, etc.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO, d), graph feature pipeline.\n",
    "\n",
    "\n",
    "def add_graph_features(graph_features, cons_fea_func, c_dim=0):\n",
    "    \"\"\"input:\n",
    "            by default, the graph feature at dim=0 of graph_features (numpy) is the original adj matrix.\n",
    "            graph_features shape: (B, N, N, C), where C is the graph feature number.\n",
    "            cons_fea_func is the function to construct new graph features with NxN and append to the C-th dimension.\n",
    "            \n",
    "            by default, c_dim is 0, use the first adjacency matrix to construct new features. \n",
    "       return:\n",
    "            graph_features, shape will be (B, N, N, C+1). \n",
    "    \"\"\"\n",
    "    if graph_features.ndim == 3:\n",
    "        graph_features = np.expand_dims(graph_features, axis=-1)\n",
    "        \n",
    "    new_graph_features = []\n",
    "    for ori_feature in graph_features[..., c_dim]:\n",
    "        new_graph_features.append(cons_fea_func(ori_feature))\n",
    "    \n",
    "    new_graph_features = np.expand_dims(np.stack(new_graph_features, axis=0), axis=-1)\n",
    "    \n",
    "    graph_features = np.concatenate([graph_features, new_graph_features], axis=-1)\n",
    "    \n",
    "    return graph_features\n",
    "\n",
    "\n",
    "def composite_node_features(*node_features, padding=False, padding_len=128, pad_value=0):\n",
    "    \"\"\" just concatenate the new_node_features with the cur_node_features (N, C1)\n",
    "        output new node features: (N, C1+C2)\n",
    "    \"\"\"\n",
    "    if isinstance(node_features[0], list):\n",
    "        res = []\n",
    "        for i in range(len(node_features[0])):\n",
    "            fea = np.concatenate((node_features[0][i],node_features[1][i]), axis=-1)\n",
    "            if padding:\n",
    "                fea = np.pad(fea, ((0,0),(0, padding_len-fea.shape[-1])), mode='constant', constant_values=pad_value)\n",
    "            res.append(fea)\n",
    "        return res\n",
    "    \n",
    "    fea = np.concatenate(node_features, axis=-1)\n",
    "    if padding:\n",
    "        fea = np.pad(fea, ((0,padding_len-fea.shape[-1])), mode='constant', constant_values=pad_value)\n",
    "        \n",
    "    return fea\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(1).astype(np.float32)\n",
    "print(a)\n",
    "b = np.stack([a,2*a])\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "\n",
    "\n",
    "def connect_graphs(g1, g2):\n",
    "    n1 = list(g1.nodes)\n",
    "    n2 = list(g2.nodes)\n",
    "    e1 = random.choices(n1, k=1)[0]\n",
    "    e2 = random.choices(n2, k=1)[0]\n",
    "    g_cur = nx.compose(g1, g2)\n",
    "    g_cur.add_edge(e1, e2)\n",
    "    return g_cur\n",
    "\n",
    "def random_connect_graph(graph_list:list):\n",
    "    # NOTE: relabeling the nodes.\n",
    "    \n",
    "    new_graphs = []\n",
    "    np.random.shuffle(graph_list)\n",
    "    node_idx = 0\n",
    "    for g in graph_list:\n",
    "        len_nodes = len(list(g.nodes))\n",
    "        mapping = {}\n",
    "        for i in range(len_nodes):\n",
    "            mapping[i] = i+node_idx\n",
    "        new_g = nx.relabel_nodes(g, mapping)\n",
    "        new_graphs.append(new_g)\n",
    "        node_idx += len_nodes\n",
    "        \n",
    "    g_all = reduce(connect_graphs, new_graphs)\n",
    "    \n",
    "    return g_all\n",
    "\n",
    "\n",
    "def generate_circulant_graph_samples(each_class_num:int, N:int, S:list):\n",
    "    samples = []\n",
    "    for s in S:\n",
    "        g = nx.circulant_graph(N, [1, s])\n",
    "        # pers = set()\n",
    "        # if each_class_num < math.factorial(N):\n",
    "        #     uniq_per = set()\n",
    "        #     while len(uniq_per) < each_class_num:\n",
    "        #         per = np.random.permutation(list(np.arange(0, 8)))\n",
    "        #         pers.add()\n",
    "        # else:\n",
    "        pers = [np.random.permutation(list(np.arange(0, N))) for _ in range(each_class_num)]\n",
    "\n",
    "        A_g = nx.to_scipy_sparse_matrix(g).todense()\n",
    "        # Permutate:\n",
    "        for per in pers:\n",
    "            A_per = A_g[per, :]\n",
    "            A_per = A_per[:, per]\n",
    "            # NOTE: set cycle length or skip length as label.\n",
    "            samples.append((nx.from_numpy_array(A_per), np.array(s).astype(np.float32)))\n",
    "    \n",
    "    return generate_training_graphs(samples)\n",
    "\n",
    "\n",
    "def generate_training_graphs(graphs_cc):\n",
    "\n",
    "    np.random.shuffle(graphs_cc)\n",
    "    \n",
    "    test_sample_size = int(len(graphs_cc)/3)\n",
    "    train_adjs, train_y, test_adjs, test_y = [],[],[],[]\n",
    "    \n",
    "    for g in graphs_cc[:-test_sample_size]:\n",
    "        # TODO: generate some deviation:\n",
    "        if isinstance(g, tuple):\n",
    "            adj, y = g\n",
    "            train_adjs.append(adj)\n",
    "            train_y.append(y)\n",
    "        else:\n",
    "            train_adjs.append(g)\n",
    "            train_y.append(nx.average_clustering(g))\n",
    "        \n",
    "    for g in graphs_cc[-test_sample_size:]:\n",
    "        if isinstance(g, tuple):\n",
    "            adj, y = g\n",
    "            test_adjs.append(adj)\n",
    "            test_y.append(y)\n",
    "        else:\n",
    "            test_adjs.append(g)\n",
    "            test_y.append(nx.average_clustering(g))\n",
    "            \n",
    "    train_adjs = [nx.to_scipy_sparse_matrix(g) for g in train_adjs]\n",
    "    test_adjs = [nx.to_scipy_sparse_matrix(g) for g in test_adjs]\n",
    "\n",
    "\n",
    "    train_y = np.stack(train_y, axis=0)\n",
    "    test_y = np.stack(test_y, axis=0)\n",
    "    \n",
    "    return (train_adjs, train_y, test_adjs, test_y)\n",
    "    \n",
    "\n",
    "def generate_cc_no_degree_corr_samples(cc_range_num=20, rand_con=True):\n",
    "    \n",
    "    def random_add_edges(graph, E=3):\n",
    "        nodes = list(graph.nodes)\n",
    "        for i in range(E):\n",
    "            e = random.sample(nodes, k=2)\n",
    "            graph.add_edge(*e)\n",
    "        return graph\n",
    "        \n",
    "    cc_range_num = cc_range_num\n",
    "    graphs_cc = []\n",
    "    for k in range(1, cc_range_num):\n",
    "        m = cc_range_num - k\n",
    "        G_tri = [nx.complete_graph(3) for _ in range(k)]\n",
    "        G_sqr = [nx.cycle_graph(4) for _ in range(m)]\n",
    "        cur_graphs = [random_connect_graph(utils.flatten_list([G_tri, G_sqr])) for _ in range(5)]\n",
    "        # repeat for 5 times:\n",
    "        for _ in range(5):\n",
    "            [graphs_cc.append(random_add_edges(g, E=3)) for g in cur_graphs]        \n",
    "\n",
    "    return generate_training_graphs(graphs_cc)\n",
    "    \n",
    "def get_value(xargs, key, default=None):\n",
    "    \"\"\"return (N, 1) all one node feature\n",
    "    \"\"\"\n",
    "    return xargs[key] if key in xargs else default\n",
    "\n",
    "# node or edge feature generation:\n",
    "\n",
    "def xargs(f):\n",
    "    def wrap(**xargs):\n",
    "        return f(**xargs)\n",
    "    return wrap\n",
    "\n",
    "@xargs\n",
    "def node_tri_cycles_feature(adj, k=2):\n",
    "    \"\"\" A^k as node features. so the dim of feature equals to the number of nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(adj, np.ndarray):\n",
    "        adj = adj.todense()\n",
    "    adj = np.multiply(adj, np.matmul(adj, adj))\n",
    "    adj = np.sum(adj, axis=1).reshape(-1, 1)\n",
    "    return adj.astype(np.float32)\n",
    "\n",
    "@xargs\n",
    "def node_k_adj_feature(adj, k=2):\n",
    "        \n",
    "    \"\"\" A^k as node features. so the dim of feature equals to the number of nodes.\n",
    "    \"\"\"\n",
    "    if not isinstance(adj, np.ndarray):\n",
    "        adj = adj.todense()\n",
    "    ori_adj = adj\n",
    "    for _ in range(k-1):\n",
    "        adj = np.matmul(adj, ori_adj)\n",
    "    return adj.astype(np.float32)\n",
    "\n",
    "@xargs\n",
    "def node_degree_feature(adj):\n",
    "    \"\"\" node (weighted, if its weighted adjacency matrix) degree as the node feature.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not isinstance(adj, np.ndarray):\n",
    "        adj = adj.todense()\n",
    "    degrees = np.sum(adj, axis=1).reshape(adj.shape[0], 1)\n",
    "    return degrees.astype(np.float32)\n",
    "\n",
    "\n",
    "@xargs\n",
    "def node_random_id_feature(adj, ratio=1.0):\n",
    "        \n",
    "    if not isinstance(adj, np.ndarray):\n",
    "        adj = adj.todense()\n",
    "\n",
    "    N = adj.shape[0]\n",
    "    id_features = np.random.randint(1, int(N*ratio), size=N).reshape(N, 1).astype(np.float32)\n",
    "    return id_features\n",
    "\n",
    "@xargs\n",
    "def node_allone_feature(adj):\n",
    "    \"\"\"return (N, 1) all one node feature\n",
    "    \"\"\"\n",
    "    if not isinstance(adj, np.ndarray):\n",
    "        adj = adj.todense()\n",
    "        \n",
    "    N = adj.shape[0]\n",
    "    return np.ones(N).reshape(N, 1).astype(np.float32)\n",
    "\n",
    "\n",
    "@xargs\n",
    "def node_gaussian_feature(adj, mean_v=0.1, std_v=1.0):\n",
    "    if not isinstance(adj, np.ndarray):\n",
    "        adj = adj.todense()\n",
    "        \n",
    "    N = adj.shape[0]\n",
    "    \n",
    "    return np.random.normal(loc=mean_v, scale=std_v, size=(N, 1)).astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "@xargs\n",
    "def node_index_feature(adj):\n",
    "    \"\"\"return (N, 1) node feature, feature equals to the index+1\n",
    "    \"\"\"\n",
    "    N = adj.shape[0]\n",
    "    return np.arange(1, N+1).reshape(N, 1).astype(np.float32)\n",
    "\n",
    "@xargs\n",
    "def node_deviated_feature(adj):\n",
    "    N = adj.shape[0]\n",
    "    block_N = int(N/2)\n",
    "    fea1 = np.arange(1, block_N+1).reshape(block_N, 1).astype(np.float32)\n",
    "    fea2 = 3 * np.arange(block_N+1, N+1).reshape(block_N, 1).astype(np.float32)\n",
    "    return np.concatenate([fea1, fea2], axis=0)\n",
    "    \n",
    "\n",
    "# node clustering coefficient\n",
    "\n",
    "@xargs\n",
    "def node_cc_avg_feature(adj):\n",
    "    N = adj.shape[0]\n",
    "    g_cur = nx.from_numpy_array(adj)\n",
    "    feats = nx.average_clustering(g_cur)\n",
    "    return feats\n",
    "\n",
    "@xargs\n",
    "def node_cc_feature(adj):\n",
    "    N = adj.shape[0]\n",
    "    g_cur = nx.from_numpy_array(adj)\n",
    "    feas_dict = nx.clustering(g_cur)\n",
    "    feats = []\n",
    "    for i in range(N):\n",
    "        feats.append(feas_dict[i])\n",
    "    feats = np.array(feats).reshape(N, 1).astype(np.float32)\n",
    "    return feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.factorial(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_adjs, train_y, test_adjs, test_y = generate_circulant_graph_samples(4, N=7, S=[2, 3, 4])\n",
    "nx.draw_circular(nx.from_numpy_array(train_adjs[0].todense()))\n",
    "print(train_adjs[0].todense())\n",
    "plt.figure()\n",
    "nx.draw_circular(nx.from_numpy_array(train_adjs[1].todense()))\n",
    "plt.figure()\n",
    "nx.draw_circular(nx.from_numpy_array(train_adjs[2].todense()))\n",
    "\n",
    "# # TODO: Permutation\n",
    "# per = np.random.permutation(list(np.arange(0, 8)))\n",
    "# A = train_adjs[0].todense()\n",
    "# print('A', A)\n",
    "# A = A[per, :]\n",
    "# A = A[:, per]\n",
    "\n",
    "# gp = nx.from_numpy_array(A)\n",
    "# Ap = nx.to_scipy_sparse_matrix(gp)\n",
    "# print('A per:', Ap.todense())\n",
    "\n",
    "\n",
    "# def test2(f):\n",
    "#     def wrap(adj, **xargs):\n",
    "#         return f(adj, **xargs)\n",
    "#     return wrap\n",
    "\n",
    "# @test2\n",
    "# def test(adj, tt=0):\n",
    "#     print('tt is',tt)\n",
    "\n",
    "# def test1(**xargs):\n",
    "#     a = xargs\n",
    "#     print('type:', a is None)\n",
    "#     test('adj', **a)\n",
    "\n",
    "\n",
    "# test1()\n",
    "g = nx.circulant_graph(11, [1, 5])\n",
    "# nx.draw_circular(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Given dataset, compare with:\n",
    "* MLP\n",
    "* CNN\n",
    "* GNN\n",
    "* GAE\n",
    "* GGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import models\n",
    "import utils\n",
    "from scipy.sparse import coo_matrix\n",
    "from models import BaseGraph, BaseGraphUtils, GraphDataset\n",
    "\n",
    "\n",
    "importlib.reload(models)\n",
    "importlib.reload(utils)\n",
    "\n",
    "\n",
    "def fill_nan_inf(a:np.ndarray):\n",
    "    a[np.isnan(a)] = 0\n",
    "    a[np.isinf(a)] = 0\n",
    "    return a\n",
    "    \n",
    "def matrix_power(m:coo_matrix, pow=1):\n",
    "    if pow==1:\n",
    "        return m\n",
    "    ori_A = m\n",
    "    for _ in range(pow-1):\n",
    "        m = m @ ori_A\n",
    "    m[m>1]=1\n",
    "    return m\n",
    "\n",
    "def construct_dataset(graph_data, node_features, norm=True, lap_encode=False, \\\n",
    "                      lap_en_dim=8, y_torch_type=torch.LongTensor, sparse=False, K=1):\n",
    "    \n",
    "    train_adjs, train_y, test_adjs, test_y = graph_data\n",
    "    # construct node features:\n",
    "    train_node_fea, val_node_fea = node_features\n",
    "    scalers_y = []\n",
    "    if norm:\n",
    "        if sparse:\n",
    "            # NOTE: normlize through each feature dimension.\n",
    "            scalers = []\n",
    "            train_node_feas = np.concatenate(train_node_fea, axis=0)\n",
    "            val_node_feas = np.concatenate(val_node_fea, axis=0)\n",
    "            print('feature len:', train_node_fea[0].shape[-1])\n",
    "            for fea_i in range(train_node_fea[0].shape[-1]):\n",
    "                _, scaler = utils.normalize(train_node_feas[..., fea_i])\n",
    "                scalers.append(scaler)\n",
    "                _, scaler = utils.normalize(val_node_feas[..., fea_i])\n",
    "            \n",
    "            for i in range(len(train_node_fea)):\n",
    "                for fea_i in range(train_node_fea[0].shape[-1]):\n",
    "                    train_node_fea[i][..., fea_i] = scalers[fea_i].transform(train_node_fea[i][..., fea_i])\n",
    "                    \n",
    "            for i in range(len(val_node_fea)):\n",
    "                for fea_i in range(val_node_fea[0].shape[-1]):\n",
    "                    val_node_fea[i][..., fea_i] = scalers[fea_i].transform(val_node_fea[i][..., fea_i])\n",
    "            # TODO: fill nan:\n",
    "            for i in range(len(train_node_fea)):\n",
    "                train_node_fea[i] = fill_nan_inf(train_node_fea[i])\n",
    "                \n",
    "            for i in range(len(val_node_fea)):\n",
    "                val_node_fea[i] = fill_nan_inf(val_node_fea[i])\n",
    "                \n",
    "            # TODO: normalize label y:\n",
    "            train_y, scaler_train_y = utils.normalize(train_y)\n",
    "            test_y, scaler_test_y = utils.normalize(test_y)\n",
    "            \n",
    "            train_y =  fill_nan_inf(train_y)\n",
    "            test_y =  fill_nan_inf(test_y)\n",
    "            scalers_y.append(scaler_train_y)\n",
    "            scalers_y.append(scaler_test_y)\n",
    "            \n",
    "        else:\n",
    "            mean_y = train_node_fea.mean()\n",
    "            std_y = train_node_fea.std()\n",
    "            scaler = utils.StandardScaler(mean=mean_y, std=std_y)\n",
    "            train_node_fea =  fill_nan_inf(scaler.transform(train_node_fea))\n",
    "            val_node_fea =  fill_nan_inf(utils.normalize(val_node_fea))\n",
    "        \n",
    "    else:\n",
    "        scalers = None\n",
    "\n",
    "    train_base_graphs = []\n",
    "    if sparse:\n",
    "        for i, adj in enumerate(train_adjs):\n",
    "            # NOTE: set A as A^K\n",
    "            adj = matrix_power(adj.tocoo(), pow=K)\n",
    "            g = models.BaseGraphUtils.from_scipy_coo(adj.tocoo())\n",
    "            g.set_node_feat(train_node_fea[i])\n",
    "            train_base_graphs.append(g)\n",
    "\n",
    "        test_base_graphs = []\n",
    "        for i, adj in enumerate(test_adjs):\n",
    "            adj = matrix_power(adj.tocoo(), pow=K)\n",
    "            g = models.BaseGraphUtils.from_scipy_coo(adj.tocoo())\n",
    "            g.set_node_feat(val_node_fea[i])\n",
    "            test_base_graphs.append(g)\n",
    "    else:\n",
    "        for i, adj in enumerate(train_adjs):\n",
    "            g = models.BaseGraphUtils.from_numpy(adj)\n",
    "            g.set_node_feat(train_node_fea[i])\n",
    "            train_base_graphs.append(g)\n",
    "\n",
    "        test_base_graphs = []\n",
    "        for i, adj in enumerate(test_adjs):\n",
    "            g = models.BaseGraphUtils.from_numpy(adj)\n",
    "            g.set_node_feat(val_node_fea[i])\n",
    "            test_base_graphs.append(g)\n",
    "    \n",
    "    \n",
    "    train_dataset = GraphDataset(x=train_base_graphs, y=y_torch_type(train_y))\n",
    "    test_dataset = GraphDataset(x=test_base_graphs, y=y_torch_type(test_y))\n",
    "\n",
    "    if lap_encode:\n",
    "        train_dataset._add_lap_positional_encodings(lap_en_dim)\n",
    "        test_dataset._add_lap_positional_encodings(lap_en_dim)\n",
    "    \n",
    "    return train_dataset, test_dataset, scalers, scalers_y\n",
    "\n",
    "\n",
    "from models import BaseGraphUtils\n",
    "# to dataloader:\n",
    "import models \n",
    "importlib.reload(models)\n",
    "\n",
    "def assemble_dataloader(train_dataset: GraphDataset, test_dataset: GraphDataset, cuda=False, batch_size=20):\n",
    "    if cuda:\n",
    "        train_dataset.cuda()\n",
    "        test_dataset.cuda()\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_dataset.collate)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=test_dataset.collate)\n",
    "\n",
    "    def print_tuple(x):\n",
    "        if isinstance(x, list) or isinstance(x, tuple):\n",
    "            adj = x[1]\n",
    "            x = x[0]\n",
    "        \n",
    "    # for x, y in train_dataloader:\n",
    "    #     print(type(x))\n",
    "    #     print_tuple(x)\n",
    "    #     print(y.is_cuda)\n",
    "    #     break\n",
    "    \n",
    "    return (train_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from curses import KEY_A3\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "\n",
    "row  = np.array([0,0,3,1,0])\n",
    "col  = np.array([0,1,3,1,2])\n",
    "data = np.array([1,1,1,1,1])\n",
    "m = coo_matrix((data,(row,col)), shape=(4,4))\n",
    "print(m.tocoo().todense())\n",
    "\n",
    "ori = m\n",
    "for _ in range(K):\n",
    "    m = m @ ori\n",
    "\n",
    "m[m>1] = 1\n",
    "\n",
    "print(m.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP model\n",
    "import models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from models import BaseGraph, BaseGraphUtils\n",
    "\n",
    "\n",
    "\n",
    "def mirror_adj(a: torch.Tensor):\n",
    "    upper_tri = torch.triu(a)\n",
    "    a1 = (upper_tri+upper_tri.T).fill_diagonal_(1.0)\n",
    "    lower_tri = torch.tril(a)\n",
    "    a2 = (lower_tri+lower_tri.T).fill_diagonal_(1.0)\n",
    "    return a1, a2\n",
    "    \n",
    "\n",
    "\n",
    "class DirectModelAdapter(nn.Module):\n",
    "    def __init__(self, model, pooling, out_dim, node_fea=None):\n",
    "        super(DirectModelAdapter, self).__init__()\n",
    "        self.model = model\n",
    "        self.pooling = pooling\n",
    "        self.node_fea = node_fea\n",
    "        self.ln = nn.Linear(model.out_dim, out_dim)\n",
    "    \n",
    "    def forward(self, graphs:BaseGraph):\n",
    "        \n",
    "        node_x, adj = graphs.get_node_features(), graphs.A\n",
    "        if self.node_fea is not None:\n",
    "            node_x = self.node_fea\n",
    "            \n",
    "        # print(graphs.graph_type)\n",
    "        if graphs.graph_type == 'pyg':\n",
    "            dense_a = graphs.pyg_graph.edge_index.to_dense()\n",
    "            dense_a1, dense_a2 = mirror_adj(dense_a)\n",
    "            # TODO: dense to sparse.\n",
    "            coo1 = BaseGraphUtils.dense_to_coo(dense_a1)\n",
    "            coo2 = BaseGraphUtils.dense_to_coo(dense_a2)\n",
    "            edge_index1 = coo1.indices()\n",
    "            edge_index2 = coo2.indices()\n",
    "            \n",
    "        else:\n",
    "            adj1 = []\n",
    "            adj2 = []\n",
    "            for a in adj:\n",
    "                a1, a2 = mirror_adj(a)\n",
    "                adj1.append(a1)\n",
    "                adj2.append(a2)\n",
    "                \n",
    "            edge_index1 = torch.stack(adj1, dim=0)\n",
    "            edge_index2 = torch.stack(adj2, dim=0)\n",
    "        \n",
    "        out = self.model(node_x, edge_index1, edge_index2, graphs)\n",
    "        out = self.pooling(out)\n",
    "        out = self.ln(out)\n",
    "        return out\n",
    "\n",
    "        \n",
    "class GNNModelAdapter(nn.Module):\n",
    "    def __init__(self, model, pooling, out_dim, node_fea=None, mid_fea=False, device='cpu',\n",
    "    neighbor_k=1):\n",
    "        super(GNNModelAdapter, self).__init__()\n",
    "        self.neighbor_k = neighbor_k\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        \n",
    "        self.mid_fea = mid_fea\n",
    "        self.pooling = pooling\n",
    "        self.node_fea = node_fea\n",
    "        self.ln = nn.Linear(model.out_dim, out_dim)\n",
    "        self.mask_vectors = None\n",
    "        self.to(device)\n",
    "\n",
    "    def mask(self, M=3, mask_len=128):\n",
    "        \"\"\"\n",
    "            set learnable mask vectors, M \\times mask_len\n",
    "        \"\"\"\n",
    "        self.mask_vectors = nn.Parameter(torch.ones(M, mask_len)).to(self.device)\n",
    "\n",
    "\n",
    "    # def preprocess_edge_index(self, edge_index:Tensor):\n",
    "    #     if self.neighbor_k == 1:\n",
    "    #         return edge_index\n",
    "\n",
    "    #     if isinstance(edge_index, SparseTensor):\n",
    "    #         edge_index = edge_index.set_value(None, layout=None)\n",
    "    #         ori = edge_index\n",
    "    #         for _ in range(self.neighbor_k-1):\n",
    "    #             edge_index = torch_sparse.matmul(edge_index, ori)\n",
    "    #     else:\n",
    "    #         ori = edge_index\n",
    "    #         spm = torch.sparse_coo_tensor(ori, None, (2, 3))\n",
    "    #         for _ in range(self.neighbor_k-1):\n",
    "    #             print('edge_index shape:', edge_index.shape)\n",
    "    #             print('ori shape:', ori.shape)\n",
    "                \n",
    "    #             edge_index = torch.matmul(edge_index, ori)\n",
    "        \n",
    "    #     return edge_index\n",
    "        \n",
    "\n",
    "    def forward(self, graphs:BaseGraph):\n",
    "        node_x, adj = graphs.get_node_features(), graphs.A\n",
    "        if self.node_fea is not None:\n",
    "            node_x = self.node_fea\n",
    "        \n",
    "        if self.mask_vectors is not None:\n",
    "            node_x = torch.cat([node_x * self.mask_vectors[i] for i in range(self.mask_vectors.shape[0])], dim=-1)\n",
    "\n",
    "        if graphs.graph_type == 'pyg' or graphs.graph_type == 'coo':\n",
    "            edge_indices = graphs.get_edge_index()\n",
    "\n",
    "            mid_feature = self.model(node_x, edge_indices, graphs=graphs)\n",
    "        else:\n",
    "            mid_feature = self.model(node_x, adj, graphs)\n",
    "            \n",
    "        out = self.pooling(mid_feature, graphs.pyg_graph.batch) if self.pooling is not None else mid_feature\n",
    "        out = self.ln(out)\n",
    "        if self.mid_fea:\n",
    "            return out, mid_feature\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "\n",
    "def choose_model(args, name, node_fea_dim=1, graph_fea_dim=1, class_num=6, node_num=20,\n",
    "                 out_mid_fea=False, node_wise=False):\n",
    "    import importlib\n",
    "    \n",
    "    # import baseline_models.gnn_lspe.nets.OGBMOL_graph_classification.gatedgcn_net as lspe_net\n",
    "    # import baseline_models.gnn_lspe.layers.gatedgcn_layer as lspe_layers\n",
    "\n",
    "    import baseline_models.gnn_baselines\n",
    "    from baseline_models import identity_GNN\n",
    "\n",
    "\n",
    "    from torch_geometric.nn import global_mean_pool, global_add_pool, global_max_pool, global_sort_pool\n",
    "    \n",
    "    importlib.reload(models)\n",
    "    importlib.reload(identity_GNN)\n",
    "    \n",
    "    \n",
    "    N = node_num\n",
    "    if name =='mlp':\n",
    "        my_model = models.ClassPredictor(N*N, 64, class_num, 3, dropout=0.6)\n",
    "    elif name == 'cnn':\n",
    "        my_model = models.SimpleCNN(graph_fea_dim, 64, class_num, dropout=0.6)\n",
    "    elif name == 'cnn_big':\n",
    "        my_model = models.SimpleCNN(graph_fea_dim, 64, class_num, dropout=0.6, kernelsize=(11, 11))\n",
    "    elif name == 'gnn':\n",
    "        # pool =  models.GateGraphPooling(None, N = N)\n",
    "        pool = global_mean_pool\n",
    "        layer_num = 2\n",
    "        my_model = GNNModelAdapter(models.MultilayerGNN(layer_num, node_fea_dim, 64, 32, dropout=0.6), pool, class_num)\n",
    "    elif name == 'gin':\n",
    "        # pool =  models.GateGraphPooling(None, N = N)\n",
    "        if node_wise:\n",
    "            pool = None\n",
    "        else:\n",
    "            pool = models.MeanPooling()\n",
    "            # pool = global_mean_pool\n",
    "        layer_num = 3\n",
    "        my_model = GNNModelAdapter(models.GINNet(args, node_fea_dim, 64, 32, 3, dropout=0.6), pool, class_num, mid_fea=out_mid_fea)\n",
    "    elif name == 'sparse_gin':\n",
    "        if node_wise:\n",
    "            pool = None\n",
    "        else:\n",
    "            pool = global_mean_pool\n",
    "            # pool = global_mean_pool\n",
    "        hid_dim = args.gnn_hid_dim # 128\n",
    "        layer_num = args.gnn_layer_num\n",
    "        bi_direct = args.bi\n",
    "        rep_fea_dim = args.rep_fea_dim # 64\n",
    "        my_model = GNNModelAdapter(models.LSDGINNet(args, node_fea_dim, hid_dim, rep_fea_dim, layer_num, dropout=0.5, bi=bi_direct),\n",
    "                     pool, class_num, mid_fea=out_mid_fea)\n",
    "                     \n",
    "    elif name == 'sparse_gin_mask':\n",
    "        if node_wise:\n",
    "            pool = None\n",
    "        else:\n",
    "            pool = global_mean_pool\n",
    "            # pool = global_mean_pool\n",
    "        layer_num = 3\n",
    "        M = 3\n",
    "        mask_len = node_fea_dim\n",
    "        node_fea_dim = M * mask_len\n",
    "        my_model = GNNModelAdapter(models.LSDGINNet(args, node_fea_dim, 128, 64, 3, dropout=0.5, bi=False),\n",
    "         pool, class_num, mid_fea=out_mid_fea, device='cuda:0')\n",
    "        my_model.mask(M, mask_len=mask_len)\n",
    "\n",
    "    elif name == 'gin_direc':\n",
    "        pool =  models.GateGraphPooling(None, N = N)\n",
    "        layer_num = 3\n",
    "        di_model = models.DiGINNet(args, node_fea_dim, 64, 32, 3, dropout=0.6)\n",
    "        my_model = DirectModelAdapter(di_model, pool, class_num)\n",
    "    elif name == 'lsd_gin':\n",
    "        pool = global_mean_pool\n",
    "        # pool = global_add_pool\n",
    "        layer_num = 3\n",
    "        di_model = models.LSDGINNet(args, node_fea_dim, 64, 32, 3, dropout=0.6)\n",
    "        my_model = DirectModelAdapter(di_model, pool, class_num)\n",
    "    \n",
    "    elif name == 'idgnn':\n",
    "        pool = global_mean_pool\n",
    "        layer_num = 3\n",
    "        idgnn_model = identity_GNN.IDGNN(args, node_fea_dim, 64, 32, layer_num)\n",
    "        my_model = GNNModelAdapter(idgnn_model, pool, class_num, mid_fea=out_mid_fea)\n",
    "        \n",
    "    elif name == 'gcn':        \n",
    "        pool = global_mean_pool\n",
    "        layer_num = 3\n",
    "        idgnn_model = models.MultiGCN(node_fea_dim, 128, 64, layer_num)\n",
    "        my_model = GNNModelAdapter(idgnn_model, pool, class_num, mid_fea=out_mid_fea)\n",
    "    elif name == 'lspe':\n",
    "        pe_init = 'lap_pe'\n",
    "        pos_enc_dim = 8\n",
    "        in_dim = node_fea_dim\n",
    "        hid_dim = 64\n",
    "        out_dim = 32\n",
    "        layer_num= 3\n",
    "        lspe_model = lspe_net.ReGatedGCNNet(pe_init, pos_enc_dim, in_dim,\n",
    "                                       hid_dim,out_dim,layer_num=layer_num)\n",
    "        \n",
    "        pool =  models.GateGraphPooling(None, N = N)\n",
    "        my_model = GNNModelAdapter(lspe_model, pool, class_num)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    my_model.cuda()\n",
    "\n",
    "    return my_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "proj_path = '/li_zhengdao/github/generativegnn'\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "class GraphEvaluator:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def evaluate(self, preds, labels, loss):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def statistic(self, epoch):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class SimpleEvaluator(GraphEvaluator):\n",
    "    def __init__(self, args, is_regression=False):\n",
    "        super(SimpleEvaluator, self).__init__()\n",
    "        self.args = args\n",
    "        self.metrics = defaultdict(list)\n",
    "        self.preds = []\n",
    "        self.labels = []\n",
    "        self.epoch_loss = []\n",
    "        self.mean_metrics = defaultdict(list)\n",
    "        self.total_metrics = {}\n",
    "        self.is_regression = is_regression\n",
    "        \n",
    "    def evaluate(self, preds, labels, loss, null_val=0.0):\n",
    "        \n",
    "        if self.is_regression:\n",
    "            preds_b = preds\n",
    "            if np.isnan(null_val):\n",
    "                mask = ~torch.isnan(labels)\n",
    "            else:\n",
    "                mask = (labels != null_val)\n",
    "                mask = mask.float()\n",
    "                mask /= torch.mean(mask)\n",
    "                # handle all zeros.\n",
    "                mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n",
    "                mse = (preds - labels) ** 2\n",
    "                mae = loss\n",
    "                mape = mae / labels\n",
    "                mae, mape, mse = [mask_and_fillna(l, mask) for l in [mae, mape, mse]]\n",
    "                rmse = torch.sqrt(mse)\n",
    "                \n",
    "                self.metrics['mae'].append(mae.item())\n",
    "                self.metrics['mape'].append(mape.item())\n",
    "                self.metrics['rmse'].append(rmse.item())\n",
    "                self.metrics['loss'].append(loss)\n",
    "            \n",
    "        else:\n",
    "            num = preds.size(0)\n",
    "            # print('evl preds shape:', preds.shape, labels.shape)\n",
    "            preds_b = preds.argmax(dim=1).squeeze()\n",
    "            labels = labels.squeeze()\n",
    "            ones = torch.zeros(num)\n",
    "            ones[preds_b == labels] = 1\n",
    "            acc = torch.sum(ones) / num\n",
    "                \n",
    "            mi_f1, ma_f1, weighted_f1 = utils.cal_f1(preds_b.cpu().detach().numpy(), labels.cpu().detach().numpy())\n",
    "            self.metrics['micro_f1'].append(mi_f1)\n",
    "            self.metrics['macro_f1'].append(ma_f1)\n",
    "            self.metrics['weighted_f1'].append(weighted_f1)\n",
    "            self.metrics['acc'].append(acc.numpy())\n",
    "            self.metrics['loss'].append(loss)\n",
    "        \n",
    "        self.preds.append(preds_b)\n",
    "        self.labels.append(labels)\n",
    "        \n",
    "        \n",
    "    def statistic(self, epoch):\n",
    "        for k, v in self.metrics.items():\n",
    "            self.mean_metrics[k].append(np.mean(v))\n",
    "            \n",
    "        self.metrics = defaultdict(list)\n",
    "            \n",
    "    def eval_on_test(self):\n",
    "        if self.is_regression:\n",
    "            preds = torch.cat(self.preds, dim=0)\n",
    "            labels = torch.cat(self.labels, dim=0)\n",
    "            mask = (labels != 0.0)\n",
    "            mask = mask.float()\n",
    "            mask /= torch.mean(mask)\n",
    "            # handle all zeros.\n",
    "            mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n",
    "            mse = (preds - labels) ** 2\n",
    "            mae = torch.abs(preds - labels)\n",
    "            mape = mae / labels\n",
    "            mae, mape, mse = [mask_and_fillna(l, mask) for l in [mae, mape, mse]]\n",
    "            rmse = torch.sqrt(mse)\n",
    "            \n",
    "            self.metrics['mae'].append(mae)\n",
    "            self.metrics['mape'].append(mape)\n",
    "            self.metrics['rmse'].append(rmse)\n",
    "            \n",
    "        else:\n",
    "            preds = torch.cat(self.preds, dim=0)\n",
    "            labels = torch.cat(self.labels, dim=0)\n",
    "            mi_f1, ma_f1, weighted_f1 = utils.cal_f1(preds.cpu().detach().numpy(), labels.cpu().detach().numpy())\n",
    "            self.total_metrics['micro_f1'] = mi_f1\n",
    "            self.total_metrics['macro_f1'] = ma_f1\n",
    "            self.total_metrics['weighted_f1'] = weighted_f1\n",
    "    \n",
    "    def print_info(self):\n",
    "        micro_f1 = self.metrics[-1]['micro_f1']\n",
    "        macro_f1 = self.metrics[-1]['macro_f1']\n",
    "        weighted_f1 = self.metrics[-1]['weighted_f1']\n",
    "        acc = self.metrics[-1]['acc']\n",
    "        loss =  self.metrics[-1]['loss']\n",
    "        print('------------- metrics -------------------')\n",
    "        print(f'micro f1: {\"%.3f\" % micro_f1}, macro f1:  {\"%.3f\" % macro_f1},weighted f1:  {\"%.3f\" % weighted_f1},\\n \\\n",
    "            acc:  {\"%.3f\" % acc}, loss: {\"%.3f\" % loss}')\n",
    "        \n",
    "    \n",
    "        \n",
    "    def plot_metrics(self):\n",
    "        \n",
    "        preds = torch.cat(self.preds, dim=0)\n",
    "        labels = torch.cat(self.labels, dim=0)\n",
    "        \n",
    "        if not self.is_regression:\n",
    "            plot_confuse_matrix(preds, labels)\n",
    "        \n",
    "        loss_list = self.mean_metrics['loss']\n",
    "        \n",
    "        plot_loss(loss_list)\n",
    "        \n",
    "        \n",
    "        \n",
    "def base_args():\n",
    "    args = utils.get_common_args()\n",
    "    args = args.parse_args([])\n",
    "    \n",
    "    args.debug = True\n",
    "    utils.DLog.init(args)\n",
    "    args.lr=0.00004\n",
    "    args.cuda=True\n",
    "    return args\n",
    "\n",
    "\n",
    "def training(epochs, trainer, train_evaluator, test_evaluator:SimpleEvaluator, train_dataloader, \n",
    "             test_dataloader, cuda=True):\n",
    "    for e in range(epochs):\n",
    "        for x, y in train_dataloader:\n",
    "            if cuda:\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "            y = y.squeeze()\n",
    "            loss, pred_y = trainer.train(x, y)\n",
    "            train_evaluator.evaluate(pred_y, y, loss)\n",
    "        \n",
    "        for x, y in test_dataloader:\n",
    "            if cuda:\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "            y = y.squeeze()\n",
    "            loss, pred_y = trainer.eval(x, y)\n",
    "            test_evaluator.evaluate(pred_y, y, loss)\n",
    "        \n",
    "        train_evaluator.statistic(e)\n",
    "        test_evaluator.statistic(e)\n",
    "        \n",
    "    train_evaluator.eval_on_test()\n",
    "    test_evaluator.eval_on_test()\n",
    "        \n",
    "      \n",
    "def plot_confuse_matrix(preds, labels):\n",
    "    \n",
    "    sns.set()\n",
    "    fig = plt.figure(figsize=(3, 1.5),tight_layout=True, dpi=150)\n",
    "    ax = fig.gca()\n",
    "    gts = [int(l) for l in labels]\n",
    "    preds = [int(l) for l in preds]\n",
    "    \n",
    "    label_names = list(set(preds))\n",
    "    C2= np.around(confusion_matrix(gts, preds, labels=label_names, normalize='true'), decimals=2)\n",
    "\n",
    "    # from confusion to ACC, micro-F1, macro-F1, weighted-f1.\n",
    "    print('Confusion:', C2)\n",
    "    font_size = 6\n",
    "    p = sns.heatmap(C2, cbar=False, annot=True, ax=ax, cmap=\"YlGnBu\", square=False, annot_kws={\"size\":font_size},\n",
    "        yticklabels=label_names,xticklabels=label_names)\n",
    "    \n",
    "    ax.tick_params(axis='x', labelsize=font_size)\n",
    "    ax.tick_params(axis='y', labelsize=font_size)\n",
    "    plt.tight_layout()\n",
    "\n",
    "            \n",
    "\n",
    "def plot_loss(loss):\n",
    "    fig = plt.figure(figsize=(3, 1.5), tight_layout=True, dpi=150)\n",
    "    plt.plot(loss)\n",
    "    plt.xticks(fontsize=6)\n",
    "    plt.yticks(fontsize=6)\n",
    "    # plt.ylim((0, 1.0))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def mask_and_fillna(loss, mask):\n",
    "    loss = loss * mask\n",
    "    loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n",
    "    return torch.mean(loss)\n",
    "\n",
    "\n",
    "class MAECal(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MAECal, self).__init__()\n",
    "        pass\n",
    "    \n",
    "    def forward(self, pred_y, labels, null_val=0.0):\n",
    "        \n",
    "        labels = labels.squeeze()\n",
    "        if np.isnan(null_val):\n",
    "            mask = ~torch.isnan(labels)\n",
    "        else:\n",
    "            mask = (labels != null_val)\n",
    "        \n",
    "        mask = mask.float()\n",
    "        mask /= torch.mean(mask)\n",
    "        mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n",
    "\n",
    "        mae = torch.abs(pred_y - labels)\n",
    "        mae = mask_and_fillna(mae, mask)\n",
    "        return mae\n",
    "\n",
    "class CELossCal(nn.Module):\n",
    "    def __init__(self, weights=None):\n",
    "        super(CELossCal, self).__init__()\n",
    "        self.crite = nn.CrossEntropyLoss(weight=weights)\n",
    "        \n",
    "    def forward(self, pred_y, y):\n",
    "        return self.crite(pred_y, y)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "# use MLP to trian:\n",
    "\n",
    "def train_mlp(train_loader, test_loader, epoch=200, plot=False):\n",
    "    args = base_args()\n",
    "    mlp_model = choose_model('mlp')\n",
    "\n",
    "    args.debug = False\n",
    "    utils.DLog.init(args)\n",
    "    # opt = optim.Adam(mlp_model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    opt = optim.SGD(mlp_model.parameters(), lr=0.001)\n",
    "    ce_loss_cal = CELossCal()\n",
    "\n",
    "    trainer = utils.Trainer(mlp_model, optimizer=opt, loss_cal=ce_loss_cal)\n",
    "    train_sim_evl= SimpleEvaluator(args)\n",
    "    test_sim_evl= SimpleEvaluator(args)\n",
    "\n",
    "    training(epoch, trainer, train_sim_evl, test_sim_evl, train_loader, test_loader)\n",
    "\n",
    "    if plot:\n",
    "        train_sim_evl.plot_metrics()\n",
    "        test_sim_evl.plot_metrics()\n",
    "    \n",
    "    return train_sim_evl, test_sim_evl\n",
    "\n",
    "# _,_ = train_mlp(va_train_dataloader, va_test_dataloader, epoch=1, plot=True)\n",
    "# train_mlp(inva_train_dataloader, inva_test_dataloader, True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "\n",
    "\n",
    "def train_cnn(train_loader, test_loader, epoch=200, plot=False, cnn_name='cnn', graph_fea_dim=1):\n",
    "# use CNN to trian:\n",
    "    args = base_args()\n",
    "    cnn_model = choose_model(cnn_name, graph_fea_dim=graph_fea_dim)\n",
    "    # opt = optim.Adam(mlp_model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    opt = optim.SGD(cnn_model.parameters(), lr=0.0001)\n",
    "    ce_loss_cal = CELossCal()\n",
    "\n",
    "    trainer = utils.Trainer(cnn_model, optimizer=opt, loss_cal=ce_loss_cal)\n",
    "    train_sim_evl= SimpleEvaluator(args)\n",
    "    test_sim_evl= SimpleEvaluator(args)\n",
    "\n",
    "    training(epoch, trainer, train_sim_evl, test_sim_evl,train_loader,test_loader)\n",
    "    if plot:\n",
    "        train_sim_evl.plot_metrics()\n",
    "        test_sim_evl.plot_metrics()\n",
    "        \n",
    "    return train_sim_evl, test_sim_evl\n",
    "    \n",
    "# _, _ = train_cnn(va_train_dataloader, va_test_dataloader, epoch=100, plot=True)\n",
    "# train_cnn(inva_train_dataloader, inva_test_dataloader, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN:\n",
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "import models\n",
    "importlib.reload(models)\n",
    "\n",
    "def train_gnn(args, train_loader, test_loader, gnn_name='gnn', epoch=200, plot=False, node_fea_dim=1,\n",
    "              class_num=6, node_num=20, **xargs):\n",
    "        \n",
    "    def get_value(key, default=None):\n",
    "        return xargs[key] if key in xargs else default\n",
    "    \n",
    "    # use GNN to trian:\n",
    "    args.debug = False\n",
    "    utils.DLog.init(args)\n",
    "    \n",
    "    is_node_wise = get_value('is_node_wise', False)\n",
    "    is_regression = get_value('is_regression', False)\n",
    "    scaler = get_value('scaler')\n",
    "    opt = get_value('opt', 'sgd')\n",
    "    lr = xargs['lr'] if 'lr' in xargs else 0.0002\n",
    "    \n",
    "    \n",
    "    gnn_model = choose_model(args, gnn_name, node_fea_dim=node_fea_dim, class_num=class_num,\n",
    "                             node_num=node_num,\n",
    "                             node_wise=is_node_wise)\n",
    "    if opt == 'adam':\n",
    "        opt = optim.Adam(gnn_model.parameters(), lr=lr, weight_decay=args.weight_decay)\n",
    "    else:\n",
    "        opt = optim.SGD(gnn_model.parameters(), lr=lr)\n",
    "        \n",
    "    \n",
    "    \n",
    "    loss_cal = get_value('loss_cal', None)\n",
    "    if loss_cal is None:\n",
    "        loss_cal =  MAECal() if is_regression else CELossCal()\n",
    "    elif loss_cal == 'mse':\n",
    "        loss_cal = nn.MSELoss()\n",
    "    elif loss_cal == 'mae':\n",
    "        loss_cal = MAECal()\n",
    "        \n",
    "\n",
    "    trainer = utils.Trainer(gnn_model, optimizer=opt, loss_cal=loss_cal, scaler=scaler)\n",
    "    train_sim_evl= SimpleEvaluator(args, is_regression=is_regression)\n",
    "    test_sim_evl= SimpleEvaluator(args,is_regression=is_regression)\n",
    "\n",
    "    training(epoch, trainer, train_sim_evl, test_sim_evl, train_loader, test_loader)\n",
    "\n",
    "    if plot:\n",
    "        train_sim_evl.plot_metrics()\n",
    "        test_sim_evl.plot_metrics()\n",
    "        \n",
    "    return train_sim_evl, test_sim_evl, gnn_model\n",
    "\n",
    "# train_gnn(va_train_dataloader, va_test_dataloader, True)\n",
    "# train_gnn(inva_train_dataloader, inva_test_dataloader, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_node_feature(all_data, sparse, node_cons_func, **xargs) -> tuple:\n",
    "    train_adj ,_, test_adj, _ = all_data\n",
    "    if sparse:\n",
    "        train_node_feas = [node_cons_func(adj=adj, **xargs) for adj in train_adj]\n",
    "        test_node_feas = [node_cons_func(adj=adj, **xargs) for adj in test_adj]\n",
    "    else:\n",
    "        train_node_feas = np.stack([node_cons_func(adj=adj, **xargs) for adj in train_adj], axis=0)\n",
    "        test_node_feas = np.stack([node_cons_func(adj=adj, **xargs) for adj in test_adj], axis=0)\n",
    "    \n",
    "    return (train_node_feas, test_node_feas) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCA function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ppp(*values: object):\n",
    "    print('------------------------------')\n",
    "    print(*values)\n",
    "    \n",
    "\n",
    "def get_mid_feat(cur_model, dataloader, pooled=False, prediction=False, logit=True):\n",
    "    \n",
    "    \n",
    "    new_loader = DataLoader(dataloader.dataset, batch_size=10, shuffle=False, collate_fn=dataloader.dataset.collate)\n",
    "\n",
    "    if prediction:\n",
    "        cur_model.mid_fea = False\n",
    "        cur_model.eval()\n",
    "        \n",
    "        preds = []\n",
    "        for x, _ in new_loader:\n",
    "            x = x.cuda()\n",
    "            out = cur_model(x)\n",
    "            preds.append(out)\n",
    "        \n",
    "        preds = torch.cat(preds, dim=0).detach().cpu().numpy()\n",
    "        if logit:\n",
    "            print('preds shape', preds.shape)\n",
    "        return preds\n",
    "    else:\n",
    "        cur_model.mid_fea = True\n",
    "        cur_model.eval()\n",
    "        \n",
    "        mid_feas = []\n",
    "        for x, _ in new_loader:\n",
    "            x = x.cuda()\n",
    "            _, mid_f_train = cur_model(x)\n",
    "            mid_feas.append(mid_f_train)\n",
    "        mid_feas = torch.cat(mid_feas, dim=0)\n",
    "        if pooled:\n",
    "            mid_feas = cur_model.pooling(mid_feas).detach().cpu().numpy()\n",
    "        if logit:\n",
    "            print('mid_feas shape', mid_feas.shape)\n",
    "        return mid_feas\n",
    "\n",
    "\n",
    "\n",
    "def get_cca_corr(fea1, fea2, CCA=True):\n",
    "    if not isinstance(fea1, np.ndarray):\n",
    "        fea1 = fea1.cpu().numpy()\n",
    "    if not isinstance(fea2, np.ndarray):\n",
    "        fea2 = fea2.cpu().numpy()\n",
    "    \n",
    "    if CCA:\n",
    "        from sklearn.cross_decomposition import CCA\n",
    "        n_compo = 1\n",
    "        cca = CCA(n_components=n_compo, scale=True, max_iter=500, tol=1e-06, copy=True)\n",
    "        cca.fit(fea1, fea2)\n",
    "        X_c, Y_c = cca.transform(fea1, fea2)\n",
    "        r2 = cca.score(fea1, fea2)\n",
    "        print('r2:',r2)\n",
    "    else:\n",
    "        X_c = fea1\n",
    "        Y_c = fea2\n",
    "    # cal correaltion:\n",
    "    from scipy import stats\n",
    "    corr = stats.pearsonr(X_c.squeeze(), Y_c.squeeze())\n",
    "    print(corr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# task1 avg degree, ok\n",
    "# task2 avg CC ok.\n",
    "# task3 node-wise degree, ok\n",
    "# task4, node-wise CC, ok\n",
    "# task5, two label classification ok.\n",
    "# task6, b1b2, node-wise degree, -> b1b2: same degree, different CC\n",
    "# task7, b1b2, graph-wise degree \n",
    "# task8, b3b4, graph-wise degree,  -> same CC, different degree.\n",
    "# task9, b1b2, graph-wise CC\n",
    "# task10, b3b4, graph-wise CC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WS orthogonal labels b1b2 b3b4 compare CCA corr "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WS orthogonal b1b2 b3b4 compare CCA corr "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* based on above figure, pick same horizontal points\n",
    "- blueK=0.2, p=0.3, orangeK=0.25, p=0.35, greenK=0.3, p= 0.6,\n",
    "- redK=0.35,p=0.6, orangek=0.25, p=0.3 (try this first)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate WS graph node feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# construct node clustring coefficient label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate WS graph dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCA (Canical Component Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check whether GNN can learn the CC ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "g = nx.triangular_lattice_graph(4,4)\n",
    "\n",
    "# TODO: given degree, and CC generate ranwom graphs:\n",
    "\n",
    "def generate_degree_CC_graphs(node_num, d, cc):\n",
    "    # NOTE: use ws graph as bases:\n",
    "    \n",
    "    g = nx.watts_strogatz_graph(n=node_num, k=d, p=0.5)\n",
    "    \n",
    "    # get a center node, connect the neighbors:\n",
    "    # by adj matrix: get the maximum degree node v, idx = Nei(v), then Nei(idx) \\in idx, else: Nei(idx) \\ idx_complement.\n",
    "    \n",
    "    adj = nx.to_numpy_array(g)\n",
    "    print(adj.shape)\n",
    "    max_degree_id = -1\n",
    "    degrees = np.sum(adj, axis=0)\n",
    "    print(degrees)\n",
    "    return g\n",
    "    \n",
    "    \n",
    "g = generate_degree_CC_graphs(10, d=5, cc=0.5)\n",
    "    \n",
    "nx.draw_spring(g)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate CC historgram with same degree\n",
    "# cc without degree correlation bias\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "importlib.reload(utils)\n",
    "\n",
    "import networkx as nx\n",
    "from functools import reduce\n",
    "\n",
    "importlib.reload(models)\n",
    "\n",
    "def plot_cc_degree_distribution(cc_degree_graphs, cc_degree_y, g_label='train'):\n",
    "    \n",
    "    # random add edges:\n",
    "    # add E edges, repeat for 5 times.\n",
    "    print(type(cc_degree_graphs[0].todense()))\n",
    "\n",
    "    data_graphs = [(cc_degree_y[i],np.mean(np.sum(cc_degree_graphs[i].todense(), axis=1)), np.mean(node_tri_cycles_feature(adj=cc_degree_graphs[i])).item(), i) for i in range(len(cc_degree_graphs))]\n",
    "\n",
    "    data_graphs_s_train = sorted(data_graphs, key=lambda x: x[0])\n",
    "\n",
    "    ccs = [d[0] for d in data_graphs_s_train]\n",
    "    degrees = [d[1]/10 for d in data_graphs_s_train]\n",
    "    tri_cycles = [d[2]/4 for d in data_graphs_s_train]\n",
    "\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(g_label)\n",
    "    plt.plot(ccs, label='labels')\n",
    "    plt.plot(degrees, label='degree',  linestyle='--')\n",
    "    plt.plot(tri_cycles, label='tri_cycles', linestyle='-.')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixed cycle graphs:\n",
    "\n",
    "cc_train_adjs, cc_train_y, cc_test_adjs, cc_test_y = generate_cc_no_degree_corr_samples(cc_range_num=20)\n",
    "\n",
    "plot_cc_degree_distribution(cc_train_adjs, cc_train_y)\n",
    "plot_cc_degree_distribution(cc_test_adjs, cc_test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Circulant graphs\n",
    "# original paper: [2,3,4,5,6,9,11,12,13,16]\n",
    "S =[2,3,4,5,6,9,11,12,13,16]\n",
    "# S = list(np.arange(2, 10))\n",
    "S = [2, 3, 4]\n",
    "train_adjs, train_y, test_adjs, test_y = generate_circulant_graph_samples(each_class_num=40, N=41, S=S)\n",
    "# TODO: predict long cycles.\n",
    "plot_cc_degree_distribution(train_adjs, train_y)\n",
    "plot_cc_degree_distribution(test_adjs, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# construct CC without degree-correlation biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Shuffle, and then split.\n",
    "# cc_train_adjs, cc_train_y, cc_test_adjs, cc_test_y\n",
    "\n",
    "\n",
    "class NodeFeaRegister(object):\n",
    "    def __init__(self, file_path=None):\n",
    "        self.id = id(self)\n",
    "        self.file_path = file_path\n",
    "        if file_path is not None:\n",
    "            self.funcs = {} # TODO: load from file.\n",
    "            pass\n",
    "        else:\n",
    "            self.funcs = {\n",
    "                \"degree\":node_degree_feature,\n",
    "                \"allone\":node_allone_feature,\n",
    "                \"index_id\":node_index_feature,\n",
    "                \"guassian\":node_gaussian_feature,\n",
    "                \"tri_cycle\":node_tri_cycles_feature,\n",
    "                \"kadj\": node_k_adj_feature,\n",
    "                \"rand_id\":node_random_id_feature\n",
    "                }\n",
    "        self.registered = []\n",
    "        \n",
    "    def register(self, func_name, **xargs):\n",
    "        if func_name not in self.funcs:\n",
    "            raise NotImplementedError\n",
    "        self.registered.append((func_name, self.funcs[func_name], xargs))\n",
    "    \n",
    "    def get_registered(self):\n",
    "        return self.registered\n",
    "    \n",
    "    def list_registered(self):\n",
    "        for i, (name, _, arg) in enumerate(self.registered):\n",
    "            print('index:', i, name, ' args: ',arg)\n",
    "\n",
    "def construct_node_features(alldata, fea_register:NodeFeaRegister):\n",
    "    node_feature_list = []\n",
    "    for fea_reg in fea_register.get_registered():\n",
    "        node_feature_list.append(generate_node_feature(alldata, sparse=True, node_cons_func=fea_reg[1], **fea_reg[2]))\n",
    "    return node_feature_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: MixCycle graphs:\n",
    "\n",
    "# NOTE: different init node features affect the performance to discover topology !!!!\n",
    "# NOTE: try different random init node features or learnable node features.\n",
    "\n",
    "node_fea_reg = NodeFeaRegister()\n",
    "node_fea_reg.register('degree')\n",
    "node_fea_reg.register('allone')\n",
    "node_fea_reg.register('guassian')\n",
    "node_fea_reg.register('tri_cycle')\n",
    "node_fea_reg.register('kadj', k=2)\n",
    "\n",
    "node_features = construct_node_features((cc_train_adjs, None, cc_test_adjs, None), node_fea_reg)\n",
    "\n",
    "\n",
    "# NOTE: e.g., randomly choise from [1,2,3,4] or [1,2], or [1,2,3,4,...100], try different length.\n",
    "ratios = [0.1, 0.3,0.6, 1.0]\n",
    "rand_id_fea_trains = []\n",
    "rand_id_fea_tests = []\n",
    "node_fea_reg_ratio = NodeFeaRegister()\n",
    "for ra in ratios:\n",
    "    node_fea_reg_ratio.register('rand_id', ratio=ra)\n",
    "\n",
    "rand_id_fea_list = construct_node_features((cc_train_adjs, None, cc_test_adjs, None), node_fea_reg_ratio)\n",
    "\n",
    "cc_dataset = (cc_train_adjs, cc_train_y, cc_test_adjs, cc_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Circulant graphs:\n",
    "\n",
    "node_fea_reg = NodeFeaRegister()\n",
    "node_fea_reg.register('degree')\n",
    "node_fea_reg.register('allone')\n",
    "node_fea_reg.register('guassian')\n",
    "node_fea_reg.register('tri_cycle')\n",
    "node_fea_reg.register('index_id')\n",
    "node_fea_reg.register('kadj', k=3)\n",
    "\n",
    "node_features = construct_node_features((train_adjs, None, test_adjs, None), node_fea_reg)\n",
    "cc_dataset = (train_adjs, train_y, test_adjs, test_y)\n",
    "\n",
    "node_fea_reg.list_registered()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_cc_degree(args, cc_dataloader, feature_dim, lr=0.00001, gnn_name='gcn', epochs=300, opt='sgd'):\n",
    "    import models\n",
    "    import utils\n",
    "    importlib.reload(utils)\n",
    "    importlib.reload(models)\n",
    "\n",
    "\n",
    "    cc_gnn_evls_train = []\n",
    "    cc_gnn_evls_test = []\n",
    "\n",
    "    # TODO: fix the normali\n",
    "    # ws_degree_graphwise_allone_dataloaders\n",
    "    train_dataloader, test_dataloader = cc_dataloader\n",
    "    train_evl, test_evl, cur_model = train_gnn(args, train_dataloader, test_dataloader, gnn_name=gnn_name,\n",
    "                                    epoch=epochs,\n",
    "                                    node_fea_dim=feature_dim,\n",
    "                                    class_num=1,\n",
    "                                    node_num=40, lr=lr, is_regression=True, is_node_wise=False, opt=opt)\n",
    "    \n",
    "    cc_gnn_evls_train.append(train_evl)\n",
    "    cc_gnn_evls_test.append(test_evl)\n",
    "        \n",
    "    cc_gnn_evls_train[0].plot_metrics()\n",
    "    cc_gnn_evls_test[0].plot_metrics()\n",
    "    return cur_model\n",
    "\n",
    "\n",
    "def plot_cc_degree_prediction(models_trained, cc_dataloader, adjs, adj_degree, g_label):\n",
    "    \n",
    "    data_graphs = [(adj_degree[i],np.mean(np.sum(adjs[i].todense(), axis=1)), np.mean(node_tri_cycles_feature(adj=adjs[i])).item(), i) for i in range(len(adjs))]\n",
    "\n",
    "    data_graphs_s_train = sorted(data_graphs, key=lambda x: x[0])\n",
    "\n",
    "    ccs = [d[0] for d in data_graphs_s_train]\n",
    "    degrees = [d[1]/10 for d in data_graphs_s_train]\n",
    "    tri_cycles = [d[2]/4 for d in data_graphs_s_train]\n",
    "\n",
    "    cc_train_preds  = get_mid_feat(models_trained, cc_dataloader, prediction=True, logit=False)\n",
    "\n",
    "    cc_train_preds = [cc_train_preds[i[-1]].item() for i in data_graphs_s_train]\n",
    "    \n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(ccs, label='CC labels')\n",
    "    plt.plot(cc_train_preds, label='CC predictions')\n",
    "    plt.plot(degrees, label='Degrees')\n",
    "    plt.plot(tri_cycles, label='tri_cycles')\n",
    "    plt.title(g_label)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: test different ratios of random number node features. (08.29)\n",
    "* ratios = [0.1, 0.3,0.6, 1.0]\n",
    "* test correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_dataloaders = []\n",
    "for i in range(len(rand_id_fea_trains)):\n",
    "    com_node_features_train = composite_node_features(allone_train_norm, rand_id_fea_trains[i], padding=False)\n",
    "    com_node_features_test = composite_node_features(allone_test_norm, rand_id_fea_tests[i], padding=False)\n",
    "    cc_dataloaders.append(assemble_dataloader(*construct_dataset(cc_dataset,\n",
    "                            (com_node_features_train, com_node_features_test),\n",
    "                             y_torch_type=torch.FloatTensor, sparse=True), cuda=False))\n",
    "\n",
    "\n",
    "kadj_fea_train = composite_node_features(allone_train_norm, kadj_train, padding=True, padding_len=84)\n",
    "kadj_fea_test = composite_node_features(allone_test_norm, kadj_test, padding=True, padding_len=84)\n",
    "cc_dataloaders.append(assemble_dataloader(*construct_dataset(cc_dataset,\n",
    "                        (kadj_fea_train, kadj_fea_test),\n",
    "                            y_torch_type=torch.FloatTensor, sparse=True), cuda=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: 0906. only all one and index features. only index and k=2 cannot handle [2,3,4],TODO: try k=3, add kadj features.\n",
    "# NOTE: 0907. Adam can converge...\n",
    "composed_fea_train = composite_node_features(node_features[5][0], node_features[4][0], padding=False)\n",
    "composed_fea_test = composite_node_features(node_features[5][1], node_features[4][1], padding=False)\n",
    "cc_dataloaders = []\n",
    "\n",
    "# composed_fea_train = node_features[4][0]\n",
    "# composed_fea_test = node_features[4][1]\n",
    "\n",
    "train_dataset, test_dataset, scalers, scalers_y = construct_dataset(cc_dataset, (composed_fea_train, composed_fea_test), \n",
    "                       y_torch_type=torch.FloatTensor, sparse=True, K=2)\n",
    "\n",
    "cc_dataloaders.append(assemble_dataloader(train_dataset, test_dataset , cuda=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check the normalization of labels and features:\n",
    "\n",
    "x_check=[]\n",
    "y_check=[]\n",
    "\n",
    "for y in cc_dataloaders[0][0].dataset.y:\n",
    "    y_check.append(y.item())\n",
    "y_check= sorted(y_check)\n",
    "plt.figure()\n",
    "plt.plot(y_check)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: set model hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gin_args = utils.DaoArgs()\n",
    "gin_args.gnn_hid_dim=128\n",
    "gin_args.gnn_layer_num=3\n",
    "gin_args.set_attr('bi_direct', False)\n",
    "gin_args.rep_fea_dim=64 # gin output dimension for linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Circulant graphs, 09.06, only index features.\n",
    "# NOTE: mean pool\n",
    "train_y_norm = scalers_y[0].transform(train_y)\n",
    "test_y_norm = scalers_y[1].transform(test_y)\n",
    "\n",
    "for cc_dataloader in cc_dataloaders:\n",
    "    fea_len = cc_dataloader[0].dataset.x[0].get_node_features().shape[-1]\n",
    "    print('xxfeature len:', fea_len)\n",
    "    models_trained = train_cc_degree(gin_args, cc_dataloader, feature_dim=fea_len, lr=0.001,\n",
    "                                     gnn_name='sparse_gin',epochs=500, opt='adam')\n",
    "    plot_cc_degree_prediction(models_trained, cc_dataloader[0], train_adjs, train_y_norm, 'training dataset')\n",
    "    plot_cc_degree_prediction(models_trained, cc_dataloader[1], test_adjs, test_y_norm, 'testing dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_norm = scalers_y[0].transform(train_y)\n",
    "test_y_norm = scalers_y[1].transform(test_y)\n",
    "\n",
    "\n",
    "\n",
    "plot_cc_degree_prediction(models_trained, cc_dataloader[0], train_adjs, train_y_norm, 'training dataset')\n",
    "plot_cc_degree_prediction(models_trained, cc_dataloader[1], test_adjs, test_y_norm, 'testing dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Circulant graphs, 09.04, try cycles features.\n",
    "# NOTE: mean pool\n",
    "# TODO: try onehot features and labels.\n",
    "\n",
    "# NOTE: only all one and index features\n",
    "tri_fea_train = composite_node_features(node_features[2][0], node_features[3][0], padding=False)\n",
    "tri_fea_test = composite_node_features(node_features[2][1], node_features[3][1], padding=False)\n",
    "cc_dataloaders = []\n",
    "cc_dataloaders.append(assemble_dataloader(\n",
    "    *construct_dataset(cc_dataset, (tri_fea_train, tri_fea_test), \n",
    "                       y_torch_type=torch.FloatTensor, sparse=True), cuda=False))\n",
    "\n",
    "\n",
    "for cc_dataloader in cc_dataloaders:\n",
    "    fea_len = cc_dataloader[0].dataset.x[0].get_node_features().shape[-1]\n",
    "    print('xxfeature len:', fea_len)\n",
    "    models_trained = train_cc_degree(cc_dataloader, feature_dim=fea_len, lr=0.0002, gnn_name='sparse_gin', epochs=500)\n",
    "    plot_cc_degree_prediction(models_trained, cc_dataloader[0], train_adjs, train_y, 'training dataset')\n",
    "    plot_cc_degree_prediction(models_trained, cc_dataloader[1], test_adjs, test_y, 'testing dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE 1:  try $A^2 \\odot {mask_i, i = 1..M}$ as input features, where $mask_i \\in R^{1 \\times N}$ is a learnable vector.\n",
    "\n",
    "* mask the padded features, so N=128, set M=3.\n",
    "* align mask features in the GNNAdapter module.\n",
    "* Done, useless using mask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try $A^2$ as the edge_index, 2022.09.02\n",
    "* good performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: only allone_train\n",
    "cc_dataloaders = []\n",
    "cc_dataloaders.append(assemble_dataloader(\n",
    "    *construct_dataset(cc_dataset,\n",
    "                        (allone_train_norm, allone_test_norm), y_torch_type=torch.FloatTensor, sparse=True, K=2), \n",
    "    cuda=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_trained = train_cc_degree(cc_dataloaders[0], feature_dim=1, lr=0.001, gnn_name='sparse_gin', epochs=1000)\n",
    "\n",
    "plot_cc_degree_prediction(models_trained, cc_dataloaders[0])\n",
    "\n",
    "# convergence speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try A^3, K=3\n",
    "* bad performance\n",
    "* so only K=2 can count the cycles so that it further predict CC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: only allone_train\n",
    "\n",
    "cc_dataloaders = []\n",
    "cc_dataloaders.append(assemble_dataloader(\n",
    "    *construct_dataset(cc_dataset,\n",
    "                        (allone_train_norm, allone_test_norm), y_torch_type=torch.FloatTensor, sparse=True, K=3), \n",
    "    cuda=False))\n",
    "\n",
    "models_trained = train_cc_degree(cc_dataloaders[0], feature_dim=1, lr=0.0012, gnn_name='sparse_gin', epochs=1000)\n",
    "\n",
    "plot_cc_degree_prediction(models_trained, cc_dataloaders[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NOTE: Kadj as features:\n",
    "\n",
    "pad_len = 128\n",
    "\n",
    "com_node_features_train = composite_node_features(allone_train, kadj_train, padding=True, padding_len=pad_len)\n",
    "com_node_features_test = composite_node_features(allone_test, kadj_test, padding=True, padding_len=pad_len)\n",
    "\n",
    "cc_dataloaders = []\n",
    "cc_dataloaders.append(assemble_dataloader(\n",
    "    *construct_dataset(cc_dataset,\n",
    "                        (com_node_features_train, com_node_features_test), y_torch_type=torch.FloatTensor, sparse=True), cuda=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train corr(degree, CC) = 0:\n",
    "# NOTE: try mask features with k_adj. 2022.08.28, Sun.\n",
    "# NOTE: try gcn/gin with k_adj_features. \n",
    "# NOTE: mean pool\n",
    "\n",
    "models_trained = train_cc_degree(cc_dataloaders, feature_dim=128, lr=0.001, gnn_name='sparse_gin_mask',epochs=300)\n",
    "\n",
    "plot_cc_degree_prediction(models_trained, cc_dataloaders)\n",
    "\n",
    "\n",
    "# NOTE: small lr converges to local optimal, try only kadj later with a larger lr.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# com_node_features_train = composite_node_features(allone_train, kadj_train[0], padding=False, padding_len=128)\n",
    "# com_node_features_test = composite_node_features(allone_test, kadj_test[0], padding=False, padding_len=128)\n",
    "\n",
    "com_node_features_train = composite_node_features(allone_train, tri_cycles_train[0], padding=False, padding_len=128)\n",
    "com_node_features_test = composite_node_features(allone_test, tri_cycles_test[0], padding=False, padding_len=128)\n",
    "\n",
    "cc_dataloaders = []\n",
    "cc_dataloaders.append(assemble_dataloader(\n",
    "    *construct_dataset(cc_dataset,\n",
    "                        (com_node_features_train, com_node_features_test), y_torch_type=torch.FloatTensor, sparse=True), cuda=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train corr(degree, CC) = 0:\n",
    "# NOTE: node_fea_dim = 128, add k_adj_features. 2022.08.23, failed\n",
    "# NOTE: node_fea_dim = 2, add tri_cycles. 2022.08.26, running\n",
    "# NOTE: try gcn/gin with k_adj_features.\n",
    "# NOTE: mean pool\n",
    "\n",
    "models_trained = train_cc_degree(cc_dataloaders, feature_dim=2, lr=0.00001, gnn_name='sparse_gin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result of GCN:\n",
    "\n",
    "# NOTE: node_fea_dim = 128, add k_adj_features. 2022.08.23\n",
    "# NOTE: mean pool\n",
    "# NOTE: node feature mean: 0.2\n",
    "    \n",
    "plot_cc_degree_prediction(models_trained, cc_dataloaders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: kadj features without mask\n",
    "com_node_features_train = composite_node_features(allone_train, kadj_train[0], padding=True, padding_len=128)\n",
    "com_node_features_test = composite_node_features(allone_test, kadj_test[0], padding=True, padding_len=128)\n",
    "\n",
    "cc_dataloaders = []\n",
    "cc_dataloaders.append(assemble_dataloader(\n",
    "    *construct_dataset(cc_dataset,\n",
    "                        (com_node_features_train, com_node_features_test), y_torch_type=torch.FloatTensor, sparse=True), cuda=False))\n",
    "\n",
    "\n",
    "\n",
    "# allone_train[0]\n",
    "print(cc_dataloaders[0][0].dataset.x[0].get_node_features().shape)\n",
    "\n",
    "print(cc_dataloaders[0][0].dataset.x[0].get_node_features()[0, 75:88])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train corr(degree, CC) = 0:\n",
    "# NOTE: node_fea_dim = 128, add k_adj_features. 2022.08.27, run again\n",
    "# NOTE: try gcn/gin with k_adj_features.\n",
    "# NOTE: mean pool\n",
    "\n",
    "\n",
    "models_trained = train_cc_degree(cc_dataloaders, feature_dim=128, lr=0.001, gnn_name='sparse_gin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: node_fea_dim = 128, add k_adj_features. 2022.08.23\n",
    "# NOTE: mean pool\n",
    "\n",
    "# NOTE: node feature mean: 0.2\n",
    "plot_cc_degree_prediction(models_trained, cc_dataloaders)\n",
    "\n",
    "# NOTE: !!!!!!!! mask is useless !!!!!!!!!!!!!\n",
    "# NOTE: !!!!!!!! cycles are useless too !!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: only all one features\n",
    "\n",
    "cc_dataloaders = []\n",
    "cc_dataloaders.append(assemble_dataloader(\n",
    "    *construct_dataset(cc_dataset,\n",
    "                        (allone_train, allone_test), y_torch_type=torch.FloatTensor, sparse=True), cuda=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: only all one features, try larger lr, 2022.08.28\n",
    "models_trained = train_cc_degree(cc_dataloaders, feature_dim=1, lr=0.0005, gnn_name='sparse_gin', epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: only all one features\n",
    "plot_cc_degree_prediction(models_trained, cc_dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: only all one and index features\n",
    "index_fea_train,  index_fea_test =  generate_node_feature([(cc_train_adjs, None, cc_test_adjs, None)] ,node_index_feature,\n",
    "                                                   sparse=True)\n",
    "\n",
    "com_fea_train = composite_node_features(allone_train, index_fea_train[0], padding=False)\n",
    "com_fea_test = composite_node_features(allone_test, index_fea_test[0], padding=False)\n",
    "\n",
    "cc_dataloaders = []\n",
    "cc_dataloaders.append(assemble_dataloader(\n",
    "    *construct_dataset(cc_dataset,\n",
    "                        (com_fea_train, com_fea_test), y_torch_type=torch.FloatTensor, sparse=True), cuda=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models_trained = train_cc_degree(cc_dataloaders, feature_dim=2, lr=0.001, gnn_name='sparse_gin', epochs=300)\n",
    "\n",
    "# with \n",
    "\n",
    "# NOTE: only all one features and index features.\n",
    "plot_cc_degree_prediction(models_trained, cc_dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: only all one and degree features\n",
    "degree_fea_train,  degree_fea_test =  generate_node_feature([(cc_train_adjs, None, cc_test_adjs, None)] ,node_degree_feature,\n",
    "                                                   sparse=True)\n",
    "\n",
    "com_fea_train = composite_node_features(allone_train, degree_fea_train[0], padding=False)\n",
    "com_fea_test = composite_node_features(allone_test, degree_fea_test[0], padding=False)\n",
    "\n",
    "cc_dataloaders = []\n",
    "cc_dataloaders.append(assemble_dataloader(\n",
    "    *construct_dataset(cc_dataset,\n",
    "                        (com_fea_train, com_fea_test), y_torch_type=torch.FloatTensor, sparse=True), cuda=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models_trained = train_cc_degree(cc_dataloaders, feature_dim=2, lr=0.001, gnn_name='sparse_gin', epochs=300)\n",
    "\n",
    "# with \n",
    "\n",
    "# NOTE: only all one features and degree features\n",
    "plot_cc_degree_prediction(models_trained, cc_dataloaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Compare the CCA of the features of these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check the correctness of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check the density?\n",
    "\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(3, 6, figsize=(8, 4), dpi=300)\n",
    "\n",
    "train_dl, _ = dataloaders[5]\n",
    "x_samples = train_dl.dataset.x\n",
    "print(type(x_samples[0]))\n",
    "\n",
    "for i in range(axes.shape[0]):\n",
    "    for j in range(axes.shape[1]):\n",
    "        index =  int(each_class_num) * j + i\n",
    "        print('index of sample:', index)\n",
    "        sns.heatmap(x_samples[index].A.cpu().numpy(), cbar=False, linecolor='indigo', square=True, linewidths=0.3,\n",
    "                         cmap=ListedColormap(['white', 'purple']), ax=axes[i, j])\n",
    "        axes[i, j].set_xticks([])\n",
    "        axes[i, j].set_yticks([])\n",
    "\n",
    "plt.axis('off')\n",
    "# ax.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot f1 curves:\n",
    "\n",
    "def get_f1s(evls):\n",
    "    mi_f1 = []\n",
    "    ma_f1 = []\n",
    "    w_f1 = []\n",
    "\n",
    "    for evl in evls:\n",
    "        mi_f1.append(evl.total_metrics['micro_f1'])\n",
    "        ma_f1.append(evl.total_metrics['macro_f1'])\n",
    "        w_f1.append(evl.total_metrics['weighted_f1'])\n",
    "    return mi_f1, ma_f1, w_f1\n",
    "\n",
    "    \n",
    "    \n",
    "def plot_f1_curves(mi_f1, ma_f1, w_f1):\n",
    "    plt.figure(figsize=(4, 3), dpi=150)\n",
    "\n",
    "\n",
    "    x = np.linspace(0, 1, 24)\n",
    "    plt.plot(x, mi_f1,  marker=\"8\")\n",
    "    plt.plot(x, ma_f1,  marker=11)\n",
    "    ax = plt.axes()\n",
    "  \n",
    "# Setting the background color of the plot \n",
    "# using set_facecolor() method\n",
    "    ax.set_facecolor(\"snow\")\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot performance of WS classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_f1, ma_f1, w_f1 = get_f1s(ws_gnn_evls)\n",
    "\n",
    "print(mi_f1[0])\n",
    "plot_f1_curves(mi_f1, ma_f1, w_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine wanted node features\n",
    "\n",
    "# case: 1 only use index feas.\n",
    "\n",
    "# case: 2, add allone vector can detect structural information:\n",
    "\n",
    "train_combined_feature = composite_node_features(train_node_index_feas, train_node_allone_feas, train_node_std_feas)\n",
    "test_combined_feature = composite_node_features(test_node_index_feas, test_node_allone_feas, test_node_std_feas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pos_en_dim = 10\n",
    "\n",
    "dataloaders = []\n",
    "for i, d in enumerate(data_sim):\n",
    "    dataloaders.append(assemble_dataloader(\n",
    "        *construct_dataset(d,\n",
    "                           (train_combined_feature[i], test_combined_feature[i]), \n",
    "                                                              lap_encode=True,\n",
    "                                                              lap_en_dim=pos_en_dim)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: set the feature dimensions:\n",
    "\n",
    "node_feature_dim = train_combined_feature.shape[-1]\n",
    "graph_feature_dim = 1\n",
    "\n",
    "print('node fea dim:', node_feature_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try again TODO: plot the performance curve.\n",
    "\n",
    "# 2. train by each method:\n",
    "\n",
    "# MLP:\n",
    "\n",
    "mlp_evls = []\n",
    "\n",
    "for train_dataloader, test_dataloader in dataloaders:\n",
    "    _, test_evl = train_mlp(train_dataloader, test_dataloader, epoch=100)\n",
    "    mlp_evls.append(test_evl)\n",
    "\n",
    "\n",
    "mi_f1, ma_f1, w_f1 = get_f1s(mlp_evls)\n",
    "\n",
    "plot_f1_curves(mi_f1, ma_f1, w_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train LSPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "\n",
    "\n",
    "def train_lspe(train_loader, test_loader, node_fea_dim, epoch=1, plot=False, model_name='lspe', graph_fea_dim=1):\n",
    "# use CNN to trian:\n",
    "    args = base_args()\n",
    "    lspe_model = choose_model(model_name, node_fea_dim=node_fea_dim, graph_fea_dim=graph_fea_dim)\n",
    "    # opt = optim.Adam(mlp_model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    opt = optim.SGD(lspe_model.parameters(), lr=0.0001)\n",
    "    ce_loss_cal = CELossCal()\n",
    "\n",
    "    trainer = utils.Trainer(lspe_model, optimizer=opt, loss_cal=ce_loss_cal)\n",
    "    train_sim_evl= SimpleEvaluator(args)\n",
    "    test_sim_evl= SimpleEvaluator(args)\n",
    "\n",
    "    training(epoch, trainer, train_sim_evl, test_sim_evl,train_loader,test_loader)\n",
    "    if plot:\n",
    "        train_sim_evl.plot_metrics()\n",
    "        test_sim_evl.plot_metrics()\n",
    "        \n",
    "    return train_sim_evl, test_sim_evl\n",
    "    \n",
    "# t_dl, v_dl = dataloaders[0]\n",
    "# _, _ = train_lspe(t_dl, v_dl, epoch=100, plot=True)\n",
    "# train_cnn(inva_train_dataloader, inva_test_dataloader, True)\n",
    "\n",
    "# node feature dim=3, added std feature\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train LSPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: LSPE:\n",
    "# NOTE: 6 classification\n",
    "\n",
    "val_lspe_evls = []\n",
    "train_lspe_evls = []\n",
    "for train_dataloader, test_dataloader in dataloaders:\n",
    "    train_evl, test_evl = train_lspe(train_dataloader, test_dataloader, node_fea_dim=node_feature_dim, model_name='lspe', epoch=100)\n",
    "    val_lspe_evls.append(test_evl)\n",
    "    train_lspe_evls.append(train_evl)\n",
    "    \n",
    "mi_f1, ma_f1, w_f1 = get_f1s(val_lspe_evls)\n",
    "\n",
    "plot_f1_curves(mi_f1, ma_f1, w_f1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(12).reshape(3, 4)\n",
    "a.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_evls = []\n",
    "for train_dataloader, test_dataloader in dataloaders:\n",
    "    train_evl, test_evl = train_cnn(train_dataloader, test_dataloader, epoch=100, graph_fea_dim=graph_feature_dim)\n",
    "    cnn_evls.append((train_evl,test_evl))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node feature dim=3, added std feature\n",
    "\n",
    "train_cnn_evls = [e[0] for e in cnn_evls]\n",
    "test_cnn_evls = [e[1] for e in cnn_evls]\n",
    "\n",
    "mi_f1, ma_f1, w_f1 = get_f1s(train_cnn_evls)\n",
    "\n",
    "plot_f1_curves(mi_f1, ma_f1, w_f1)\n",
    "\n",
    "\n",
    "mi_f1, ma_f1, w_f1 = get_f1s(test_cnn_evls)\n",
    "\n",
    "plot_f1_curves(mi_f1, ma_f1, w_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node feature dim=2\n",
    "\n",
    "train_cnn_evls = [e[0] for e in cnn_evls]\n",
    "test_cnn_evls = [e[1] for e in cnn_evls]\n",
    "\n",
    "mi_f1, ma_f1, w_f1 = get_f1s(train_cnn_evls)\n",
    "\n",
    "plot_f1_curves(mi_f1, ma_f1, w_f1)\n",
    "\n",
    "\n",
    "mi_f1, ma_f1, w_f1 = get_f1s(test_cnn_evls)\n",
    "\n",
    "plot_f1_curves(mi_f1, ma_f1, w_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion:\n",
    "# visualize the train loader:\n",
    "\n",
    "# node feature dim=2\n",
    "\n",
    "\n",
    "train_cnn_evls[4].plot_metrics()\n",
    "\n",
    "\n",
    "train_dl, _ = dataloaders[4]\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(8, 6), dpi=150)\n",
    "\n",
    "x_samples = train_dl.dataset.x\n",
    "print(x_samples[0][1].shape)\n",
    "\n",
    "for i in range(axes.shape[0]):\n",
    "    for j in range(axes.shape[1]):\n",
    "        index =  4*i+j\n",
    "        print('index of sample:', index)\n",
    "        sns.heatmap(x_samples[index][1].cpu().numpy(), cbar=False, linecolor='indigo', square=True, linewidths=0.3,\n",
    "                         cmap=ListedColormap(['white', 'purple']), ax=axes[i, j])\n",
    "        axes[i, j].set_xticks([])\n",
    "        axes[i, j].set_yticks([])\n",
    "\n",
    "plt.axis('off')\n",
    "# ax.grid()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train GIN with lspe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use degree node features:\n",
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "args = utils.get_common_args()\n",
    "\n",
    "args = args.parse_args({})\n",
    "args.pos_en = 'lap_pe'\n",
    "args.pos_en_dim = pos_en_dim\n",
    "\n",
    "gnn_evls = []\n",
    "for train_dataloader, test_dataloader in dataloaders:\n",
    "    train_evl, test_evl = train_gnn(args, train_dataloader, test_dataloader, gnn_name='gin', epoch=200, node_fea_dim=node_feature_dim)\n",
    "    gnn_evls.append(test_evl)\n",
    "    \n",
    "# node feature dim=3, added std feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mi_f1, ma_f1, w_f1 = get_f1s(gnn_evls)\n",
    "\n",
    "plot_f1_curves(mi_f1, ma_f1, w_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train GNN without laspe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use degree node features:\n",
    "\n",
    "gnn_evls = []\n",
    "for train_dataloader, test_dataloader in dataloaders:\n",
    "    train_evl, test_evl = train_gnn(train_dataloader, test_dataloader, gnn_name='gnn', epoch=200, node_fea_dim=node_feature_dim)\n",
    "    gnn_evls.append(test_evl)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node feature dim=3, added std feature\n",
    "\n",
    "mi_f1, ma_f1, w_f1 = get_f1s(gnn_evls)\n",
    "\n",
    "plot_f1_curves(mi_f1, ma_f1, w_f1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node feature dim=2\n",
    "\n",
    "mi_f1, ma_f1, w_f1 = get_f1s(gnn_evls)\n",
    "\n",
    "plot_f1_curves(mi_f1, ma_f1, w_f1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion:\n",
    "# plot loss function:\n",
    "\n",
    "evl = gnn_evls[10]\n",
    "evl.plot_metrics()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP gin:\n",
    "# NOTE: 6 classification\n",
    "\n",
    "gin_evls = []\n",
    "train_gin_evls = []\n",
    "for train_dataloader, test_dataloader in dataloaders:\n",
    "    train_evl, test_evl = train_gnn(train_dataloader, test_dataloader,\n",
    "                                    node_fea_dim=node_feature_dim, gnn_name='gin', epoch=100)\n",
    "    gin_evls.append(test_evl)\n",
    "    train_gin_evls.append(train_evl)\n",
    "    \n",
    "mi_f1, ma_f1, w_f1 = get_f1s(gin_evls)\n",
    "\n",
    "plot_f1_curves(mi_f1, ma_f1, w_f1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion:\n",
    "# plot loss function:\n",
    "\n",
    "evl = gin_evls[15]\n",
    "evl.plot_metrics()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP gin:\n",
    "# NOTE: 6 classification, \n",
    "\n",
    "# NOTE node feature dim=3, added std feature\n",
    "\n",
    "gin_evls = []\n",
    "train_gin_evls = []\n",
    "for train_dataloader, test_dataloader in dataloaders:\n",
    "    train_evl, test_evl = train_gnn(train_dataloader, test_dataloader, \n",
    "                                    node_fea_dim=node_feature_dim, gnn_name='gin', epoch=100)\n",
    "    gin_evls.append(test_evl)\n",
    "    train_gin_evls.append(train_evl)\n",
    "    \n",
    "mi_f1, ma_f1, w_f1 = get_f1s(gin_evls)\n",
    "\n",
    "plot_f1_curves(mi_f1, ma_f1, w_f1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Di GIN\n",
    "# NOTE:use both direction:\n",
    "# NOTE: 6 classification, 3 features, direct\n",
    "# NOTE: added lap_en:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use both direction:\n",
    "# NOTE: 6 classification, 3 features, direct\n",
    "# NOTE: added lap_en:\n",
    " \n",
    "\n",
    "args = utils.get_common_args()\n",
    "\n",
    "args = args.parse_args({})\n",
    "args.pos_en = 'lap_pe'\n",
    "args.pos_en_dim = pos_en_dim\n",
    "\n",
    "gin_evls = []\n",
    "train_evls = []\n",
    "for train_dataloader, test_dataloader in dataloaders:\n",
    "    train_evl, test_evl = train_gnn(args, train_dataloader, test_dataloader,\n",
    "                                    gnn_name='gin_direc', epoch=100, node_fea_dim=node_feature_dim)\n",
    "    gin_evls.append(test_evl)\n",
    "    train_evls.append(train_evl)\n",
    "    \n",
    "mi_f1, ma_f1, w_f1 = get_f1s(gin_evls)\n",
    "\n",
    "plot_f1_curves(mi_f1, ma_f1, w_f1)\n",
    "\n",
    "\n",
    "evl = gin_evls[1]\n",
    "evl.plot_metrics()\n",
    "\n",
    "evl = gin_evls[10]\n",
    "evl.plot_metrics()\n",
    "\n",
    "evl = gin_evls[15]\n",
    "evl.plot_metrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use both direction:\n",
    "# NOTE: 6 classification\n",
    "gin_evls = []\n",
    "train_evls = []\n",
    "for train_dataloader, test_dataloader in dataloaders:\n",
    "    train_evl, test_evl = train_gnn(train_dataloader, test_dataloader,gnn_name='gin_direc', epoch=100, node_fea_dim=node_feature_dim)\n",
    "    gin_evls.append(test_evl)\n",
    "    train_evls.append(train_evl)\n",
    "    \n",
    "mi_f1, ma_f1, w_f1 = get_f1s(gin_evls)\n",
    "\n",
    "plot_f1_curves(mi_f1, ma_f1, w_f1)\n",
    "\n",
    "\n",
    "evl = gin_evls[1]\n",
    "evl.plot_metrics()\n",
    "\n",
    "evl = gin_evls[10]\n",
    "evl.plot_metrics()\n",
    "\n",
    "evl = gin_evls[15]\n",
    "evl.plot_metrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use both direction:\n",
    "# NOTE: 6 classification\n",
    "# NOTE: 3 node features !!\n",
    "\n",
    "gin_evls = []\n",
    "train_evls = []\n",
    "for train_dataloader, test_dataloader in dataloaders:\n",
    "    train_evl, test_evl = train_gnn(train_dataloader, test_dataloader,gnn_name='gin_direc', epoch=100, node_fea_dim=node_feature_dim)\n",
    "    gin_evls.append(test_evl)\n",
    "    train_evls.append(train_evl)\n",
    "    \n",
    "mi_f1, ma_f1, w_f1 = get_f1s(gin_evls)\n",
    "\n",
    "plot_f1_curves(mi_f1, ma_f1, w_f1)\n",
    "\n",
    "\n",
    "evl = gin_evls[1]\n",
    "evl.plot_metrics()\n",
    "\n",
    "evl = gin_evls[10]\n",
    "evl.plot_metrics()\n",
    "\n",
    "evl = gin_evls[15]\n",
    "evl.plot_metrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: concate the allone features and index features, not together training.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try laplacian\n",
    "# construct new features:\n",
    "\n",
    "\n",
    "utils.calculate_normalized_laplacian()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_kernel_cnn_evls = []\n",
    "for train_dataloader, test_dataloader in dataloaders:\n",
    "    _, test_evl = train_cnn(train_dataloader, test_dataloader, epoch=200, cnn_name='cnn_big')\n",
    "    big_kernel_cnn_evls.append(test_evl)\n",
    "    \n",
    "mi_f1, ma_f1, w_f1 = get_f1s(big_kernel_cnn_evls)\n",
    "\n",
    "plot_f1_curves(mi_f1, ma_f1, w_f1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: 6 class:\n",
    "\n",
    "big_kernel_cnn_evls = []\n",
    "cnn_train_evls = []\n",
    "for train_dataloader, test_dataloader in dataloaders:\n",
    "    train_evl, test_evl = train_cnn(train_dataloader, test_dataloader, epoch=200, cnn_name='cnn_big')\n",
    "    big_kernel_cnn_evls.append(test_evl)\n",
    "    cnn_train_evls.append(train_evl)\n",
    "    \n",
    "mi_f1, ma_f1, w_f1 = get_f1s(big_kernel_cnn_evls)\n",
    "\n",
    "plot_f1_curves(mi_f1, ma_f1, w_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Real-world dataset from PyG generic datasets\n",
    "website: `https://pytorch-geometric.readthedocs.io/en/latest/notes/data_cheatsheet.html`\n",
    "\n",
    "* graph classification:\n",
    "    * TUDataset\n",
    "    * ZINC\n",
    "\n",
    "## TODO: \n",
    "1. use. # due: 6.30.\n",
    "2. profile. # due: 7.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 1. how to use them.\n",
    "# classTUDataset\n",
    "\n",
    "\n",
    "import torch_geometric.datasets as pygdataset\n",
    "\n",
    "\n",
    "tudataset = pygdataset.tu_dataset.TUDataset(root='/li_zhengdao/github/GenerativeGNN/dataset/', name='AIDS')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tudataset.get(1)\n",
    "a.y\n",
    "print(type(a))\n",
    "\n",
    "# TODO: transform to BaseGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "import importlib\n",
    "from models import BaseGraphUtils\n",
    "importlib.reload(models)\n",
    "\n",
    "\n",
    "refresh_import()\n",
    "\n",
    "tu_base_graphs = []\n",
    "for a in tudataset:\n",
    "    tu_base_graphs.append(BaseGraphUtils.from_pyg_graph(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tu_y = []\n",
    "for g in tu_base_graphs:\n",
    "    tu_y.append(g.label)\n",
    "tu_y = torch.stack(tu_y, dim=0).squeeze()\n",
    "\n",
    "train_tu_dataset = GraphDataset(x=tu_base_graphs, y=torch.LongTensor(tu_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import importlib\n",
    "import models\n",
    "from models import GraphDataset\n",
    "\n",
    "importlib.reload(utils)\n",
    "importlib.reload(models)\n",
    "\n",
    "\n",
    "train_x, train_y, test_x, test_y = utils.random_split_dataset(train_tu_dataset, [0.8, 0.2])\n",
    "\n",
    "print(len(train_x))\n",
    "from collections import Counter\n",
    "tr_y = Counter(train_y.numpy())\n",
    "print(tr_y)\n",
    "\n",
    "tu_train_dataset = GraphDataset(x=train_x, y=train_y)\n",
    "tu_test_dataset = GraphDataset(x=test_x, y=test_y)\n",
    "\n",
    "tu_train_dataloader, tu_test_dataloader = assemble_dataloader(tu_train_dataset, tu_test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg =train_tu_dataset.x[0]\n",
    "print(gg.graph_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: torch.LongTensor(0) will always fail !!!!!!!!!!!!!!!!!\n",
    "a = torch.from_numpy(np.array([0.0])).long()\n",
    "d = torch.LongTensor(1).repeat(10).reshape(10, 1)\n",
    "b = np.repeat(np.array([0]), 11)\n",
    "c =torch.from_numpy(b).long().reshape(11, 1)\n",
    "\n",
    "print(d.squeeze().shape)\n",
    "print(c.squeeze().shape)\n",
    "print(torch.cat([d, c], dim=0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in tu_train_dataloader:\n",
    "    print(x.batch_num)\n",
    "    print(y.shape)\n",
    "    print(x.adj_type)\n",
    "    print(x.pyg_graph.num_nodes)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profiling\n",
    "\n",
    "# TUDataset.\n",
    "# NOTE node feature dim=3, added std feature\n",
    "\n",
    "\n",
    "args = utils.get_common_args()\n",
    "\n",
    "args = args.parse_args({})\n",
    "args.pos_en = 'lap_pe'\n",
    "args.pos_en_dim = pos_en_dim\n",
    "\n",
    "\n",
    "gin_evls = []\n",
    "train_gin_evls = []\n",
    "train_evl, test_evl = train_gnn(args, tu_train_dataloader, tu_test_dataloader, \n",
    "                                    node_fea_dim=node_feature_dim, gnn_name='lsd_gin', epoch=100)\n",
    "gin_evls.append(test_evl)\n",
    "train_gin_evls.append(train_evl)\n",
    "    \n",
    "mi_f1, ma_f1, w_f1 = get_f1s(gin_evls)\n",
    "\n",
    "print(mi_f1[0])\n",
    "print(ma_f1[0])\n",
    "print(w_f1[0])\n",
    "plot_f1_curves(mi_f1, ma_f1, w_f1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "i = torch.tensor([[0,1,2,3,2], [0,1,2, 2,0]])\n",
    " \n",
    "# 指定坐标上的值\n",
    "v = torch.tensor([1,2,3,3, 1])\n",
    " \n",
    "a = torch.sparse_coo_tensor(indices=i, values=v, size=[4, 4])\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# draw confusion map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "evl = big_kernel_cnn_evls[1]\n",
    "evl.plot_metrics()\n",
    "\n",
    "evl = big_kernel_cnn_evls[10]\n",
    "evl.plot_metrics()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try GCN + GNN. involve more spectral information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# incorporate the spectral methods:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: transform data, and construct node features.\n",
    "# construct pipline for the data.\n",
    "# like transform object in cv.\n",
    "\n",
    "\n",
    "def trans_spectral(d_loader):\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "# in future, add node feature, distinguish topological feature\n",
    "* add temporal information to node features.\n",
    "* add non-tological related node features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to train a probabilistic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare graph similarity in spectra domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygsp import graphs, filters\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "G = graphs.Logo()\n",
    "G.compute_fourier_basis()  # Fourier to plot the eigenvalues.\n",
    " # G.estimate_lmax() is otherwise sufficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = filters.Heat(G, tau=50)\n",
    "\n",
    "DELTAS = [20, 30, 1090]\n",
    "s = np.zeros(G.N)\n",
    "s[DELTAS] = 1\n",
    "s = g.filter(s)\n",
    "print(s)\n",
    "G.plot_signal(s, highlight=DELTAS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg\n",
    "\n",
    "# # construct G:\n",
    "\n",
    "def normlized_laplacian(adj):\n",
    "    d = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(d, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = np.diag(d_inv_sqrt)\n",
    "    normalized_laplacian = np.eye(adj.shape[0]) - np.matmul(np.matmul(d_mat_inv_sqrt, adj), d_mat_inv_sqrt)\n",
    "    return normalized_laplacian\n",
    "\n",
    "eigs = [linalg.eig(normlized_laplacian(adj)) for adj in samples]\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(8, 6), dpi=100)\n",
    "for i in range(axes.shape[0]):\n",
    "    for j in range(axes.shape[1]):\n",
    "        axes[i, j].plot(eigs[3*i+j][0].squeeze())\n",
    "\n",
    "\n",
    "# d = np.array(adj.sum(1))\n",
    "# d_inv_sqrt = np.power(d, -0.5).flatten()\n",
    "# d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "# d_mat_inv_sqrt = sp.diags(d_inv_sqrt).toarray()\n",
    "# normalized_laplacian = sp.eye(adj.shape[0]) - np.matmul(np.matmul(d_mat_inv_sqrt, adj), d_mat_inv_sqrt)\n",
    "# return normalized_laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
