{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark on OGB, https://ogb.stanford.edu/docs/home/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The OGB package is out of date. Your version is 1.3.2, while the latest version is 1.3.4.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/li_zhengdao/github/GenerativeGNN/utils.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import random\n",
    "import argparse\n",
    "import configparser\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_sparse\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch_geometric.utils import negative_sampling, to_networkx\n",
    "from typing import Union, Tuple\n",
    "from torch_geometric.typing import OptPairTensor, Adj, OptTensor, Size\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "\n",
    "from ogb.linkproppred import PygLinkPropPredDataset, Evaluator\n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy\n",
    "import math\n",
    "\n",
    "\n",
    "from dataset_utils import node_feature_utils\n",
    "from dataset_utils.node_feature_utils import *\n",
    "import utils\n",
    "\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adj': '12', 'cc': '3'}\n",
      "adj : 12 3\n"
     ]
    }
   ],
   "source": [
    "def to_dict(var_str:str):\n",
    "    d = {}\n",
    "    for i in var_str.split(';'):\n",
    "        kv = i.split(':')\n",
    "        d[kv[0]] = kv[1]\n",
    "    return d\n",
    "\n",
    "def tt(adj=None, cc=2):\n",
    "    print('adj :', adj, cc)\n",
    "\n",
    "def ttt(name, **xx):\n",
    "    print(xx)\n",
    "    tt(**xx)\n",
    "    \n",
    "a = 'degree@adj:12;cc:3'\n",
    "a = 'deg'\n",
    "args = a.split(\"@\")\n",
    "ttt(args[0], **to_dict(args[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 0])\n",
      "torch.Size([10, 1]) tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(10, 0)\n",
    "print(a.shape)\n",
    "b = torch.ones(10, 1)\n",
    "c = torch.cat([a, b], dim=-1)\n",
    "print(c.shape, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.font_manager:findfont: Font family ['Times New Roman'] not found. Falling back to DejaVu Sans.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family ['Times New Roman'] not found. Falling back to DejaVu Sans.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkQAAAFLCAYAAAA3XpMSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAzwUlEQVR4nO3debxd0/3/8ddbmphSgqqYQ9oYStFQYyuGmqm51VYltFXV8kV/NDUkvlptVYuS0FYqpaX0m4oaaghCh1BjFC3RJgQJKTLKYPj8/ljrcBznDrk55+7cu9/Px2M/zjl7rb3WZ597uZ+stfbeigjMzMzMymyZogMwMzMzK5oTIjMzMys9J0RmZmZWek6IzMzMrPScEJmZmVnpOSEyMzOz0nNCZGZmZqXnhMjMzMxKzwmRmZmZlZ4TIrMSkTRaUkjqV7WvX943usC4hucYBhUVQ2coy3madUVOiMyaRNIukq6VNFXSQkmvSPqzpG9J6lV0fG2RNF5SQ57tI2lQTgSGN6I9M7NG+0DRAZh1N5I+AIwAvgbMA/4EPAOsDOwB/Aw4VtI+EfFcYYG+6wVgE2BWgTFcAvwOWBq+DzMrISdEZo33A1Iy9ABwUES8UCmQ1AM4K2+3SNomIuYXE2YSEW8A/yo4hv8C/y0yBjMrN0+ZmTWQpAHAycCrwP7VyRBARLwVEcOAq4GPASfWHB+SxrfQ9hRJU2r7k3SepIfzlNwCSU9L+qGk3u2M+X1riPJU2c5VMUWdOkdLuiHHtUDSq5Juk7RLTfvDgbvzx2E17fWr1KleWyNpfUlvS7qrhZh7Svpvno5cpmp/L0kn5+9jnqQ5eZrygPZ8FzV9fFbS7VXf6xRJV0narKbehyRdKGlynhp9WdJ1tfXa0d/+ku6WNEvSfEkT87l8oKbeOz8vSZtIuj7HGLlscH4/OLd5v6TXJb0g6ZzK9yXpqNzHfEnPSfp/dWJaS9LZku7L57Uwfw8jJX24Tv3KGrUNJZ0qaVL+7iZLOktSzzrHHCLpntz+AkkvShon6ZDF+f7MlpRHiMwa6yjSPzR+EREvtVLvHOALwFeBHy5BfwcDx5ASjvG57+2A04CdJX06jwAtrrOBwcD6+X3Fo1XvRwATgXHADGBt4EBgnKSDI+KGXG880I/03dyTP1fMrNd5RDwr6d58DutExPM1VfYBVgN+FBFvA0haFrgVGJTjHAX0BPYFbpD0rYi4pM0zT239hHcT27HAy8C6wO7AQ8Djud7qwASgfz6v3wEbAIcC+0raMyL+0o7+TgZ+kvu7mjTVekDe96n8fdau5/oIcB/wD2B0/j4WVZUfRJqiHQv8NX8PZ6TuNCu/vyHHfQhwnqSXIuLKqjY+DZwC3AncD7wBbAUcB+wp6RMRUW+q9UJgR+A6YC6wP+n36OP5u6mc93HASGAacD3wCtAX+GSOf0wrX5tZY0WEN2/eGrSREpMAdm9H3Rdy3b5V+wIY30L9KcCUmn1rA73q1D0rt/XFmv2j8/5+Vfv65X2ja+qOT/+LaDH+DersWzOf19M1+wflPoa30NbwXD6oat8xed+pder/Xy77WNW+7+d9/wuoav8HSdOXC4G12vFz2S+38xiwWk3ZB4A1qj7/Ktc9t6bePnn/JGCZNs6zPynReAlYt2r/ssCfc/0j6/y8Aji7TvyDc9kiYJua7+ElUrI1Ddiwqmzd/P08VtPWh4Hedfr4cu7j9BZ+v14G1qna34uUDAdwSNX+h3K/H67Tx2q1+7x5a+bmKTOzxuqbX6e2o26lztod7SwiXoiIRXWKKiMhu3e07Xb0PbnOvmmkf9V/VNL6S9jF/wELgC9V75TUh5S0PBoRT+R9y5BGLf4NDIuId0ZTImIOKUnqRRpRa8s38uuJEfFKdUFEvBl55E/pSsEjSKMa36updwtwB2kUZ8c2+vsCKdH6SUS883sTEQtJI32Qkpxa00lJYEt+ExEPVLU3B7gJWAG4NCL+U1U2FfgLsGn1FF1EvBwRc+u0fRUwm5Z/vy6KqlG9/Dt6egvn8kbe3qP2uzdrNk+ZmRWvw/8wkSRgCOmPzGakK9mq21triSJrve8NgaHArqSkbtmaKmsBz3a0/YiYJemPwOGStoiIibnosNzXVVXVNwJWAV4krVOqbW71/LpxO7r+JGnU4p426m0MLAfcHRGv1ym/G/gMsCVppKclW+XX8XXKJpCSwi3rlE1sIRmueLTOvmltlPUA1iCN8gEg6WDgWOATpO+4R9UxLf1+1TvfCcCbvHu+kKYYzwMel3Q16Tv7S0TMbqFds6ZxQmTWWNNJfyjXBZ5qo+66+fWFVmu17mfAN0mjTX8k/VFbmMuG8f4kpSEkfQT4O7AS6Y/YjaQRg7dJ02M7N6jvq4DDSaNElYToSOAt0lqbilXz68fy1pIV29HnysALkdcmtWKl/NrSWrFpNfUWu52ICEkvUX8UsbU1apB+HrXebEfZOwufJZ0CnE9aI3Y78DxQuSryf2j5Z1zvXN6S9Arp+604nzTCdhxprdK3gTcl3QycVG8U0qxZnBCZNdbfSAnBbqTFxnVJ2pj0r+vXSElURdDyf5crU3WvoHyVz/GktS7bV49SSOpLSoia5STSaMGREfGb6gJJl5GvUGuAW0l/jI+QdBqwHrATcHtEVH9vlT/wYyLiUJbMTKCvpGXaSIoqfa7RQnnfmnrtaec9I2p5BHCNFtpoyE0zW5Knzs4kJXZbRsTLNXGd2srha1DzDwKlW06sRlWylKc2fwX8StJqwKdI05CHk6ZdPx4RbzXmjMxa5zVEZo31a9IoyVfzFUgtqayn+E3NH93XqDMaoHR5ep+a3RsCAsbVmbL51GLE3JK3ct896pT1z683VO/MfyjrrZmp/FGr11aLIuJN0rTK2sAuwBdJ5/ybmqr/JCUNW9e7tHsx/Z008tFWUvcv0nTWNpJWqFM+KL8+2kY7j9TUr7YtaVqurTaa4UOkJHxCdTKUbQ0s38qx9X7/ticl+4/UKSMiXomIsRHxOeAuYFPSGiyzTuGEyKyBIuJp4KekfwnfKGnN6nJJy0g6kzQFNJN0eXK1B4B+knauOqZXbrNWZTRhB733XjzrkG4OuaReza/r1imr9L1Tzf7vkNYyLU5bbamsFToyb/NIl2i/IydOl5JuE3B+C/e72azevXPqGJFfL5K0anWBpA9IWiP3uQi4hpQ4DK2ptxewJ+kO5X9to7+rSdNVJ0t6Z01O/rn/KH8c3Y64G+1l0vTYJ6oTPkmrABe3ceyJ+fewckwv3l0APrpq/yDVLPjKP7vK976gw9GbLSZPmZk13lDSv6y/CkzK6yH+TVorsgfwUdL/6D9ffaVP9tNc5xZJ1wCvkxbmzuTdNSlAuqJL0hjSPWQelHQnaapiP9J9Y/qzZO4i3TNmjKQ/5ZgnRsSNwGWkxdxjJF1HWgeyHWnh7c2ke95U+xdpwfPnJS0krUUJ4OKofx+b6vN8QNJTpKuxegJXRcS8OlWH5f5PIN0D6F7SH/W1gc2BLUijFLWjHbX93SLpfNJ6lkmSrq9qZzfSupcLc/XTSCNJZ0jagXSvnn6khd+vA0PaWosUEf/O04E/AR7L3+c80r17NiKNwtWOiDVdRLwtaSRpbc9ESTeSfof3JiXEL7Zy+H35mGt577n8ISKq7y00Fpgt6b7cZk/S7/umwP9FRIcX5ZsttqKv+/fmrbtupKuvriUtmn6Dd+8dMwHo38pxh5LWBS0kJUE/A3pT/z5EvUl/oCeTEpanSTfc60mdexqxePch+gBphOLZqvhHV5UPIl2qPZs01XczKSEZTs29dnL9bUlXUs2u+i765bK6x1Qde3rVMXu08t31ID025S+k9VYLcvx/Ar4OrLgYP7+DSUnhzNzOZOBKqu59lOt9CLgo/3wWkdY8/R7YrE6bLZ4n6UaMle9nQf4dOBn4QE29uj+vqvLBuXzwYvZf73ejJ/Dd/HtV+S7Pp+Xfx0obG5KSxUmk3+MppIS1V03940gJ3xTSaNR/SUnl14GeRf837K1cmyKaui7PzDKlx3rcR/oD8amIeKbgkMwaSunRLkeRbto5pdhozBaP1xCZdZJI64sOIa0vukNSh2/IaGZmjeWEyKwTRcTdpKTo1zTmSjAzM2sAL6o262SRFiXfWHQcZmb2Lq8hMjMzs9LzlJmZmZmVnhMiMzMzKz0nRGZmZlZ6XlQNvHnz9d1+IdWta25TdAhNteZ6zxUdQlOtOmX9okNoqn4r3Vt0CE2lAQOKDqGpXnyq1ZuNd3l9l9mi6BCaal7fXkWH0HQf/OAH1VYdjxCZmZlZ6TkhMjMzs9Lr0glRflJySOpTdCxmZmbWdS1WQiRpdE5AQtIiSc9IOkvSEq1FalRiI2m5HOM/JL0paeyStGdmZmbl0JERoluBNYGPAj8hPT35/zUwpiXRg/TE5J8B4wqOxczMzLqIjiRECyNiekQ8GxGXkhKPAySdnEdm5kmaKmmkpN6VgyStL+lGSa/lOk9I2kdSP+DuXO21PFI0Oh+zjKShkiZLmi9poqRDWwosIuZFxHER8UtgegfOzczMzEqoEZfdzyc9vftt4ARgMrAhMBI4D/hGrjcC6AV8GpgHbArMBaaSHnY5BtgImJ3bBBgKfAn4OjApH/sbSTMi4p4GxG5mZmbW8YRIkoDdgD2BiyPiwqriKZLOAC7j3YRoPWBMRPwjf/5PVVuv5rcvR8TMvG9Z4LvA7hExoXKMpJ2AYwEnRGZmZtYQHUmI9pM0F+hJmnK7GhguaXfSiM7GwEq57eUkrRARr5PW9VwqaQ/SNNuYiHislX4+AqwA3JFyr3f0Ah7pQNxmZmZmdXVkDdHdwJakRdXLR8RRwOrATcBjpOmvgcDxuX4vgIi4nDSVdhWwOfCgpG+10k9l/dG+ub/KtinQ4joiMzMzs8XVkRGieRHxTM2+gaTk6pSIeBtA0uG1B0bEVNI02mWSfgB8FbgYWJSr9Kiq/iSwEFjP64XMzMysmRr1LLNnSFNo35J0I7AjaSH0OyRdCPwJeBpYBdgF+GcufhYI0nTcLcD8iJgj6XzgAknLAH8BVs5tz46IX9cLRNKmpFGpVYEPStoSICIebdC5mpmZWTfTkIQoIiZKOhk4DfgBcC9pPdGVVdV6kK40W4d0JdmtwEn5+BckDQN+CFyRjxsMnAnMyG1tCMwEHgbObSWcW4DqJ2FW1hu1+WA3MzMzKydFdPsHvbfJT7vv+vy0+67NT7vv2vy0+67NT7tPuvSzzMzMzMwawQmRmZmZlZ4TIjMzMys9J0RmZmZWel5UbWZmZqXnESIzMzMrPSdEZmZmVnqNulN1l3bT1IdLPW+4z4zJRYdQmIeX26noEAqzybw3iw6hMLM/9kLRIRRmzbfKe+6zZ65XdAiFmfRy0REUa+DAgb4PkZmZmVlbnBCZmZlZ6XX5hEjSIEkhqU/RsZiZmVnXtNgJkaTROQEJSYskPSPpLElLtB6pUYlNbucGSdMkzZP0qKQvLkmbZmZm1r11NIm5FRgCLAvsQ3qK/RukJ90XbQfgMeBHwEvAfsCVkmZFxE2FRmZmZmZLpY5OmS2MiOkR8WxEXAqMAw6QdLKkf+SRmamSRkrqXTlI0vqSbpT0Wq7zhKR9JPUD7s7VXssjRaPzMctIGippsqT5kiZKOrSlwCLi3Ig4MyL+FhH/joiLSAncwR08VzMzM+vmGnXZ/XxgNeBt4ARgMrAhMBI4D/hGrjcC6AV8GpgHbArMBaYChwBjgI2A2blNgKHAl4CvA5Pysb+RNCMi7mlnfCsD/+z46ZmZmVl3tqTrfgTsBuwJXBwRF1YVT5F0BnAZ7yZE6wFjIuIf+fN/qtp6Nb99OSJm5n3LAt8Fdo+ICZVjJO0EHAu0mRBJOhzYJtc3MzMze5+OJkT7SZoL9CRNu10NDJe0O2lEZ2Ngpdz+cpJWiIjXgZ8Bl0ragzTNNiYiHmuln48AKwB3pNzrHb2AR9oKUtIuwBXAVyPiicU8RzMzMyuJjq4huhvYEvgosHxEHAWsDtxEWtB8CDAQOD7X7wUQEZeTptKuAjYHHpT0rVb6qaw/2jf3V9k2BVpcRwQgaWfgRuCkiLhyMc7NzMzMSqajI0TzIuKZmn0DSQnWKRHxNrwzXfUeETGVNI12maQfAF8FLgYW5So9qqo/CSwE1luM9UJIGkRKzk6LiF+09zgzMzMrp0Y+y+wZ0hTatyTdCOxIWgj9DkkXAn8CngZWAXbh3cXOzwJBmo67BZgfEXMknQ9cIGkZ4C+kBdI7ArMj4te1QeRpspuAi4AxkvrmokUR8WptfTMzM7OG3ak6IiYCJwOnAY8DXyStJ6rWg3Sl2T9Jl8I/TV5wHREvAMOAH5LuH3RJPuZM4JzcVuW4fUlXstVzFGnd0VBgWtX2hyU8RTMzM+umFFHqB70Dftq9n3ZfTn7afTn5affl5Kfd+2n3ZmZmZm1yQmRmZmal54TIzMzMSs8JkZmZmZWeF1WbmZlZ6XmEyMzMzErPCZGZmZmVnhMiMzMzK71GPrqjy7rmkee7zEKqoddNLDqEdrn42CeLDqFdzr6nazz3975FrT0Deekxffg5RYfQLmv99pCiQ2iXZcZfVHQI7fLAwK7x3/voH/+l6BDaZcS9XeNP0h+/cEHRIbTbAVf/0zdmNDMzM2uLEyIzMzMrvaUiIZI0WNLMJrU9XNKjzWjbzMzMuoeGJUSSRkuKvC2S9IyksyS1Z53StcCARsViZmZmtjgavaj6VmAIsCywDzACeAP4QWsHRcR8YH5L5ZJ6RcSiBsZpZmZm9o5GT5ktjIjpEfFsRFwKjAMOkHSypH9ImidpqqSRknpXDqqdMqtMc0n6iqTJwIK8v4+kyyXNkDRb0l2StqgOQNJ3JL0kaY6kUcByDT5HMzMz62aavYZoPtALeBs4AfgYcBSwK3BeG8d+BDgEOBjYMu/7PfBhYG9gIPAwcKekVQEkHQ4MB74LbA1MA77RqJMxMzOz7qkp9yGSJGA3YE/g4oi4sKp4iqQzgMtoPVnpBXw5ImbkNncCPgl8OCIW5jrflnQgcCjwC+B/gFERMSqXnyFpdzxKZGZmZq1odEK0n6S5QE/S6NPVwPCclAwFNgZWyv0uJ2mFiHi9hbaerSRD2RZAb+CVlG+9Y3mgf36/CSnRqjYB2KXjp2RmZmbdXaMToruB44BFwIsR8aakfsBNwKXA6cCrwE7AKNIoUEsJ0byaz71JU2CD6tSduYRxm5mZWYk1OiGaFxHP1OwbSBotOiUi3oZ31vosroeBvsCbETGlhTr/BLYFqp/HsF0H+jIzM7MS6YwbMz5DmkL7lqQNJR0JfL0D7YwjTX+NlbSHpH6SdpD0fUlb5zoXAUdLGiJpgKSzSQu5zczMzFrU9IQoIiYCJwOnAY8DXyStJ1rcdoJ0b6N7gSuAp4HfAesDL+U61wLnkK5geyiXXbrEJ2FmZmbdWsOmzCJicCtlFwC1j8W9qqp8NDC66vNw0uXzte3MIV2+f0IrfZ0LnFuz+7SW6puZmZktFc8yMzMzMyuSEyIzMzMrPSdEZmZmVnpKa5XNzMzMyssjRGZmZlZ6TojMzMys9JwQmZmZWek15Wn3Xc3wvw8v9UKqnV84rOgQCrPzqn8sOoTCLLPzakWHUJjtv7VG0SEU5taRjxQdQmEeOaK89+m9ZsUDiw6hUD//+c/VVh2PEJmZmVnpOSEyMzOz0nNCZGZmZqW3xAmRpO0lvSXp5kYE1EiSQtKBRcdhZmZmS7dGjBAdA1wMfFrSWg1oz8zMzKxTLVFCJKk38DngUuBmYHBN+f6SHpC0QNJ/JV1fVbaspB9JmippoaRnJB1TVb6ZpD9JmivpJUlXSfpQVfl4ST+TdJ6kVyVNlzS8qnxKfnt9HimqfDYzMzN7jyUdIToc+FdEPAX8BjhakgAk7QtcD9wCbAXsBvy96tgrgSOAE4BNgGOBufnYPsBdwCPA1sBewBrAdTX9HwXMA7YFTgXOkvSZXLZNfh0CrFn12czMzOw9lvQ+RMeQEiGAW4GVgZ2B8cDpwO8iYlhV/YkAkgaQkqnPRMS4XPafqnrfBB6JiO9Wdkg6GpgqaUBEPJ13PxYRZ+f3kyR9k5R43RERM3JuNjMipi/heZqZmVk31uERIkkbAZ8ErgGIiDeBa0lJEsCWwJ0tHL4l8BZwTwvlWwC75OmyuZLmAv/KZf2r6j1Wc9w04MPtPwszMzOzJRshOiYf/2IeiQEQsDCP1Mxv5djWygB6AzcCp9Upm1b1/o2assC3EjAzM7PF1KGESNIHgC8DpwC31xSPJa0Neow0fXVFnSb+QUpcdgbG1Sl/GDgEmJJHnjrqDaDHEhxvZmZmJdDR0ZT9gFWAURHxePUGjCGNHp0NHCHpbEmbSNpc0mkAETEF+DXwK0kHStpA0iBJh+f2RwCrAtdI2kZSf0l7SrpC0uIkOFOA3ST1lbRKB8/VzMzMurmOJkTHAOMiYladsjGkK8NeBQ4DDgAeJV019smqescB/weMJK0P+iWwIkBEvAjsSBrduZ00onQhMBN4ezHiPAX4DDCVdMWamZmZ2ft0aMosIvZvpezvpLVEkKbN/tBCvQXAyXmrVz4JOLiVfgbV2XdgzecbSWuRzMzMzFrkBchmZmZWek6IzMzMrPScEJmZmVnpOSEyMzOz0lNEFB2DmZmZWaE8QmRmZmal54TIzMzMSs8JkZmZmZXekjzctdtY54rvlHoh1V33/LjoEApz+So3FB1CYU6+7Z9Fh1CYYWdeUnQIhRne8/miQyjM+AfWLjqEwoyfuXfRIRTq5z//udqq4xEiMzMzKz0nRGZmZlZ6XS4hkjRa0tii4zAzM7Puo+EJUU5YIm9vSHpJ0h2SjpbU9ARM0sG5vxmSZkuaIGnPZvdrZmZmXVezEpRbgTWBfsDewN3ARcBNkjq0kFtSj3YmVJ8G7gD2AQbmvm+UtFVH+jUzM7Pur1kJ0cKImB4RL0TEwxFxLvBZUnI0GEDSyZL+IWmepKmSRkrqXWlA0mBJMyUdIOlJYCGwXm1HkrbJo0GnAUTE/0TEeRHxQERMiojvApOA/Zt0rmZmZtbFddoaooi4C5gIHJx3vQ2cAHwMOArYFTiv5rAVgNOAr+R6L1cXStqVNBp0ekT8qF6/eVTpg8CrDTkRMzMz63Y6+z5E/wI+DhARF1btnyLpDOAy4BtV+3sC34iIiZUdkiqvBwFXAl+JiGtb6fPbQG/gugbEb2ZmZt1QZydEAgJA0u7AUGBjYKUcy3KSVoiI13P9RcBjddrZFtgPODQixrbYmfQFYBjw2Yh4uaV6ZmZmVm6dfdn9JsBkSf2Am0jJziGkxc/H5zq9qurPj4h6d5H+N2m06WhJPet1JOnzwOXA4RExrjHhm5mZWXfUaQlRXu+zOTCGlAAtA5wSEfdFxNPAWovR3H9Ja44+AlxXmxRJOgK4AjgiIm5uRPxmZmbWfTUrIVpWUl9Ja0v6hKTvAjeQRoWuBJ4hrQ/6lqQNJR0JfH1xOshTYLuSptyuqVzOn6fJrgROAe7PcfSVtHLDzs7MzMy6lWYlRHsB04AppHsS7UK6ouyzEfFWXiR9MukKsseBL5LWEy2WiJhOSoo2B34rqQfwNdJ6pBE5hsp20ZKdkpmZmXVXDV9UHRGDyfcaaqPeBcAFNbuvqiofDYxuof3qz9OAjap2DWpfpGZmZmZJl3uWmZmZmVmjOSEyMzOz0nNCZGZmZqWn+rf5MTMzMysPjxCZmZlZ6TkhMjMzs9JzQmRmZmal19kPd10qvbD2ut1+IdUhX7m86BCa6sNb/azoEJpq3yuOKDqEpjp2qyOLDqG5hg8sOoKmuuZz84oOoakO+tilRYfQVFf2fbroEJrua1/7mtqq4xEiMzMzKz0nRGZmZlZ6TojMzMys9ApNiCStLulSSc9JWihpuqTbJO3YzuOHS3q0zv6vSRovabakkNSn0bGbmZlZ91H0ouoxQC/gKOA/wBrAbsBqS9juCsCtefvBErZlZmZm3VxhCVEetfkUMCgi7sm7nwX+XlPnfOCzwLLAg8BJETFR0mBgWK5XuUpsSESMjogL8/5BTT4NMzMz6waKHCGam7cDJd0XEQvr1Pk9MB/YG5gFHAvcKWkAcC2wGbAXsHuuP6vpUZuZmVm3U9gaooh4ExhMmi6bKemvks6V9HEASTsBnwQOi4gHI2JSRHwbmAkcGhHzSQnVmxExPW/zCzkZMzMz69IKXUMUEWMk3UyaOtuONBJ0qqSvACsCvYFXpPfcT2l5oH9nx2pmZmbdV9GLqomIBcAdeTtH0uXA2cBIYBowqM5hMzsrPjMzM+v+Ck+I6ngSOBB4GOhLmhKb0kLdRUCPzgnLzMzMuqsirzJbjbRo+lfAY8AcYGvgVOAGYBwwARgr6VTgaWAtYF/g+oh4EJgCbCBpS+B5YE5ELJTUl5RMfSR3t7mkOcBzEfFq55yhmZmZdRVF3phxLnA/cBJwL/A4cA7wS+CbERHAPrnsClJC9DtgfeCl3MYY0r2G7gZmAJUnYH4deCS3RW7jEeCApp6RmZmZdUmFjRDly+yH5q2lOnOAE/LWUhuH1tk/HBjeiDjNzMys+/OzzMzMzKz0nBCZmZlZ6TkhMjMzs9JzQmRmZmalp3Qxl5mZmVl5eYTIzMzMSs8JkZmZmZWeEyIzMzMrvaXxWWad7tg/X13qhVQjH7iz6BAKc9FqxxUdQmG+8PKCokMozN0HXFR0CIU5QpOKDqEwL874atEhFObh19YvOoRC7bfffmqrjkeIzMzMrPScEJmZmVnpFZoQSVpd0qWSnpO0UNJ0SbdJ2rGdxw+X9GjNvlUlXSzpKUnzc9s/k7RyU07CzMzMuryi1xCNAXoBRwH/AdYAdgNWW4I218rbt4EngfWBy/K+9z0I1szMzKywhEhSH+BTwKCIuCfvfhb4e02d84HPAssCDwInRcRESYOBYbleZVH0kIgYDRxS1dW/JZ0O/EbSByLizWadk5mZmXVNRY4Qzc3bgZLui4iFder8HpgP7A3MAo4F7pQ0ALgW2AzYC9g915/VQl8rA7OdDJmZmVk9ha0hysnJYNJ02UxJf5V0rqSPA0jaCfgkcFhEPBgRkyLi28BM4NCImE9KqN6MiOl5m1/bj6QPAWcCv+iUEzMzM7Mup9A1RBExRtLNpKmz7UgjQadK+gqwItAbeEV6z+0Dlgf6t6d9SSsBN5PWEg1vXORmZmbWnRS9qJqIWADckbdzJF0OnA2MBKYBg+ocNrOtdiV9ELgVmAMcFBFvNChkMzMz62YKT4jqeBI4EHgY6EuaEpvSQt1FQI/anXlk6DZgIXBATrrMzMzM6iryKrPVSIumfwU8RhrJ2Ro4FbgBGAdMAMZKOhV4mnTp/L7A9RHxIDAF2EDSlsDzuY1lgduBFYAvASvlBAlgRkS81RnnZ2ZmZl1H0VeZ3Q+cRFoT1BOYCvwSODciQtI+wPeBK4DVgenAvcBLuY0xwMHA3UAfYAgpSdo2lz9T0+cGudzMzMzsHYUlRPky+6F5a6nOHOCEvLXURr2bLbb5EDczMzOzCj/LzMzMzErPCZGZmZmVnhMiMzMzKz0nRGZmZlZ6ioi2a5mZmZl1Yx4hMjMzs9JzQmRmZmal54TIzMzMSm9pfJZZp/vp4/d1mYVUJ2/236JDaJe779io6BDa5VN/u6zoENrl+1sNKTqEdjmzxwVFh9Aut2y6bNEhtMver51RdAjt8u/ec4sOoV2mXvN80SG0S5/jlys6hHYZuMx1RYfQfqte2OYNmz1CZGZmZqXnhMjMzMxKzwmRmZmZlV6hCZGk1SVdKuk5SQslTZd0m6Qd23n8cEmP1tn/c0n/ljRf0gxJN0jauOEnYGZmZt1C0YuqxwC9gKOA/wBrALsBqy1huw8BvwWeA1YFhgO3S9ogIt5awrbNzMysmyksIZLUB/gUMCgi7sm7nwX+XlPnfOCzwLLAg8BJETFR0mBgWK5XuUpsSESMjohfVHU1RdIZwESgH/DvJp2SmZmZdVFFjhDNzduBku6LiIV16vwemA/sDcwCjgXulDQAuBbYDNgL2D3Xn1XbgKQVgSHAZGBqo0/CzMzMur7C1hBFxJvAYNJ02UxJf5V0rqSPA0jaCfgkcFhEPBgRkyLi28BM4NCImE9KqN6MiOl5m19pX9I3JFWSrr2Bz0TEos48RzMzM+saCl1UHRFjgLWAA4BbgUHAw3k6bAugN/CKpLmVDdgA6N+O5n8LbAXsDDwNXCepa9ztyszMzDpV0YuqiYgFwB15O0fS5cDZwEhgGilJqjWzHe3OIk2hTZJ0H/AacBBwTUMCNzMzs26j8ISojieBA4GHgb6kKbEpLdRdBPRoR5vKW9e4X7+ZmZl1qiKvMluNtGj6V8BjwBxga+BU4AZgHDABGCvpVNK011rAvsD1EfEgMAXYQNKWwPO5jbWBzwG3AzOAdYDvkBZn39I5Z2dmZmZdSdFXmd0PnERaE9STdBXYL4FzIyIk7QN8H7gCWB2YDtwLvJTbGAMcDNwN9CFdTXY76XL+/wFWyXXvBXaIiJc74bzMzMysiyksIcqX2Q/NW0t15gAn5K2lNg6tU7RPI2I0MzOzcvCzzMzMzKz0nBCZmZlZ6TkhMjMzs9JzQmRmZmalp4hou5aZmZlZN+YRIjMzMys9J0RmZmZWekvjozs6Xb/v3Nxt5g2nLPeFokNomM03WK/oEBriuh+8WXQIDXPXoBFFh9AQC177adEhNMTnNjit6BAa5vLl7iw6hIb41KevKjqEhvmixhQdQsNM32VLtVXHI0RmZmZWek6IzMzMrPSWmoRI0mhJY4uOw8zMzMqnKQmRpPGSLmxCu5+WdKOkFyWFpAPr1JGk/5U0TdJ8SeMkfbTRsZiZmVn3sdSMELXTisBE4PhW6pxKehjs14FtgXnAbZKWa354ZmZm1hU1PCGSNBrYGTgxj+KEpP6SRkmanEdtnpJ0YgvHD5M0Q9JsSZdJ6lUpi4g/RcQZEXF9C8cK+B/gexFxQ0Q8BnwZWAs4sLFnamZmZt1FMy67PxEYADwOnJX3vQY8DxwGvALsAPxC0rSIuK7q2N2ABcAgoB9wRa5/ejv73gDoC4yr7IiIWZLuB7YHftehMzIzM7NureEJUU5AFgGvR8T0qqJhVe8nS9oeOByoTogWAUdHxOvAE5LOAn4s6cyIeLsd3ffNry/V7H+pqszMzMzsPTrtxoySjgeOBtYDlgd6AY/WVJuYk6GKCUBvYF3g2U4I08zMzEqoUxZVS/o8cD4wCtgD2JI0HdarlcM6ojIitUbN/jWqyszMzMzeo1kJ0SKgR9XnHYG/RcTIiHgkIp4B+tc5bgtJy1d93g6YC0xtZ7+TSYnPbpUdklYiXW02YTHiNzMzsxJp1pTZFGBbSf1ICc0k4MuS9iQlLUcC2+T31XoBoyR9j7So+mzgksr6IUm9gY9U1d9A0pbAqxHxXEREvv/RGZIm5fbPAV4Exjb8LM3MzKxbaFZCdD7wa+BJ0nqhjYGtgGuBAK4BRgJ71xx3Jyl5uhdYNtcbXlW+NXB31efKExp/DQzO788j3a/oF0Af4C/AXhGxYElPyszMzLqnpiREEfE06TL3akPyVm1o1TGDq/YPo46IGA+0+sTaiAjS5f5ntVbPzMzMrKKr3anazMzMrOGcEJmZmVnpOSEyMzOz0nNCZGZmZqWntAbZzMzMrLw8QmRmZmal54TIzMzMSs8JkZmZmZVepz3tfmnW7zs3d8pCqgsO3LrpfRx064Cm9wHATt9oehebv3BT0/v45s5fbnofn97rV03vA2CTk05oeh8jHtmo6X1stXffpvcx4epTm97HdnvU3oi/8daf9PGm9/Ghnfs0vQ+AH94/pul97LXXXk3vY97rX2x6H+uvd3zT+wDYaequTe/jB+t8uOl9AAz56Fqt3tQZPEJkZmZm5oTIzMzMbKlJiCSNljS26DjMzMysfJqSEEkaL+nCJrQ7VNIDkuZIelnSWEkb1dRZTtIISa9ImitpjKQ1Gh2LmZmZdR9LzQhRO+0MjAC2Az4D9ARul7RiVZ0LgP2Bw3L9tYA/dHKcZmZm1oU0PCGSNJqUiJwoKfLWX9IoSZMlzZf0lKQTWzh+mKQZkmZLukxSr0pZROwVEaMj4omImAgMBtYDBuZjVwaOAU6OiLsi4iFgCLCDpO0afa5mZmbWPTTjsvsTgQHA48BZed9rwPOkUZtXgB2AX0iaFhHXVR27G7AAGAT0A67I9U9voa+V8+ur+XUgadRoXKVCRPxL0nPA9sB9S3BeZmZm1k01PCGKiFmSFgGvR8T0qqJhVe8nS9oeOByoTogWAUdHxOvAE5LOAn4s6cyIeLu6H0nLABcCf42Ix/PuvsCiiJhZE9ZLuczMzMzsfTrtxoySjgeOJk1xLQ/0Ah6tqTYxJ0MVE4DewLrAszV1RwCbATs1I14zMzMrj05ZVC3p88D5wChgD2BL0nRYr1YOa629S4D9gF0i4vmqoulAL0l9ag5ZI5eZmZmZvU+zEqJFQI+qzzsCf4uIkRHxSEQ8A/Svc9wWkpav+rwdMBeYCqDkEuAgYNeImFxz/EPAG6S1SORjNiKNSk1YwnMyMzOzbqpZU2ZTgG0l9SMlNJOAL0vaE5gMHAlsk99X6wWMkvQ90qLqs4FLqtYPjQC+AHwWmCOpsi5oVkTMz+uXRgE/lfQqMBu4GJgQEV5QbWZmZnU1KyE6H/g18CRpvdDGwFbAtUAA1wAjgdqnH95JSp7uBZbN9YZXlR+XX8fXHDcEGJ3fnwS8DYzJbdwGNP9JpGZmZtZlNSUhioinSZe5VxuSt2pDq44ZXLV/GHVERJtPq42IBcDxeTMzMzNrU1e7U7WZmZlZwzkhMjMzs9JzQmRmZmal54TIzMzMSk8RUXQMZmZmZoXyCJGZmZmVnhMiMzMzK71Oe7jr0myHnb7bKfOGa665afP7WGuzpvcBsNWWKze9jwmTvtj0PjbZZM3m97F58/sAWLP3Vk3v466fjWh6H1tttVzT+9h0nU74mayya9O7GHXSb5vexyYfXKXpfQBs0neDpvexyjbN/2/kkt//s+l9bNqn+d8VwCbrbNL0Pvps1Tn/f9xgyCZt3sfQI0RmZmZWek6IzMzMrPQ6JSGSNFrS2E7oZ5CkkNSn2X2ZmZlZ97HYCZGk8ZIubHQgkoZKekDSHEkvSxoraaOaOstJGiHpFUlzJY2RtEajYzEzM7NyWZqmzHYGRgDbAZ8BegK3S1qxqs4FwP7AYbn+WsAfOjlOMzMz62YWKyGSNJqUiJyYp6ZCUn9JoyRNljRf0lOSTmzh+GGSZkiaLekySb0qZRGxV0SMjognImIiMBhYDxiYj10ZOAY4OSLuioiHgCHADpK2q+lqR0mPSVog6T5JnXPplZmZmXVJiztCdCIwAfglsGbens/bYcCmwP8C50o6vObY3YBNgEHAEcDBwLBW+qpc1/1qfh1IGjUaV6kQEf8CngO2rzn2x8ApwDbADOBGST3beY5mZmZWMot1H6KImCVpEfB6REyvKqpObCZL2h44HLiuav8i4OiIeB14QtJZwI8lnRkRb1f3I2kZ4ELgrxHxeN7dF1gUETNrwnopl1U7OyLuyG0dRUrYDqqJx8zMzAxo0I0ZJR0PHE2a4loe6AU8WlNtYk6GKiYAvYF1gWdr6o4ANgN26mBIEypvIuJVSU+RRqfMzMzM3meJF1VL+jxwPjAK2APYEriClBR1pL1LgP2AXSLi+aqi6UCvOpfUr5HLzMzMzDqkIwnRIqBH1ecdgb9FxMiIeCQingH61zluC0nLV33eDpgLTAVQcglpamvXiJhcc/xDwBuktUjkYzYijUpNqKm7XVWdVYABQPPvp25mZmZdUkemzKYA20rqR0poJgFflrQnMBk4krSYuTah6QWMkvQ9oB9wNnBJ1fqhEcAXgM8CcyRV1gXNioj5ef3SKOCnkl4FZgMXAxMi4r6avs6S9AppfdH3gf8CYztwrmZmZlYCHRkhOh94C3iSdAXXbaR7AV0L3A+sBoysc9ydpOTp3lz3j8DwqvLjSFeWjQemVW2fq6pzEnATMCa3M510tVqt7wAXkUaV+gL7R8SixTxPMzMzK4nFHiGKiKd5/2XuQ/JWbWjVMYOr9te91D4i2nwSbUQsAI7PW73y8UClnZvaas/MzMwMlq47VZuZmZkVwgmRmZmZlZ4TIjMzMys9J0RmZmZWeoqIomMwMzMzK5RHiMzMzKz0nBCZmZlZ6TkhMjMzs9JryNPuu7pPnHNXpyyk2mGzNZvex8B1Vmx6HwAbr7JS0/vo9931mt7HGttt0fQ+2KAT+gDYcNumd7H1P/7Y9D5222DDpvex+eofbXofA/o0v48//O+spvexyRZrNL0PgPX6f6j5fay7ctP7+NGPvtP0PgYOHNj0PgA22mijpvexzjrrNL0PgAEDBrR582ePEJmZmVnpOSEyMzOz0uu0hEjSaEljO6s/MzMzs/bqUEIkabykCxscC5KGSnpA0hxJL0saK2mjmjrLSRoh6RVJcyWNkdQ5k9xmZmbWLS1tU2Y7AyOA7YDPAD2B2yVVrxS+ANgfOCzXXwv4QyfHaWZmZt3IYidEkkaTEpETJUXe+ksaJWmypPmSnpJ0YgvHD5M0Q9JsSZdJ6lUpi4i9ImJ0RDwREROBwcB6wMB87MrAMcDJEXFXRDwEDAF2kLRdVR8fk3RT7mOOpD9L6r+452pmZmbl0JHL7k8EBgCPA2flfa8Bz5NGbV4BdgB+IWlaRFxXdexuwAJgENAPuCLXP72FvirXSL6aXweSRo3GVSpExL8kPQdsD9wnaW3gXmA8sCswG9ixg+dqZmZmJbDYSUJEzJK0CHg9IqZXFQ2rej9Z0vbA4UB1QrQIODoiXgeekHQW8GNJZ0bE29X9SFoGuBD4a0Q8nnf3BRZFxMyasF7KZQDHA7OAz0fEG3nf04t7nmZmZlYeDRs1kXQ8cDRpimt5oBfwaE21iTkZqpgA9AbWBZ6tqTsC2AzYaTFD2RL4c1UyZGZmZtaqhiyqlvR54HxgFLAHKSm5gpQUdaS9S4D9gF0i4vmqoulAL0l9ag5ZI5cBzO9In2ZmZlZeHU2IFgE9qj7vCPwtIkZGxCMR8QxQbxHzFpKWr/q8HTAXmAqg5BLgIGDXiJhcc/xDwBuktUjkYzYijUpNyLseAz4lqWcHz83MzMxKpqMJ0RRgW0n9JH0ImARsLWlPSQMknQNsU+e4XsAoSZtK2gc4G7ikav3QCOBLwBeAOZL65m15SOuXSKNQP5W0i6SBpJGoCRFxX27jEmAl4HeStpb0UUlH1t7PyMzMzKyiownR+cBbwJPADOA20r2ArgXuB1YDRtY57k5S8nRvrvtHYHhV+XGkK8vGA9Oqts9V1TkJuAkYk9uZDhxcKYyIV0hXl/UG7iGNKn2VNLJkZmZm9j4dWlQdEU+TLnOvNiRv1YZWHTO4av8w6oiINp9GGxELSFeSHd9KnceAPdtqy8zMzAyWvjtVm5mZmXU6J0RmZmZWek6IzMzMrPScEJmZmVnpKSKKjsHMzMysUB4hMjMzs9JzQmRmZmal54TIzMzMSs8JkZmZmZWeEyIzMzMrPSdEZmZmVnpOiMzMzKz0nBCZmZlZ6TkhMjMzs9JzQmRmZmal54TIzMzMSs8JkZmZmZWeEyIzMzMrPSdEZmZmVnpOiMzMzKz0nBCZmZlZ6TkhMjMzs9JzQmRmZmal54TIzMzMSs8JkZmZmZWeEyIzMzMrPSdEZmZmVnpOiMzMzKz0nBCZmZlZ6TkhMjMzs9JzQmRmZmal54TIzMzMSs8JkZmZmZWeEyIzMzMrPSdEZmZmVnpOiMzMzKz0/j+uJgxTtzd1aAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x338.2 with 13 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "# here..\n",
    "cmaps = {}\n",
    "\n",
    "gradient = np.linspace(0, 1, 256)\n",
    "gradient = np.vstack((gradient, gradient))\n",
    "\n",
    "\n",
    "x_y_label_font = 20\n",
    "x_y_legend_font = 20\n",
    "\n",
    "plt.rc('font', family='Times New Roman')\n",
    "fig_dpi = 220\n",
    "fig_shape_squre = (6, 5)\n",
    "\n",
    "def plot_color_gradients(category, cmap_list):\n",
    "    # Create figure and adjust figure height to number of colormaps\n",
    "    nrows = len(cmap_list)\n",
    "    figh = 0.35 + 0.15 + (nrows + (nrows - 1) * 0.1) * 0.22\n",
    "    fig, axs = plt.subplots(nrows=nrows + 1, figsize=(6.4, figh), dpi=100)\n",
    "    fig.subplots_adjust(top=1 - 0.35 / figh, bottom=0.15 / figh,\n",
    "                        left=0.2, right=0.99)\n",
    "    axs[0].set_title(f'{category} colormaps', fontsize=14)\n",
    "\n",
    "    for ax, name in zip(axs, cmap_list):\n",
    "        ax.imshow(gradient, aspect='auto', cmap=plt.get_cmap(name))\n",
    "        ax.text(-0.01, 0.5, name, va='center', ha='right', fontsize=10,\n",
    "                transform=ax.transAxes)\n",
    "\n",
    "    # Turn off *all* ticks & spines, not just the ones with colormaps.\n",
    "    for ax in axs:\n",
    "        ax.set_axis_off()\n",
    "\n",
    "    # Save colormap list for later.\n",
    "    cmaps[category] = cmap_list\n",
    "    plt.show()\n",
    "\n",
    "class MyColor(object):\n",
    "    def __init__(self, cmap_name='tab10', skip_idx=5, backup_name='Set1', backup_color=3):\n",
    "        self.color_set  = plt.get_cmap(cmap_name).colors\n",
    "        self.backup_set = plt.get_cmap(backup_name).colors\n",
    "        self.backup_color = backup_color\n",
    "        self.skip_idx=skip_idx\n",
    "        self.idx = 0\n",
    "        self.color_len = len(self.color_set)\n",
    "        \n",
    "    def get_color(self):\n",
    "        if self.idx == self.color_len - 1:\n",
    "            self.idx = 0\n",
    "        if self.idx == self.skip_idx:\n",
    "            self.idx += 1\n",
    "            return self.backup_set[self.backup_color]\n",
    "        color = self.color_set[self.idx]\n",
    "        self.idx += 1\n",
    "        return color\n",
    "    \n",
    "\n",
    "plot_color_gradients('Qualitative',\n",
    "                     ['Pastel1', 'Pastel2', 'Paired', 'Accent', 'Dark2',\n",
    "                      'Set1', 'Set2', 'Set3', 'tab10', 'tab20', 'tab20b',\n",
    "                      'tab20c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "a = utils.DaoArgs()\n",
    "\n",
    "print(a.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spd_matrix(G, S, max_spd=5):\n",
    "    spd_matrix = np.zeros((G.number_of_nodes(), len(S)), dtype=np.float32)\n",
    "    for i, node_S in enumerate(S):\n",
    "        for node, length in nx.shortest_path_length(G, source=node_S).items():\n",
    "            spd_matrix[node, i] = min(length, max_spd)\n",
    "    return spd_matrix\n",
    "\n",
    "\n",
    "class Logger(object):\n",
    "    def __init__(self, runs, info=None):\n",
    "        self.info = info\n",
    "        self.results = [[] for _ in range(runs)]\n",
    "\n",
    "    def add_result(self, run, result):\n",
    "        assert len(result) == 2\n",
    "        assert run >= 0 and run < len(self.results)\n",
    "        self.results[run].append(result)\n",
    "\n",
    "    def print_statistics(self, run=None):\n",
    "        if run is not None:\n",
    "            result = 100 * torch.tensor(self.results[run])\n",
    "            argmax = result[:, 0].argmax().item()\n",
    "            print(f'Run {run + 1:02d}:')\n",
    "            print(f'Highest Valid: {result[:, 0].max():.2f}')\n",
    "            print(f'   Final Test: {result[argmax, 1]:.2f}')\n",
    "        else:\n",
    "            result = 100 * torch.tensor(self.results)\n",
    "            best_results = []\n",
    "            for r in result:\n",
    "                valid = r[:, 0].max().item()\n",
    "                test = r[r[:, 0].argmax(), 1].item()\n",
    "                best_results.append((valid, test))\n",
    "            best_result = torch.tensor(best_results)\n",
    "            print(f'All runs:')\n",
    "            r = best_result[:, 0]\n",
    "            print(f'Highest Valid: {r.mean():.4f} ± {r.std():.4f}')\n",
    "            r = best_result[:, 1]\n",
    "            print(f'   Final Test: {r.mean():.4f} ± {r.std():.4f}')\n",
    "\n",
    "\n",
    "class SAGEConv(MessagePassing):\n",
    "    r\"\"\"The GraphSAGE operator from the `\"Inductive Representation Learning on\n",
    "    Large Graphs\" <https://arxiv.org/abs/1706.02216>`_ paper\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{x}^{\\prime}_i = \\mathbf{W}_1 \\mathbf{x}_i + \\mathbf{W}_2 \\cdot\n",
    "        \\mathrm{mean}_{j \\in \\mathcal{N(i)}} \\mathbf{x}_j\n",
    "\n",
    "    Args:\n",
    "        in_channels (int or tuple): Size of each input sample. A tuple\n",
    "            corresponds to the sizes of source and target dimensionalities.\n",
    "        out_channels (int): Size of each output sample.\n",
    "        normalize (bool, optional): If set to :obj:`True`, output features\n",
    "            will be :math:`\\ell_2`-normalized, *i.e.*,\n",
    "            :math:`\\frac{\\mathbf{x}^{\\prime}_i}\n",
    "            {\\| \\mathbf{x}^{\\prime}_i \\|_2}`.\n",
    "            (default: :obj:`False`)\n",
    "        root_weight (bool, optional): If set to :obj:`False`, the layer will\n",
    "            not add transformed root node features to the output.\n",
    "            (default: :obj:`True`)\n",
    "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
    "            an additive bias. (default: :obj:`True`)\n",
    "        **kwargs (optional): Additional arguments of\n",
    "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: Union[int, Tuple[int, int]],\n",
    "                 out_channels: int, normalize: bool = False,\n",
    "                 root_weight: bool = True,\n",
    "                 bias: bool = True, **kwargs):  # yapf: disable\n",
    "        kwargs.setdefault('aggr', 'mean')\n",
    "        super(SAGEConv, self).__init__(**kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.normalize = normalize\n",
    "        self.root_weight = root_weight\n",
    "\n",
    "        if isinstance(in_channels, int):\n",
    "            in_channels = (in_channels, in_channels)\n",
    "\n",
    "        self.lin_l = Linear(in_channels[0], out_channels, bias=bias)\n",
    "        if self.root_weight:\n",
    "            self.lin_r = Linear(in_channels[1], out_channels, bias=False)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.lin_l.reset_parameters()\n",
    "        if self.root_weight:\n",
    "            self.lin_r.reset_parameters()\n",
    "\n",
    "\n",
    "    def forward(self, x: Union[Tensor, OptPairTensor], edge_index: Adj,\n",
    "                edge_attr: OptTensor = None, size: Size = None) -> Tensor:\n",
    "        \"\"\"\"\"\"\n",
    "        if isinstance(x, Tensor):\n",
    "            x: OptPairTensor = (x, x)\n",
    "\n",
    "        # Node and edge feature dimensionalites need to match.\n",
    "        if isinstance(edge_index, Tensor):\n",
    "            assert edge_attr is not None\n",
    "            assert x[0].size(-1) == edge_attr.size(-1)\n",
    "        elif isinstance(edge_index, SparseTensor):\n",
    "            assert x[0].size(-1) == edge_index.size(-1)\n",
    "\n",
    "        # propagate_type: (x: OptPairTensor, edge_attr: OptTensor)\n",
    "        out = self.propagate(edge_index, x=x, edge_attr=edge_attr, size=size)\n",
    "        out = self.lin_l(out)\n",
    "\n",
    "        x_r = x[1]\n",
    "        if self.root_weight and x_r is not None:\n",
    "            out += self.lin_r(x_r)\n",
    "\n",
    "        if self.normalize:\n",
    "            out = F.normalize(out, p=2., dim=-1)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def message(self, x_j: Tensor, edge_attr: Tensor) -> Tensor:\n",
    "        return F.relu(x_j + edge_attr)\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels,\n",
    "                                   self.out_channels)\n",
    "\n",
    "\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout):\n",
    "        super(GraphSAGE,self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        \n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj_t, edge_attr, emb_ea):\n",
    "        edge_attr = torch.mm(edge_attr, emb_ea)\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = conv(x, adj_t, edge_attr)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, adj_t, edge_attr)  # no nonlinearity\n",
    "        return x\n",
    "\n",
    "\n",
    "class LinkPredictor(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(LinkPredictor, self).__init__()\n",
    "\n",
    "        self.lins = torch.nn.ModuleList()\n",
    "        self.lins.append(torch.nn.Linear(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.lins.append(torch.nn.Linear(hidden_channels, hidden_channels))\n",
    "        self.lins.append(torch.nn.Linear(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for lin in self.lins:\n",
    "            lin.reset_parameters()\n",
    "\n",
    "    def forward(self, x_i, x_j):\n",
    "        x = x_i * x_j\n",
    "        for lin in self.lins[:-1]:\n",
    "            x = lin(x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.lins[-1](x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Generation\n",
    "\n",
    "https://networkx.org/documentation/latest/tutorial.html#graph-generators-and-graph-operations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate ER-graph Dataset: 1. invariant to node; 2. variant to node.\n",
    "# Generate SBM-graph Dataset: 1. invariant to node; 2. variant to node.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# synthetic dataset generation\n",
    "---\n",
    "- a) `permutation invariant`\n",
    "- b) `permutation variant`\n",
    "- c) `build pipeline to construct node feautures.`\n",
    "- d) `build pipeline to construct graph feautures, e.g., laplacian matrix, wavelets, etc.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset_utils.synthetic_dataset_generator as data_gen\n",
    "\n",
    "# train_adjs, train_y, test_adjs, test_y = data_gen.generate_circulant_graph_samples(4, N=7, S=[2, 3, 4])\n",
    "# nx.draw_circular(nx.from_numpy_array(train_adjs[0].todense()))\n",
    "# print(train_adjs[0].todense())\n",
    "# plt.figure()\n",
    "# nx.draw_circular(nx.from_numpy_array(train_adjs[1].todense()))\n",
    "# plt.figure()\n",
    "# nx.draw_circular(nx.from_numpy_array(train_adjs[2].todense()))\n",
    "\n",
    "# # TODO: Permutation\n",
    "# per = np.random.permutation(list(np.arange(0, 8)))\n",
    "# A = train_adjs[0].todense()\n",
    "# print('A', A)\n",
    "# A = A[per, :]\n",
    "# A = A[:, per]\n",
    "\n",
    "# gp = nx.from_numpy_array(A)\n",
    "# Ap = nx.to_scipy_sparse_matrix(gp)\n",
    "# print('A per:', Ap.todense())\n",
    "\n",
    "\n",
    "# def test2(f):\n",
    "#     def wrap(adj, **xargs):\n",
    "#         return f(adj, **xargs)\n",
    "#     return wrap\n",
    "\n",
    "# @test2\n",
    "# def test(adj, tt=0):\n",
    "#     print('tt is',tt)\n",
    "\n",
    "# def test1(**xargs):\n",
    "#     a = xargs\n",
    "#     print('type:', a is None)\n",
    "#     test('adj', **a)\n",
    "\n",
    "\n",
    "# test1()\n",
    "g = nx.circulant_graph(8, [1, 3])\n",
    "g1 = nx.circulant_graph(4, [1])\n",
    "\n",
    "gg = nx.disjoint_union_all([g, g, g1])\n",
    "subg = nx.subgraph(gg, list(range(0, 15, 1)))\n",
    "nx.draw_circular(subg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Given dataset, compare with:\n",
    "* MLP\n",
    "* CNN\n",
    "* GNN\n",
    "* GAE\n",
    "* GGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP model\n",
    "import models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from models import BaseGraph, BaseGraphUtils\n",
    "\n",
    "\n",
    "\n",
    "def mirror_adj(a: torch.Tensor):\n",
    "    upper_tri = torch.triu(a)\n",
    "    a1 = (upper_tri+upper_tri.T).fill_diagonal_(1.0)\n",
    "    lower_tri = torch.tril(a)\n",
    "    a2 = (lower_tri+lower_tri.T).fill_diagonal_(1.0)\n",
    "    return a1, a2\n",
    "    \n",
    "\n",
    "\n",
    "class DirectModelAdapter(nn.Module):\n",
    "    def __init__(self, model, pooling, out_dim, node_fea=None):\n",
    "        super(DirectModelAdapter, self).__init__()\n",
    "        self.model = model\n",
    "        self.pooling = pooling\n",
    "        self.node_fea = node_fea\n",
    "        self.ln = nn.Linear(model.out_dim, out_dim)\n",
    "    \n",
    "    def forward(self, graphs:BaseGraph):\n",
    "        \n",
    "        node_x, adj = graphs.get_node_features(), graphs.A\n",
    "        if self.node_fea is not None:\n",
    "            node_x = self.node_fea\n",
    "            \n",
    "        # print(graphs.graph_type)\n",
    "        if graphs.graph_type == 'pyg':\n",
    "            dense_a = graphs.pyg_graph.edge_index.to_dense()\n",
    "            dense_a1, dense_a2 = mirror_adj(dense_a)\n",
    "            # TODO: dense to sparse.\n",
    "            coo1 = BaseGraphUtils.dense_to_coo(dense_a1)\n",
    "            coo2 = BaseGraphUtils.dense_to_coo(dense_a2)\n",
    "            edge_index1 = coo1.indices()\n",
    "            edge_index2 = coo2.indices()\n",
    "            \n",
    "        else:\n",
    "            adj1 = []\n",
    "            adj2 = []\n",
    "            for a in adj:\n",
    "                a1, a2 = mirror_adj(a)\n",
    "                adj1.append(a1)\n",
    "                adj2.append(a2)\n",
    "                \n",
    "            edge_index1 = torch.stack(adj1, dim=0)\n",
    "            edge_index2 = torch.stack(adj2, dim=0)\n",
    "        \n",
    "        out = self.model(node_x, edge_index1, edge_index2, graphs)\n",
    "        out = self.pooling(out)\n",
    "        out = self.ln(out)\n",
    "        return out\n",
    "\n",
    "        \n",
    "class GNNModelAdapter(nn.Module):\n",
    "    def __init__(self, model, pooling, out_dim, node_fea=None, mid_fea=False, device='cpu',\n",
    "    neighbor_k=1):\n",
    "        super(GNNModelAdapter, self).__init__()\n",
    "        self.neighbor_k = neighbor_k\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        \n",
    "        self.mid_fea = mid_fea\n",
    "        self.pooling = pooling\n",
    "        self.node_fea = node_fea\n",
    "        self.ln = nn.Linear(model.out_dim, out_dim)\n",
    "        self.mask_vectors = None\n",
    "        self.to(device)\n",
    "\n",
    "    def mask(self, M=3, mask_len=128):\n",
    "        \"\"\"\n",
    "            set learnable mask vectors, M \\times mask_len\n",
    "        \"\"\"\n",
    "        self.mask_vectors = nn.Parameter(torch.ones(M, mask_len)).to(self.device)\n",
    "\n",
    "\n",
    "    # def preprocess_edge_index(self, edge_index:Tensor):\n",
    "    #     if self.neighbor_k == 1:\n",
    "    #         return edge_index\n",
    "\n",
    "    #     if isinstance(edge_index, SparseTensor):\n",
    "    #         edge_index = edge_index.set_value(None, layout=None)\n",
    "    #         ori = edge_index\n",
    "    #         for _ in range(self.neighbor_k-1):\n",
    "    #             edge_index = torch_sparse.matmul(edge_index, ori)\n",
    "    #     else:\n",
    "    #         ori = edge_index\n",
    "    #         spm = torch.sparse_coo_tensor(ori, None, (2, 3))\n",
    "    #         for _ in range(self.neighbor_k-1):\n",
    "    #             print('edge_index shape:', edge_index.shape)\n",
    "    #             print('ori shape:', ori.shape)\n",
    "                \n",
    "    #             edge_index = torch.matmul(edge_index, ori)\n",
    "        \n",
    "    #     return edge_index\n",
    "        \n",
    "\n",
    "    def forward(self, graphs:BaseGraph):\n",
    "        node_x, adj = graphs.get_node_features(), graphs.A\n",
    "        if self.node_fea is not None:\n",
    "            node_x = self.node_fea\n",
    "        \n",
    "        if self.mask_vectors is not None:\n",
    "            node_x = torch.cat([node_x * self.mask_vectors[i] for i in range(self.mask_vectors.shape[0])], dim=-1)\n",
    "\n",
    "        if graphs.graph_type == 'pyg' or graphs.graph_type == 'coo':\n",
    "            edge_indices = graphs.get_edge_index()\n",
    "\n",
    "            mid_feature = self.model(node_x, edge_indices, graphs=graphs)\n",
    "        else:\n",
    "            mid_feature = self.model(node_x, adj, graphs)\n",
    "            \n",
    "        out = self.pooling(mid_feature, graphs.pyg_graph.batch) if self.pooling is not None else mid_feature\n",
    "        out = self.ln(out)\n",
    "        if self.mid_fea:\n",
    "            return out, mid_feature\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "\n",
    "def choose_model(args, name, node_fea_dim=1, graph_fea_dim=1, class_num=6, node_num=20,\n",
    "                 out_mid_fea=False, node_wise=False):\n",
    "    import importlib\n",
    "    \n",
    "    # import baseline_models.gnn_lspe.nets.OGBMOL_graph_classification.gatedgcn_net as lspe_net\n",
    "    # import baseline_models.gnn_lspe.layers.gatedgcn_layer as lspe_layers\n",
    "\n",
    "    import baseline_models.gnn_baselines\n",
    "    from baseline_models import identity_GNN\n",
    "\n",
    "\n",
    "    from torch_geometric.nn import global_mean_pool, global_add_pool, global_max_pool, global_sort_pool\n",
    "    \n",
    "    importlib.reload(models)\n",
    "    importlib.reload(identity_GNN)\n",
    "    \n",
    "    \n",
    "    N = node_num\n",
    "    if name =='mlp':\n",
    "        my_model = models.ClassPredictor(N*N, 64, class_num, 3, dropout=0.6)\n",
    "    elif name == 'cnn':\n",
    "        my_model = models.SimpleCNN(graph_fea_dim, 64, class_num, dropout=0.6)\n",
    "    elif name == 'cnn_big':\n",
    "        my_model = models.SimpleCNN(graph_fea_dim, 64, class_num, dropout=0.6, kernelsize=(11, 11))\n",
    "    elif name == 'gnn':\n",
    "        # pool =  models.GateGraphPooling(None, N = N)\n",
    "        pool = global_mean_pool\n",
    "        layer_num = 2\n",
    "        my_model = GNNModelAdapter(models.MultilayerGNN(layer_num, node_fea_dim, 64, 32, dropout=0.6), pool, class_num)\n",
    "    elif name == 'gin':\n",
    "        # pool =  models.GateGraphPooling(None, N = N)\n",
    "        if node_wise:\n",
    "            pool = None\n",
    "        else:\n",
    "            pool = models.MeanPooling()\n",
    "            # pool = global_mean_pool\n",
    "        layer_num = 3\n",
    "        my_model = GNNModelAdapter(models.GINNet(args, node_fea_dim, 64, 32, 3, dropout=0.6), pool, class_num, mid_fea=out_mid_fea)\n",
    "    elif name == 'sparse_gin':\n",
    "        if node_wise:\n",
    "            pool = None\n",
    "        else:\n",
    "            pool = global_mean_pool\n",
    "            # pool = global_mean_pool\n",
    "        hid_dim = args.gnn_hid_dim # 128\n",
    "        layer_num = args.gnn_layer_num\n",
    "        bi_direct = args.bi\n",
    "        rep_fea_dim = args.rep_fea_dim # 64\n",
    "        my_model = GNNModelAdapter(models.LSDGINNet(args, node_fea_dim, hid_dim, rep_fea_dim, layer_num, dropout=0.5, bi=bi_direct),\n",
    "                     pool, class_num, mid_fea=out_mid_fea)\n",
    "                     \n",
    "    elif name == 'sparse_gin_mask':\n",
    "        if node_wise:\n",
    "            pool = None\n",
    "        else:\n",
    "            pool = global_mean_pool\n",
    "            # pool = global_mean_pool\n",
    "        layer_num = 3\n",
    "        M = 3\n",
    "        mask_len = node_fea_dim\n",
    "        node_fea_dim = M * mask_len\n",
    "        my_model = GNNModelAdapter(models.LSDGINNet(args, node_fea_dim, 128, 64, 3, dropout=0.5, bi=False),\n",
    "         pool, class_num, mid_fea=out_mid_fea, device='cuda:0')\n",
    "        my_model.mask(M, mask_len=mask_len)\n",
    "\n",
    "    elif name == 'gin_direc':\n",
    "        pool =  models.GateGraphPooling(None, N = N)\n",
    "        layer_num = 3\n",
    "        di_model = models.DiGINNet(args, node_fea_dim, 64, 32, 3, dropout=0.6)\n",
    "        my_model = DirectModelAdapter(di_model, pool, class_num)\n",
    "    elif name == 'lsd_gin':\n",
    "        pool = global_mean_pool\n",
    "        # pool = global_add_pool\n",
    "        layer_num = 3\n",
    "        di_model = models.LSDGINNet(args, node_fea_dim, 64, 32, 3, dropout=0.6)\n",
    "        my_model = DirectModelAdapter(di_model, pool, class_num)\n",
    "    \n",
    "    elif name == 'idgnn':\n",
    "        pool = global_mean_pool\n",
    "        layer_num = 3\n",
    "        idgnn_model = identity_GNN.IDGNN(args, node_fea_dim, 64, 32, layer_num)\n",
    "        my_model = GNNModelAdapter(idgnn_model, pool, class_num, mid_fea=out_mid_fea)\n",
    "        \n",
    "    elif name == 'gcn':        \n",
    "        pool = global_mean_pool\n",
    "        layer_num = 3\n",
    "        idgnn_model = models.MultiGCN(node_fea_dim, 128, 64, layer_num)\n",
    "        my_model = GNNModelAdapter(idgnn_model, pool, class_num, mid_fea=out_mid_fea)\n",
    "    elif name == 'lspe':\n",
    "        pe_init = 'lap_pe'\n",
    "        pos_enc_dim = 8\n",
    "        in_dim = node_fea_dim\n",
    "        hid_dim = 64\n",
    "        out_dim = 32\n",
    "        layer_num= 3\n",
    "        lspe_model = lspe_net.ReGatedGCNNet(pe_init, pos_enc_dim, in_dim,\n",
    "                                       hid_dim,out_dim,layer_num=layer_num)\n",
    "        \n",
    "        pool =  models.GateGraphPooling(None, N = N)\n",
    "        my_model = GNNModelAdapter(lspe_model, pool, class_num)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    my_model.cuda()\n",
    "\n",
    "    return my_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "proj_path = '/li_zhengdao/github/generativegnn'\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "class GraphEvaluator:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def evaluate(self, preds, labels, loss):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def statistic(self, epoch):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class SimpleEvaluator(GraphEvaluator):\n",
    "    def __init__(self, args, is_regression=False):\n",
    "        super(SimpleEvaluator, self).__init__()\n",
    "        self.args = args\n",
    "        self.metrics = defaultdict(list)\n",
    "        self.preds = []\n",
    "        self.labels = []\n",
    "        self.epoch_loss = []\n",
    "        self.mean_metrics = defaultdict(list)\n",
    "        self.total_metrics = {}\n",
    "        self.is_regression = is_regression\n",
    "        \n",
    "    def evaluate(self, preds, labels, loss, null_val=0.0):\n",
    "        \n",
    "        if self.is_regression:\n",
    "            preds_b = preds\n",
    "            if np.isnan(null_val):\n",
    "                mask = ~torch.isnan(labels)\n",
    "            else:\n",
    "                mask = (labels != null_val)\n",
    "                mask = mask.float()\n",
    "                mask /= torch.mean(mask)\n",
    "                # handle all zeros.\n",
    "                mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n",
    "                mse = (preds - labels) ** 2\n",
    "                mae = loss\n",
    "                mape = mae / labels\n",
    "                mae, mape, mse = [mask_and_fillna(l, mask) for l in [mae, mape, mse]]\n",
    "                rmse = torch.sqrt(mse)\n",
    "                \n",
    "                self.metrics['mae'].append(mae.item())\n",
    "                self.metrics['mape'].append(mape.item())\n",
    "                self.metrics['rmse'].append(rmse.item())\n",
    "                self.metrics['loss'].append(loss)\n",
    "            \n",
    "        else:\n",
    "            num = preds.size(0)\n",
    "            # print('evl preds shape:', preds.shape, labels.shape)\n",
    "            preds_b = preds.argmax(dim=1).squeeze()\n",
    "            labels = labels.squeeze()\n",
    "            ones = torch.zeros(num)\n",
    "            ones[preds_b == labels] = 1\n",
    "            acc = torch.sum(ones) / num\n",
    "                \n",
    "            mi_f1, ma_f1, weighted_f1 = utils.cal_f1(preds_b.cpu().detach().numpy(), labels.cpu().detach().numpy())\n",
    "            self.metrics['micro_f1'].append(mi_f1)\n",
    "            self.metrics['macro_f1'].append(ma_f1)\n",
    "            self.metrics['weighted_f1'].append(weighted_f1)\n",
    "            self.metrics['acc'].append(acc.numpy())\n",
    "            self.metrics['loss'].append(loss)\n",
    "        \n",
    "        self.preds.append(preds_b)\n",
    "        self.labels.append(labels)\n",
    "        \n",
    "        \n",
    "    def statistic(self, epoch):\n",
    "        for k, v in self.metrics.items():\n",
    "            self.mean_metrics[k].append(np.mean(v))\n",
    "            \n",
    "        self.metrics = defaultdict(list)\n",
    "            \n",
    "    def eval_on_test(self):\n",
    "        if self.is_regression:\n",
    "            preds = torch.cat(self.preds, dim=0)\n",
    "            labels = torch.cat(self.labels, dim=0)\n",
    "            mask = (labels != 0.0)\n",
    "            mask = mask.float()\n",
    "            mask /= torch.mean(mask)\n",
    "            # handle all zeros.\n",
    "            mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n",
    "            mse = (preds - labels) ** 2\n",
    "            mae = torch.abs(preds - labels)\n",
    "            mape = mae / labels\n",
    "            mae, mape, mse = [mask_and_fillna(l, mask) for l in [mae, mape, mse]]\n",
    "            rmse = torch.sqrt(mse)\n",
    "            \n",
    "            self.metrics['mae'].append(mae)\n",
    "            self.metrics['mape'].append(mape)\n",
    "            self.metrics['rmse'].append(rmse)\n",
    "            \n",
    "        else:\n",
    "            preds = torch.cat(self.preds, dim=0)\n",
    "            labels = torch.cat(self.labels, dim=0)\n",
    "            mi_f1, ma_f1, weighted_f1 = utils.cal_f1(preds.cpu().detach().numpy(), labels.cpu().detach().numpy())\n",
    "            self.total_metrics['micro_f1'] = mi_f1\n",
    "            self.total_metrics['macro_f1'] = ma_f1\n",
    "            self.total_metrics['weighted_f1'] = weighted_f1\n",
    "    \n",
    "    def print_info(self):\n",
    "        micro_f1 = self.metrics[-1]['micro_f1']\n",
    "        macro_f1 = self.metrics[-1]['macro_f1']\n",
    "        weighted_f1 = self.metrics[-1]['weighted_f1']\n",
    "        acc = self.metrics[-1]['acc']\n",
    "        loss =  self.metrics[-1]['loss']\n",
    "        print('------------- metrics -------------------')\n",
    "        print(f'micro f1: {\"%.3f\" % micro_f1}, macro f1:  {\"%.3f\" % macro_f1},weighted f1:  {\"%.3f\" % weighted_f1},\\n \\\n",
    "            acc:  {\"%.3f\" % acc}, loss: {\"%.3f\" % loss}')\n",
    "        \n",
    "    \n",
    "        \n",
    "    def plot_metrics(self):\n",
    "        \n",
    "        preds = torch.cat(self.preds, dim=0)\n",
    "        labels = torch.cat(self.labels, dim=0)\n",
    "        \n",
    "        if not self.is_regression:\n",
    "            plot_confuse_matrix(preds, labels)\n",
    "        \n",
    "        loss_list = self.mean_metrics['loss']\n",
    "        \n",
    "        plot_loss(loss_list)\n",
    "        \n",
    "        \n",
    "        \n",
    "def base_args():\n",
    "    args = utils.get_common_args()\n",
    "    args = args.parse_args([])\n",
    "    \n",
    "    args.debug = True\n",
    "    utils.DLog.init(args)\n",
    "    args.lr=0.00004\n",
    "    args.cuda=True\n",
    "    return args\n",
    "\n",
    "\n",
    "def training(epochs, trainer, train_evaluator, test_evaluator:SimpleEvaluator, train_dataloader, \n",
    "             test_dataloader, cuda=True):\n",
    "    for e in range(epochs):\n",
    "        for x, y in train_dataloader:\n",
    "            if cuda:\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "            y = y.squeeze()\n",
    "            loss, pred_y = trainer.train(x, y)\n",
    "            train_evaluator.evaluate(pred_y, y, loss)\n",
    "        \n",
    "        for x, y in test_dataloader:\n",
    "            if cuda:\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "            y = y.squeeze()\n",
    "            loss, pred_y = trainer.eval(x, y)\n",
    "            test_evaluator.evaluate(pred_y, y, loss)\n",
    "        \n",
    "        train_evaluator.statistic(e)\n",
    "        test_evaluator.statistic(e)\n",
    "        \n",
    "    train_evaluator.eval_on_test()\n",
    "    test_evaluator.eval_on_test()\n",
    "        \n",
    "      \n",
    "def plot_confuse_matrix(preds, labels):\n",
    "    \n",
    "    sns.set()\n",
    "    fig = plt.figure(figsize=(3, 1.5),tight_layout=True, dpi=150)\n",
    "    ax = fig.gca()\n",
    "    gts = [int(l) for l in labels]\n",
    "    preds = [int(l) for l in preds]\n",
    "    \n",
    "    label_names = list(set(preds))\n",
    "    C2= np.around(confusion_matrix(gts, preds, labels=label_names, normalize='true'), decimals=2)\n",
    "\n",
    "    # from confusion to ACC, micro-F1, macro-F1, weighted-f1.\n",
    "    print('Confusion:', C2)\n",
    "    font_size = 6\n",
    "    p = sns.heatmap(C2, cbar=False, annot=True, ax=ax, cmap=\"YlGnBu\", square=False, annot_kws={\"size\":font_size},\n",
    "        yticklabels=label_names,xticklabels=label_names)\n",
    "    \n",
    "    ax.tick_params(axis='x', labelsize=font_size)\n",
    "    ax.tick_params(axis='y', labelsize=font_size)\n",
    "    plt.tight_layout()\n",
    "\n",
    "            \n",
    "\n",
    "def plot_loss(loss):\n",
    "    fig = plt.figure(figsize=(3, 1.5), tight_layout=True, dpi=150)\n",
    "    plt.plot(loss)\n",
    "    plt.xticks(fontsize=6)\n",
    "    plt.yticks(fontsize=6)\n",
    "    # plt.ylim((0, 1.0))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def mask_and_fillna(loss, mask):\n",
    "    loss = loss * mask\n",
    "    loss = torch.where(torch.isnan(loss), torch.zeros_like(loss), loss)\n",
    "    return torch.mean(loss)\n",
    "\n",
    "\n",
    "class MAECal(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MAECal, self).__init__()\n",
    "        pass\n",
    "    \n",
    "    def forward(self, pred_y, labels, null_val=0.0):\n",
    "        \n",
    "        labels = labels.squeeze()\n",
    "        if np.isnan(null_val):\n",
    "            mask = ~torch.isnan(labels)\n",
    "        else:\n",
    "            mask = (labels != null_val)\n",
    "        \n",
    "        mask = mask.float()\n",
    "        mask /= torch.mean(mask)\n",
    "        mask = torch.where(torch.isnan(mask), torch.zeros_like(mask), mask)\n",
    "\n",
    "        mae = torch.abs(pred_y - labels)\n",
    "        mae = mask_and_fillna(mae, mask)\n",
    "        return mae\n",
    "\n",
    "class CELossCal(nn.Module):\n",
    "    def __init__(self, weights=None):\n",
    "        super(CELossCal, self).__init__()\n",
    "        self.crite = nn.CrossEntropyLoss(weight=weights)\n",
    "        \n",
    "    def forward(self, pred_y, y):\n",
    "        return self.crite(pred_y, y)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOTA methods: GraphSNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_graphsnn_data(adjs):\n",
    "    adjs = [ad.toarray() for ad in adjs]\n",
    "    \n",
    "    new_adjs = []\n",
    "    dataset_length = len(adjs)\n",
    "    for itr in np.arange(dataset_length):\n",
    "        A_array = adjs[itr]\n",
    "        G = nx.from_numpy_matrix(A_array)\n",
    "        \n",
    "        sub_graphs = []\n",
    "        subgraph_nodes_list = []\n",
    "        sub_graphs_adj = []\n",
    "        sub_graph_edges = []\n",
    "        new_adj = torch.zeros(A_array.shape[0], A_array.shape[0])\n",
    "\n",
    "        for i in np.arange(len(A_array)):\n",
    "            s_indexes = []\n",
    "            for j in np.arange(len(A_array)):\n",
    "                s_indexes.append(i)\n",
    "                if(A_array[i][j]==1):\n",
    "                    s_indexes.append(j)\n",
    "            sub_graphs.append(G.subgraph(s_indexes))\n",
    "\n",
    "        for i in np.arange(len(sub_graphs)):\n",
    "            subgraph_nodes_list.append(list(sub_graphs[i].nodes))\n",
    "            \n",
    "        for index in np.arange(len(sub_graphs)):\n",
    "            sub_graphs_adj.append(nx.adjacency_matrix(sub_graphs[index]).toarray())\n",
    "            \n",
    "        for index in np.arange(len(sub_graphs)):\n",
    "            sub_graph_edges.append(sub_graphs[index].number_of_edges())\n",
    "\n",
    "        for node in np.arange(len(subgraph_nodes_list)):\n",
    "            sub_adj = sub_graphs_adj[node]\n",
    "            for neighbors in np.arange(len(subgraph_nodes_list[node])):\n",
    "                index = subgraph_nodes_list[node][neighbors]\n",
    "                count = torch.tensor(0).float()\n",
    "                if(index==node):\n",
    "                    continue\n",
    "                else:\n",
    "                    c_neighbors = set(subgraph_nodes_list[node]).intersection(subgraph_nodes_list[index])\n",
    "                    if index in c_neighbors:\n",
    "                        nodes_list = subgraph_nodes_list[node]\n",
    "                        sub_graph_index = nodes_list.index(index)\n",
    "                        c_neighbors_list = list(c_neighbors)\n",
    "                        for i, item1 in enumerate(nodes_list):\n",
    "                            if(item1 in c_neighbors):\n",
    "                                for item2 in c_neighbors_list:\n",
    "                                    j = nodes_list.index(item2)\n",
    "                                    count += sub_adj[i][j]\n",
    "\n",
    "                    new_adj[node][index] = count/2\n",
    "                    new_adj[node][index] = new_adj[node][index]/(len(c_neighbors)*(len(c_neighbors)-1))\n",
    "                    new_adj[node][index] = new_adj[node][index] * (len(c_neighbors)**2)\n",
    "\n",
    "        weight = torch.FloatTensor(new_adj)\n",
    "        weight = weight / weight.sum(1, keepdim=True)\n",
    "        \n",
    "        weight = weight + torch.FloatTensor(A_array)\n",
    "\n",
    "        coeff = weight.sum(1, keepdim=True)\n",
    "        coeff = torch.diag((coeff.T)[0])\n",
    "        \n",
    "        weight = weight + coeff\n",
    "\n",
    "        weight = weight.detach().numpy()\n",
    "        weight = np.nan_to_num(weight, nan=0)\n",
    "\n",
    "        # TODO: transform to COO:\n",
    "        weight = utils.numpy_to_csr(weight).tocoo()\n",
    "        new_adjs.append(weight)\n",
    "        \n",
    "    return new_adjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "# use MLP to trian:\n",
    "\n",
    "def train_mlp(train_loader, test_loader, epoch=200, plot=False):\n",
    "    args = base_args()\n",
    "    mlp_model = choose_model('mlp')\n",
    "\n",
    "    args.debug = False\n",
    "    utils.DLog.init(args)\n",
    "    # opt = optim.Adam(mlp_model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    opt = optim.SGD(mlp_model.parameters(), lr=0.001)\n",
    "    ce_loss_cal = CELossCal()\n",
    "\n",
    "    trainer = utils.Trainer(mlp_model, optimizer=opt, loss_cal=ce_loss_cal)\n",
    "    train_sim_evl= SimpleEvaluator(args)\n",
    "    test_sim_evl= SimpleEvaluator(args)\n",
    "\n",
    "    training(epoch, trainer, train_sim_evl, test_sim_evl, train_loader, test_loader)\n",
    "\n",
    "    if plot:\n",
    "        train_sim_evl.plot_metrics()\n",
    "        test_sim_evl.plot_metrics()\n",
    "    \n",
    "    return train_sim_evl, test_sim_evl\n",
    "\n",
    "# _,_ = train_mlp(va_train_dataloader, va_test_dataloader, epoch=1, plot=True)\n",
    "# train_mlp(inva_train_dataloader, inva_test_dataloader, True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "\n",
    "\n",
    "def train_cnn(train_loader, test_loader, epoch=200, plot=False, cnn_name='cnn', graph_fea_dim=1):\n",
    "# use CNN to trian:\n",
    "    args = base_args()\n",
    "    cnn_model = choose_model(cnn_name, graph_fea_dim=graph_fea_dim)\n",
    "    # opt = optim.Adam(mlp_model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    opt = optim.SGD(cnn_model.parameters(), lr=0.0001)\n",
    "    ce_loss_cal = CELossCal()\n",
    "\n",
    "    trainer = utils.Trainer(cnn_model, optimizer=opt, loss_cal=ce_loss_cal)\n",
    "    train_sim_evl= SimpleEvaluator(args)\n",
    "    test_sim_evl= SimpleEvaluator(args)\n",
    "\n",
    "    training(epoch, trainer, train_sim_evl, test_sim_evl,train_loader,test_loader)\n",
    "    if plot:\n",
    "        train_sim_evl.plot_metrics()\n",
    "        test_sim_evl.plot_metrics()\n",
    "        \n",
    "    return train_sim_evl, test_sim_evl\n",
    "    \n",
    "# _, _ = train_cnn(va_train_dataloader, va_test_dataloader, epoch=100, plot=True)\n",
    "# train_cnn(inva_train_dataloader, inva_test_dataloader, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN:\n",
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "import models\n",
    "importlib.reload(models)\n",
    "\n",
    "def train_gnn(args, train_loader, test_loader, gnn_name='gnn', epoch=200, plot=False, node_fea_dim=1,\n",
    "              class_num=6, node_num=20, **xargs):\n",
    "        \n",
    "    def get_value(key, default=None):\n",
    "        return xargs[key] if key in xargs else default\n",
    "    \n",
    "    # use GNN to trian:\n",
    "    args.debug = False\n",
    "    utils.DLog.init(args)\n",
    "    \n",
    "    is_node_wise = get_value('is_node_wise', False)\n",
    "    is_regression = get_value('is_regression', False)\n",
    "    scaler = get_value('scaler')\n",
    "    opt = get_value('opt', 'sgd')\n",
    "    lr = xargs['lr'] if 'lr' in xargs else 0.0002\n",
    "    \n",
    "    \n",
    "    gnn_model = choose_model(args, gnn_name, node_fea_dim=node_fea_dim, class_num=class_num,\n",
    "                             node_num=node_num,\n",
    "                             node_wise=is_node_wise)\n",
    "    if opt == 'adam':\n",
    "        opt = optim.Adam(gnn_model.parameters(), lr=lr, weight_decay=args.weight_decay)\n",
    "    else:\n",
    "        opt = optim.SGD(gnn_model.parameters(), lr=lr)\n",
    "    \n",
    "    \n",
    "    loss_cal = get_value('loss_cal', None)\n",
    "    if loss_cal is None:\n",
    "        loss_cal =  MAECal() if is_regression else CELossCal()\n",
    "    elif loss_cal == 'mse':\n",
    "        loss_cal = nn.MSELoss()\n",
    "    elif loss_cal == 'mae':\n",
    "        loss_cal = MAECal()\n",
    "        \n",
    "\n",
    "    trainer = utils.Trainer(gnn_model, optimizer=opt, loss_cal=loss_cal, scaler=scaler)\n",
    "    train_sim_evl= SimpleEvaluator(args, is_regression=is_regression)\n",
    "    test_sim_evl= SimpleEvaluator(args,is_regression=is_regression)\n",
    "\n",
    "    training(epoch, trainer, train_sim_evl, test_sim_evl, train_loader, test_loader)\n",
    "\n",
    "    if plot:\n",
    "        train_sim_evl.plot_metrics()\n",
    "        test_sim_evl.plot_metrics()\n",
    "        \n",
    "    return train_sim_evl, test_sim_evl, gnn_model\n",
    "\n",
    "# train_gnn(va_train_dataloader, va_test_dataloader, True)\n",
    "# train_gnn(inva_train_dataloader, inva_test_dataloader, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCA function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ppp(*values: object):\n",
    "    print('------------------------------')\n",
    "    print(*values)\n",
    "    \n",
    "\n",
    "def construct_dataset(graph_data, node_features, norm=True, lap_encode=False, \\\n",
    "                      lap_en_dim=8, y_torch_type=torch.LongTensor, sparse=False, K=1, regression=True):\n",
    "    \n",
    "    train_adjs, train_y, test_adjs, test_y = graph_data\n",
    "    # construct node features:\n",
    "    train_node_fea, val_node_fea = node_features\n",
    "    scalers_y = []\n",
    "    if norm:\n",
    "        if sparse:\n",
    "            # NOTE: normlize through each feature dimension.\n",
    "            scalers = []\n",
    "            train_node_feas = np.concatenate(train_node_fea, axis=0)\n",
    "            val_node_feas = np.concatenate(val_node_fea, axis=0)\n",
    "            print('feature len:', train_node_fea[0].shape[-1])\n",
    "            for fea_i in range(train_node_fea[0].shape[-1]):\n",
    "                _, scaler = utils.normalize(train_node_feas[..., fea_i])\n",
    "                scalers.append(scaler)\n",
    "                _, scaler = utils.normalize(val_node_feas[..., fea_i])\n",
    "            \n",
    "            for i in range(len(train_node_fea)):\n",
    "                for fea_i in range(train_node_fea[0].shape[-1]):\n",
    "                    train_node_fea[i][..., fea_i] = scalers[fea_i].transform(train_node_fea[i][..., fea_i])\n",
    "                    \n",
    "            for i in range(len(val_node_fea)):\n",
    "                for fea_i in range(val_node_fea[0].shape[-1]):\n",
    "                    val_node_fea[i][..., fea_i] = scalers[fea_i].transform(val_node_fea[i][..., fea_i])\n",
    "            # TODO: fill nan:\n",
    "            for i in range(len(train_node_fea)):\n",
    "                train_node_fea[i] = utils.fill_nan_inf(train_node_fea[i])\n",
    "                \n",
    "            for i in range(len(val_node_fea)):\n",
    "                val_node_fea[i] = utils.fill_nan_inf(val_node_fea[i])\n",
    "                \n",
    "            # TODO: normalize label y:\n",
    "            if regression:\n",
    "                train_y, scaler_train_y = utils.normalize(train_y)\n",
    "                test_y, scaler_test_y = utils.normalize(test_y)\n",
    "                \n",
    "                train_y =  utils.fill_nan_inf(train_y)\n",
    "                test_y =  utils.fill_nan_inf(test_y)\n",
    "                scalers_y.append(scaler_train_y)\n",
    "                scalers_y.append(scaler_test_y)\n",
    "            else:\n",
    "                if not isinstance(train_y[0], np.ndarray):\n",
    "                    train_y = [np.array([y]).astype(np.int32) for y in train_y]\n",
    "                    test_y = [np.array([y]).astype(np.int32) for y in test_y]\n",
    "                \n",
    "                train_y = np.concatenate(train_y, axis=0)\n",
    "                test_y = np.concatenate(test_y, axis=0)\n",
    "            \n",
    "        else:\n",
    "            mean_y = train_node_fea.mean()\n",
    "            std_y = train_node_fea.std()\n",
    "            scaler = utils.StandardScaler(mean=mean_y, std=std_y)\n",
    "            train_node_fea =  utils.fill_nan_inf(scaler.transform(train_node_fea))\n",
    "            val_node_fea =  utils.fill_nan_inf(utils.normalize(val_node_fea))\n",
    "        \n",
    "    else:\n",
    "        scalers = None\n",
    "\n",
    "    train_base_graphs = []\n",
    "    if sparse:\n",
    "        for i, adj in enumerate(train_adjs):\n",
    "            # NOTE: set A as A^K\n",
    "            adj = utils.matrix_power(adj.tocoo(), pow=K)\n",
    "            g = models.BaseGraphUtils.from_scipy_coo(adj.tocoo())\n",
    "            g.set_node_feat(train_node_fea[i])\n",
    "            train_base_graphs.append(g)\n",
    "\n",
    "        test_base_graphs = []\n",
    "        for i, adj in enumerate(test_adjs):\n",
    "            adj = utils.matrix_power(adj.tocoo(), pow=K)\n",
    "            g = models.BaseGraphUtils.from_scipy_coo(adj.tocoo())\n",
    "            g.set_node_feat(val_node_fea[i])\n",
    "            test_base_graphs.append(g)\n",
    "    else:\n",
    "        for i, adj in enumerate(train_adjs):\n",
    "            g = models.BaseGraphUtils.from_numpy(adj)\n",
    "            g.set_node_feat(train_node_fea[i])\n",
    "            train_base_graphs.append(g)\n",
    "\n",
    "        test_base_graphs = []\n",
    "        for i, adj in enumerate(test_adjs):\n",
    "            g = models.BaseGraphUtils.from_numpy(adj)\n",
    "            g.set_node_feat(val_node_fea[i])\n",
    "            test_base_graphs.append(g)\n",
    "    \n",
    "    \n",
    "    train_dataset = models.GraphDataset(x=train_base_graphs, y=y_torch_type(train_y))\n",
    "    test_dataset = models.GraphDataset(x=test_base_graphs, y=y_torch_type(test_y))\n",
    "\n",
    "    if lap_encode:\n",
    "        train_dataset._add_lap_positional_encodings(lap_en_dim)\n",
    "        test_dataset._add_lap_positional_encodings(lap_en_dim)\n",
    "    \n",
    "    return train_dataset, test_dataset, scalers, scalers_y\n",
    "\n",
    "\n",
    "from models import BaseGraphUtils\n",
    "# to dataloader:\n",
    "import models \n",
    "importlib.reload(models)\n",
    "\n",
    "def assemble_dataloader(train_dataset: models.GraphDataset, test_dataset: models.GraphDataset, cuda=False, batch_size=20):\n",
    "    if cuda:\n",
    "        train_dataset.cuda()\n",
    "        test_dataset.cuda()\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_dataset.collate)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=test_dataset.collate)\n",
    "\n",
    "    def print_tuple(x):\n",
    "        if isinstance(x, list) or isinstance(x, tuple):\n",
    "            adj = x[1]\n",
    "            x = x[0]\n",
    "        \n",
    "    # for x, y in train_dataloader:\n",
    "    #     print(type(x))\n",
    "    #     print_tuple(x)\n",
    "    #     print(y.is_cuda)\n",
    "    #     break\n",
    "    \n",
    "    return (train_dataloader, test_dataloader)\n",
    "\n",
    "def get_mid_feat(cur_model, dataloader, pooled=False, prediction=False, logit=True):\n",
    "    \n",
    "    \n",
    "    new_loader = DataLoader(dataloader.dataset, batch_size=10, shuffle=False, collate_fn=dataloader.dataset.collate)\n",
    "\n",
    "    if prediction:\n",
    "        cur_model.mid_fea = False\n",
    "        cur_model.eval()\n",
    "        \n",
    "        preds = []\n",
    "        for x, _ in new_loader:\n",
    "            x = x.cuda()\n",
    "            out = cur_model(x)\n",
    "            preds.append(out)\n",
    "        \n",
    "        preds = torch.cat(preds, dim=0).detach().cpu().numpy()\n",
    "        if logit:\n",
    "            print('preds shape', preds.shape)\n",
    "        return preds\n",
    "    else:\n",
    "        cur_model.mid_fea = True\n",
    "        cur_model.eval()\n",
    "        \n",
    "        mid_feas = []\n",
    "        for x, _ in new_loader:\n",
    "            x = x.cuda()\n",
    "            _, mid_f_train = cur_model(x)\n",
    "            mid_feas.append(mid_f_train)\n",
    "        mid_feas = torch.cat(mid_feas, dim=0)\n",
    "        if pooled:\n",
    "            mid_feas = cur_model.pooling(mid_feas).detach().cpu().numpy()\n",
    "        if logit:\n",
    "            print('mid_feas shape', mid_feas.shape)\n",
    "        return mid_feas\n",
    "\n",
    "\n",
    "\n",
    "def get_cca_corr(fea1, fea2, CCA=True):\n",
    "    if not isinstance(fea1, np.ndarray):\n",
    "        fea1 = fea1.cpu().numpy()\n",
    "    if not isinstance(fea2, np.ndarray):\n",
    "        fea2 = fea2.cpu().numpy()\n",
    "    \n",
    "    if CCA:\n",
    "        from sklearn.cross_decomposition import CCA\n",
    "        n_compo = 1\n",
    "        cca = CCA(n_components=n_compo, scale=True, max_iter=500, tol=1e-06, copy=True)\n",
    "        cca.fit(fea1, fea2)\n",
    "        X_c, Y_c = cca.transform(fea1, fea2)\n",
    "        r2 = cca.score(fea1, fea2)\n",
    "        print('r2:',r2)\n",
    "    else:\n",
    "        X_c = fea1\n",
    "        Y_c = fea2\n",
    "    # cal correaltion:\n",
    "    from scipy import stats\n",
    "    corr = stats.pearsonr(X_c.squeeze(), Y_c.squeeze())\n",
    "    print(corr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# task1 avg degree, ok\n",
    "# task2 avg CC ok.\n",
    "# task3 node-wise degree, ok\n",
    "# task4, node-wise CC, ok\n",
    "# task5, two label classification ok.\n",
    "# task6, b1b2, node-wise degree, -> b1b2: same degree, different CC\n",
    "# task7, b1b2, graph-wise degree \n",
    "# task8, b3b4, graph-wise degree,  -> same CC, different degree.\n",
    "# task9, b1b2, graph-wise CC\n",
    "# task10, b3b4, graph-wise CC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WS orthogonal labels b1b2 b3b4 compare CCA corr "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WS orthogonal b1b2 b3b4 compare CCA corr "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: 0918. For all benchmark datasets.\n",
    "* check the correlation between degree and CC \n",
    "* or $|C_k|$, $k>3$\n",
    "* Why related to training difficulty? because that GNN cannot learn them.\n",
    "* Why cannot learn? computational graphs are the same and is bounded by k-WL-test.\n",
    "* So requires additional information of subgraphs, but $O(kN)$ complexity.\n",
    "* so sampling may be a good choice.\n",
    "* may use DP to sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* based on above figure, pick same horizontal points\n",
    "- blueK=0.2, p=0.3, orangeK=0.25, p=0.35, greenK=0.3, p= 0.6,\n",
    "- redK=0.35,p=0.6, orangek=0.25, p=0.3 (try this first)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate WS graph node feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# construct node clustring coefficient label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate WS graph dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCA (Canical Component Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check whether GNN can learn the CC ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate CC historgram with same degree\n",
    "# cc without degree correlation bias\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load OGB dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: OGB:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "importlib.reload(utils)\n",
    "\n",
    "import networkx as nx\n",
    "from functools import reduce\n",
    "import models\n",
    "importlib.reload(models)\n",
    "\n",
    "\n",
    "def graphs_statistics(adjs:list, labels:list):\n",
    "    \n",
    "    statistics = []\n",
    "    for i, A in enumerate(adjs):\n",
    "        nx_g = nx.from_numpy_array(A)\n",
    "        avg_cc = nx.average_clustering(nx_g)\n",
    "        avg_degree = np.mean(node_feature_utils.node_degree_feature(adj=A)).item()\n",
    "        tris = np.mean(node_feature_utils.node_tri_cycles_feature(adj=A)).item()\n",
    "        cycles = nx.cycle_basis(nx_g)\n",
    "        statistics.append((avg_cc, avg_degree, tris, cycles, labels[i]))\n",
    "    \n",
    "    return statistics\n",
    "\n",
    "\n",
    "def plot_graphs_statistics(title:str, statistics_sorted:list, data_labels:tuple, axe=None, \n",
    "                           sort_idx=None, show_idx:list=None):\n",
    "    \n",
    "    # NOTE: Sorted by other index\n",
    "    if sort_idx is not None:\n",
    "        statistics_sorted = sorted(statistics_sorted, key=lambda x: x[sort_idx])\n",
    "        \n",
    "    # TODO: Filter cycles and degree outliers, abs(z-score) > 3.\n",
    "    statistics_sorted_filtered = []\n",
    "    for i in statistics_sorted:\n",
    "        # NOTE: filter any outliers:\n",
    "        need_filter = False\n",
    "        for l in range(len(i)):\n",
    "            if i[l] < -3 or i[l] > 3 or i[l] < -3 or i[l] > 3:\n",
    "                need_filter = True\n",
    "                break\n",
    "        if need_filter:\n",
    "            continue\n",
    "        \n",
    "        statistics_sorted_filtered.append(i)\n",
    "    \n",
    "    statistics_sorted = statistics_sorted_filtered\n",
    "    # TODO: Get correlation:\n",
    "    corrs = utils.get_corrs(statistics_sorted, cate='all')\n",
    "    if axe is not None:\n",
    "        axe.set_title(title+f\"\\n C(dc)={round(corrs['pearson'][1, -2], 2)}, C(dy)={round(corrs['pearson'][1, -1], 2)},\" \\\n",
    "                     + f\" C(cy)={round(corrs['pearson'][-2, -1], 2)}\")\n",
    "        # NOTE: C: correlation, d:degree, c: cycles, y:labels of samples.\n",
    "        \n",
    "        \n",
    "        colors = MyColor()\n",
    "        \n",
    "        from matplotlib import lines\n",
    "        \n",
    "        line_styles = list(lines.lineStyles.keys())\n",
    "        def get_style(next_id):\n",
    "            return line_styles[next_id%len(line_styles)]\n",
    "        \n",
    "        if show_idx is None:\n",
    "            show_idx = list(range(len(statistics_sorted[0])))\n",
    "            \n",
    "        for sid in show_idx:\n",
    "            # TODO: color?\n",
    "            axe.plot([i[sid] for i in statistics_sorted], label=data_labels[sid], \n",
    "                     linestyle=get_style(sid), color=colors.get_color())\n",
    "            \n",
    "    else:\n",
    "        plt.figure()\n",
    "        plt.title(title)\n",
    "        plt.plot([i[0] for i in statistics_sorted], label='avg_cc')\n",
    "        plt.plot([i[1] for i in statistics_sorted], label='avg_degree',  linestyle='--')\n",
    "        plt.plot([i[2] for i in statistics_sorted], label='tri_cycles', linestyle='-.')\n",
    "        plt.plot([i[3] for i in statistics_sorted], label='num_cyclesK=5', linestyle='-.')\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_cc_degree_distribution(cc_degree_graphs, cc_degree_y, g_label='train'):\n",
    "    \n",
    "    # random add edges:\n",
    "    # add E edges, repeat for 5 times.\n",
    "    print(type(cc_degree_graphs[0].todense()))\n",
    "\n",
    "    data_graphs = [(cc_degree_y[i],np.mean(np.sum(cc_degree_graphs[i].todense(), axis=1)), np.mean(node_feature_utils.node_tri_cycles_feature(adj=cc_degree_graphs[i])).item(), i) for i in range(len(cc_degree_graphs))]\n",
    "\n",
    "    data_graphs_s_train = sorted(data_graphs, key=lambda x: x[0])\n",
    "\n",
    "    ccs = [d[0] for d in data_graphs_s_train]\n",
    "    degrees = [d[1]/10 for d in data_graphs_s_train]\n",
    "    tri_cycles = [d[2]/4 for d in data_graphs_s_train]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(g_label)\n",
    "    plt.plot(ccs, label='labels')\n",
    "    plt.plot(degrees, label='degree',  linestyle='--')\n",
    "    plt.plot(tri_cycles, label='tri_cycles', linestyle='-.')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "statistics_sorted = [(1, 2, 3), (2,3,4)]\n",
    "cr = pd.DataFrame(statistics_sorted).values\n",
    "print(type(cr))\n",
    "print(cr[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixed cycle graphs:\n",
    "\n",
    "cc_train_adjs, cc_train_y, cc_test_adjs, cc_test_y = data_gen.generate_cc_no_degree_corr_samples(cc_range_num=20)\n",
    "\n",
    "plot_cc_degree_distribution(cc_train_adjs, cc_train_y)\n",
    "plot_cc_degree_distribution(cc_test_adjs, cc_test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx.circulant_graph(10, [1, 2])\n",
    "cycles = nx.cycle_basis(g)\n",
    "counter = defaultdict(int)\n",
    "for i in cycles:\n",
    "    counter[len(i)] += 1\n",
    "print(counter)\n",
    "nx.draw_circular(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Circulant graphs\n",
    "# original paper: [2,3,4,5,6,9,11,12,13,16]\n",
    "S =[2,3,4,5,6,9,11,12,13,16]\n",
    "# S = list(np.arange(2, 10))\n",
    "S = [2, 3, 4, 5]\n",
    "train_adjs, train_y, test_adjs, test_y = data_gen.generate_circulant_graph_samples(each_class_num=40, N=41, S=S)\n",
    "# TODO: predict long cycles.\n",
    "plot_cc_degree_distribution(train_adjs, train_y)\n",
    "# plot_cc_degree_distribution(test_adjs, test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: use graphSNN:\n",
    "train_adjs = set_graphsnn_data(train_adjs)\n",
    "test_adjs = set_graphsnn_data(test_adjs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# construct CC without degree-correlation biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_utils.node_feature_utils import *\n",
    "# NOTE: MixCycle graphs:\n",
    "\n",
    "# NOTE: different init node features affect the performance to discover topology !!!!\n",
    "# NOTE: try different random init node features or learnable node features.\n",
    "\n",
    "node_fea_reg = NodeFeaRegister()\n",
    "node_fea_reg.register('degree')\n",
    "node_fea_reg.register('allone')\n",
    "node_fea_reg.register('guassian', dim=41)\n",
    "node_fea_reg.register('tri_cycle')\n",
    "node_fea_reg.register('kadj', k=2)\n",
    "\n",
    "node_features = construct_node_features((cc_train_adjs, None, cc_test_adjs, None), node_fea_reg)\n",
    "\n",
    "\n",
    "# NOTE: e.g., randomly choise from [1,2,3,4] or [1,2], or [1,2,3,4,...100], try different length.\n",
    "ratios = [0.1, 0.3, 0.6, 1.0]\n",
    "rand_id_fea_trains = []\n",
    "rand_id_fea_tests = []\n",
    "node_fea_reg_ratio = NodeFeaRegister()\n",
    "for ra in ratios:\n",
    "    node_fea_reg_ratio.register('rand_id', ratio=ra)\n",
    "\n",
    "rand_id_fea_list = construct_node_features((cc_train_adjs, None, cc_test_adjs, None), node_fea_reg_ratio)\n",
    "\n",
    "cc_dataset = (cc_train_adjs, cc_train_y, cc_test_adjs, cc_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.range(1, 4).reshape(2, 2)\n",
    "b = torch.stack([a,a,a], dim=0)\n",
    "b = torch.cat([b,b], dim=-1)\n",
    "c = b - 2\n",
    "d = torch.where(a>3, b, c)\n",
    "print('a:', a)\n",
    "print('b:', b)\n",
    "print('c:', c)\n",
    "print('d:', d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Circulant graphs:\n",
    "\n",
    "node_fea_reg = node_feature_utils.NodeFeaRegister()\n",
    "node_fea_reg.register('degree')\n",
    "node_fea_reg.register('allone')\n",
    "node_fea_reg.register('guassian', dim=2)\n",
    "node_fea_reg.register('tri_cycle')\n",
    "node_fea_reg.register('index_id')\n",
    "node_fea_reg.register('kadj', k=2)\n",
    "node_fea_reg.register('kadj', k=3)\n",
    "node_fea_reg.register('kadj', k=4)\n",
    "\n",
    "node_features = construct_node_features((train_adjs, None, test_adjs, None), node_fea_reg)\n",
    "cc_dataset = (train_adjs, train_y, test_adjs, test_y)\n",
    "\n",
    "node_fea_reg.list_registered()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cc_degree(args, cc_dataloader, feature_dim, lr=0.00001, gnn_name='gcn', epochs=300, opt='sgd', regression=True, class_num=1):\n",
    "    import models\n",
    "    import utils\n",
    "    importlib.reload(utils)\n",
    "    importlib.reload(models)\n",
    "\n",
    "    cc_gnn_evls_train = []\n",
    "    cc_gnn_evls_test = []\n",
    "\n",
    "    # TODO: fix the normali\n",
    "    # ws_degree_graphwise_allone_dataloaders\n",
    "    train_dataloader, test_dataloader = cc_dataloader\n",
    "    train_evl, test_evl, cur_model = train_gnn(args, train_dataloader, test_dataloader, gnn_name=gnn_name,\n",
    "                                    epoch=epochs,\n",
    "                                    node_fea_dim=feature_dim,\n",
    "                                    class_num=class_num,\n",
    "                                    node_num=40, lr=lr, is_regression=regression, is_node_wise=False, opt=opt)\n",
    "    \n",
    "    cc_gnn_evls_train.append(train_evl)\n",
    "    cc_gnn_evls_test.append(test_evl)\n",
    "        \n",
    "    cc_gnn_evls_train[0].plot_metrics()\n",
    "    cc_gnn_evls_test[0].plot_metrics()\n",
    "    return cur_model\n",
    "\n",
    "\n",
    "def plot_cc_degree_prediction(models_trained, cur_dataloader, adjs, adj_degree, g_label, regression=True):\n",
    "    \n",
    "    data_graphs = [(adj_degree[i],np.mean(np.sum(adjs[i].todense(), axis=1)),\n",
    "                    np.mean(node_tri_cycles_feature(adj=adjs[i])).item(), i) for i in range(len(adjs))]\n",
    "\n",
    "    data_graphs_s_train = sorted(data_graphs, key=lambda x: x[0])\n",
    "    print('len data graphs:', len(data_graphs_s_train))\n",
    "    \n",
    "    ccs = [d[0] for d in data_graphs_s_train]\n",
    "    degrees = [d[1]/10 for d in data_graphs_s_train]\n",
    "    tri_cycles = [d[2]/4 for d in data_graphs_s_train]\n",
    "\n",
    "    cc_train_preds  = get_mid_feat(models_trained, cur_dataloader, prediction=True, logit=False)\n",
    "    if not regression:\n",
    "        cc_train_preds = np.argmax(cc_train_preds, axis=-1)\n",
    "        print(cc_train_preds.shape)\n",
    "        cc_train_preds = [cc_train_preds[i[-1]] for i in data_graphs_s_train]\n",
    "    else:\n",
    "        cc_train_preds = [cc_train_preds[i[-1]].item() for i in data_graphs_s_train]\n",
    "    \n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(ccs, label='CC labels')\n",
    "    plt.plot(cc_train_preds, label='CC predictions')\n",
    "    plt.plot(degrees, label='Degrees')\n",
    "    # plt.plot(tri_cycles, label='tri_cycles')\n",
    "    plt.title(g_label)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: test different ratios of random number node features. (08.29)\n",
    "* ratios = [0.1, 0.3,0.6, 1.0]\n",
    "* test correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_dataloaders = []\n",
    "for i in range(len(rand_id_fea_trains)):\n",
    "    com_node_features_train = composite_node_features(allone_train_norm, rand_id_fea_trains[i], padding=False)\n",
    "    com_node_features_test = composite_node_features(allone_test_norm, rand_id_fea_tests[i], padding=False)\n",
    "    cc_dataloaders.append(assemble_dataloader(*construct_dataset(cc_dataset,\n",
    "                            (com_node_features_train, com_node_features_test),\n",
    "                             y_torch_type=torch.FloatTensor, sparse=True), cuda=False))\n",
    "\n",
    "\n",
    "kadj_fea_train = composite_node_features(allone_train_norm, kadj_train, padding=True, padding_len=84)\n",
    "kadj_fea_test = composite_node_features(allone_test_norm, kadj_test, padding=True, padding_len=84)\n",
    "cc_dataloaders.append(assemble_dataloader(*construct_dataset(cc_dataset,\n",
    "                        (kadj_fea_train, kadj_fea_test),\n",
    "                            y_torch_type=torch.FloatTensor, sparse=True), cuda=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testbb(*args):\n",
    "    print(args[-1])\n",
    "\n",
    "\n",
    "def testaaa(*args):\n",
    "    print(len(args))\n",
    "    ids = []\n",
    "    for i in args:\n",
    "        ids.append(i)\n",
    "    ids = tuple(ids)\n",
    "    \n",
    "    testbb(*ids)\n",
    "    \n",
    "testaaa(('adf',1), (\"b\", 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: 0906. only all one and index features. only index and k=2 cannot handle [2,3,4],TODO: try k=3, add kadj features.\n",
    "# NOTE: 0907. Adam can converge...\n",
    "\"\"\"\n",
    "index: 0 degree  args:  {}\n",
    "index: 1 allone  args:  {}\n",
    "index: 2 guassian  args:  {}\n",
    "index: 3 tri_cycle  args:  {}\n",
    "index: 4 index_id  args:  {}\n",
    "index: 5 kadj  args:  {'k': 2}\n",
    "index: 6 kadj  args:  {'k': 3}\n",
    "index: 7 kadj  args:  {'k': 4}\n",
    "\"\"\"\n",
    "\n",
    "from xml.dom.domreg import registered\n",
    "\n",
    "\n",
    "def get_features_by_ids(*indices, cur_features, pad=None):\n",
    "    if len(indices) < 2:\n",
    "        return (cur_features[indices[0]][0], cur_features[indices[0]][1])\n",
    "    \n",
    "    train_fea = composite_node_features(*tuple([cur_features[i][0] for i in indices]), padding=pad)\n",
    "    test_fea = composite_node_features(*tuple([cur_features[i][1] for i in indices]), padding=pad)\n",
    "    return (train_fea, test_fea)\n",
    "        \n",
    "\n",
    "# NOTE: k=3 can learn S=[2,3,4], but overfitting, K=2\n",
    "# composed_fea_train, composed_fea_test = get_features_by_ids(5, 4, cur_features=node_features, pad=False)\n",
    "\n",
    "\n",
    "cc_dataloaders = []\n",
    "\n",
    "# NOTE: only index features cannot learn S=[2,3,4], K=2.\n",
    "# composed_fea_train, composed_fea_test =  get_features_by_ids(4, cur_features=node_features)\n",
    "\n",
    "\n",
    "# NOTE: test only k=3 and K=1, works, overfitting. k=1? works?\n",
    "# composed_fea_train, composed_fea_test =  get_features_by_ids(6, cur_features=node_features)\n",
    "# composed_fea_train, composed_fea_test =  get_features_by_ids(5, cur_features=node_features)\n",
    "\n",
    "# NOTE: try dim=41 gaussian\n",
    "composed_fea_train, composed_fea_test =  get_features_by_ids(2, 4, cur_features=node_features)\n",
    "\n",
    "\n",
    "regression = False\n",
    "\n",
    "if regression:\n",
    "    y_torch_type = torch.FloatTensor\n",
    "else:\n",
    "    cc_dataset_classification = (train_adjs, train_y_class, test_adjs, test_y_class)\n",
    "    cc_dataset = cc_dataset_classification\n",
    "    y_torch_type = torch.LongTensor\n",
    "\n",
    "train_dataset, test_dataset, scalers, scalers_y = construct_dataset(cc_dataset, (composed_fea_train, composed_fea_test), \n",
    "                    y_torch_type=y_torch_type, sparse=True, K=1, regression=regression)\n",
    "\n",
    "\n",
    "cc_dataloaders.append(assemble_dataloader(train_dataset, test_dataset , cuda=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check the normalization of labels and features:\n",
    "\n",
    "x_check=[]\n",
    "y_check=[]\n",
    "\n",
    "for y in cc_dataloaders[0][0].dataset.y:\n",
    "    y_check.append(y.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: set model hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gin_args = utils.DaoArgs()\n",
    "gin_args.gnn_hid_dim=64\n",
    "gin_args.gnn_layer_num=3\n",
    "gin_args.set_attr('bi', False)\n",
    "gin_args.rep_fea_dim=32 # gin output dimension for linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Circulant graphs, 09.06, only index features.\n",
    "# NOTE: mean pool\n",
    "train_y_norm = scalers_y[0].transform(train_y)\n",
    "test_y_norm = scalers_y[1].transform(test_y)\n",
    "\n",
    "for cc_dataloader in cc_dataloaders:\n",
    "    fea_len = cc_dataloader[0].dataset.x[0].get_node_features().shape[-1]\n",
    "    print('xxfeature len:', fea_len)\n",
    "    models_trained = train_cc_degree(gin_args, cc_dataloader, feature_dim=fea_len, lr=0.002,\n",
    "                                     gnn_name='sparse_gin',epochs=200, opt='adam')\n",
    "    plot_cc_degree_prediction(models_trained, cc_dataloader[0], train_adjs, train_y_norm, 'training dataset')\n",
    "    plot_cc_degree_prediction(models_trained, cc_dataloader[1], test_adjs, test_y_norm, 'testing dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Circulant graphs, 09.06, only index features.\n",
    "# NOTE: mean pool\n",
    "\n",
    "# NOTE: try classificaiton !!!!!!!!!!!!!!\n",
    "\n",
    "for cc_dataloader in cc_dataloaders:\n",
    "    fea_len = cc_dataloader[0].dataset.x[0].get_node_features().shape[-1]\n",
    "    print('xxfeature len:', fea_len)\n",
    "    models_trained = train_cc_degree(gin_args, cc_dataloader, feature_dim=fea_len, lr=0.002,\n",
    "                                     gnn_name='sparse_gin',epochs=200, opt='adam', regression=False, class_num=4)\n",
    "    plot_cc_degree_prediction(models_trained, cc_dataloader[0], train_adjs, train_y_class, 'training dataset',regression=False)\n",
    "    plot_cc_degree_prediction(models_trained, cc_dataloader[1], test_adjs, test_y_class, 'testing dataset',regression=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_norm = scalers_y[0].transform(train_y)\n",
    "test_y_norm = scalers_y[1].transform(test_y)\n",
    "\n",
    "plot_cc_degree_prediction(models_trained, cc_dataloader[0], train_adjs, train_y_norm, 'training dataset')\n",
    "plot_cc_degree_prediction(models_trained, cc_dataloader[1], test_adjs, test_y_norm, 'testing dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Circulant graphs, 09.04, try cycles features.\n",
    "# NOTE: mean pool\n",
    "# TODO: try onehot features and labels.\n",
    "\n",
    "# NOTE: only all one and index features\n",
    "tri_fea_train = composite_node_features(node_features[2][0], node_features[3][0], padding=False)\n",
    "tri_fea_test = composite_node_features(node_features[2][1], node_features[3][1], padding=False)\n",
    "cc_dataloaders = []\n",
    "cc_dataloaders.append(assemble_dataloader(\n",
    "    *construct_dataset(cc_dataset, (tri_fea_train, tri_fea_test), \n",
    "                       y_torch_type=torch.FloatTensor, sparse=True), cuda=False))\n",
    "\n",
    "\n",
    "for cc_dataloader in cc_dataloaders:\n",
    "    fea_len = cc_dataloader[0].dataset.x[0].get_node_features().shape[-1]\n",
    "    print('xxfeature len:', fea_len)\n",
    "    models_trained = train_cc_degree(cc_dataloader, feature_dim=fea_len, lr=0.0002, gnn_name='sparse_gin', epochs=500)\n",
    "    plot_cc_degree_prediction(models_trained, cc_dataloader[0], train_adjs, train_y, 'training dataset')\n",
    "    plot_cc_degree_prediction(models_trained, cc_dataloader[1], test_adjs, test_y, 'testing dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE 1:  try $A^2 \\odot {mask_i, i = 1..M}$ as input features, where $mask_i \\in R^{1 \\times N}$ is a learnable vector.\n",
    "\n",
    "* mask the padded features, so N=128, set M=3.\n",
    "* align mask features in the GNNAdapter module.\n",
    "* Done, useless using mask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try $A^2$ as the edge_index, 2022.09.02\n",
    "* good performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: only allone_train\n",
    "cc_dataloaders = []\n",
    "cc_dataloaders.append(assemble_dataloader(\n",
    "    *construct_dataset(cc_dataset,\n",
    "                        (allone_train_norm, allone_test_norm), y_torch_type=torch.FloatTensor, sparse=True, K=2), \n",
    "    cuda=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_trained = train_cc_degree(cc_dataloaders[0], feature_dim=1, lr=0.001, gnn_name='sparse_gin', epochs=1000)\n",
    "\n",
    "plot_cc_degree_prediction(models_trained, cc_dataloaders[0])\n",
    "\n",
    "# convergence speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try A^3, K=3\n",
    "* bad performance\n",
    "* so only K=2 can count the cycles so that it further predict CC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: only allone_train\n",
    "\n",
    "cc_dataloaders = []\n",
    "cc_dataloaders.append(assemble_dataloader(\n",
    "    *construct_dataset(cc_dataset,\n",
    "                        (allone_train_norm, allone_test_norm), y_torch_type=torch.FloatTensor, sparse=True, K=3), \n",
    "    cuda=False))\n",
    "\n",
    "models_trained = train_cc_degree(cc_dataloaders[0], feature_dim=1, lr=0.0012, gnn_name='sparse_gin', epochs=1000)\n",
    "\n",
    "plot_cc_degree_prediction(models_trained, cc_dataloaders[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NOTE: Kadj as features:\n",
    "\n",
    "pad_len = 128\n",
    "\n",
    "com_node_features_train = composite_node_features(allone_train, kadj_train, padding=True, padding_len=pad_len)\n",
    "com_node_features_test = composite_node_features(allone_test, kadj_test, padding=True, padding_len=pad_len)\n",
    "\n",
    "cc_dataloaders = []\n",
    "cc_dataloaders.append(assemble_dataloader(\n",
    "    *construct_dataset(cc_dataset,\n",
    "                        (com_node_features_train, com_node_features_test), y_torch_type=torch.FloatTensor, sparse=True), cuda=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train corr(degree, CC) = 0:\n",
    "# NOTE: try mask features with k_adj. 2022.08.28, Sun.\n",
    "# NOTE: try gcn/gin with k_adj_features. \n",
    "# NOTE: mean pool\n",
    "\n",
    "models_trained = train_cc_degree(cc_dataloaders, feature_dim=128, lr=0.001, gnn_name='sparse_gin_mask',epochs=300)\n",
    "\n",
    "plot_cc_degree_prediction(models_trained, cc_dataloaders)\n",
    "\n",
    "\n",
    "# NOTE: small lr converges to local optimal, try only kadj later with a larger lr.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# com_node_features_train = composite_node_features(allone_train, kadj_train[0], padding=False, padding_len=128)\n",
    "# com_node_features_test = composite_node_features(allone_test, kadj_test[0], padding=False, padding_len=128)\n",
    "\n",
    "com_node_features_train = composite_node_features(allone_train, tri_cycles_train[0], padding=False, padding_len=128)\n",
    "com_node_features_test = composite_node_features(allone_test, tri_cycles_test[0], padding=False, padding_len=128)\n",
    "\n",
    "cc_dataloaders = []\n",
    "cc_dataloaders.append(assemble_dataloader(\n",
    "    *construct_dataset(cc_dataset,\n",
    "                        (com_node_features_train, com_node_features_test), y_torch_type=torch.FloatTensor, sparse=True), cuda=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train corr(degree, CC) = 0:\n",
    "# NOTE: node_fea_dim = 128, add k_adj_features. 2022.08.23, failed\n",
    "# NOTE: node_fea_dim = 2, add tri_cycles. 2022.08.26, running\n",
    "# NOTE: try gcn/gin with k_adj_features.\n",
    "# NOTE: mean pool\n",
    "\n",
    "models_trained = train_cc_degree(cc_dataloaders, feature_dim=2, lr=0.00001, gnn_name='sparse_gin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result of GCN:\n",
    "\n",
    "# NOTE: node_fea_dim = 128, add k_adj_features. 2022.08.23\n",
    "# NOTE: mean pool\n",
    "# NOTE: node feature mean: 0.2\n",
    "    \n",
    "plot_cc_degree_prediction(models_trained, cc_dataloaders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: kadj features without mask\n",
    "com_node_features_train = composite_node_features(allone_train, kadj_train[0], padding=True, padding_len=128)\n",
    "com_node_features_test = composite_node_features(allone_test, kadj_test[0], padding=True, padding_len=128)\n",
    "\n",
    "cc_dataloaders = []\n",
    "cc_dataloaders.append(assemble_dataloader(\n",
    "    *construct_dataset(cc_dataset,\n",
    "                        (com_node_features_train, com_node_features_test), y_torch_type=torch.FloatTensor, sparse=True), cuda=False))\n",
    "\n",
    "\n",
    "\n",
    "# allone_train[0]\n",
    "print(cc_dataloaders[0][0].dataset.x[0].get_node_features().shape)\n",
    "\n",
    "print(cc_dataloaders[0][0].dataset.x[0].get_node_features()[0, 75:88])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train corr(degree, CC) = 0:\n",
    "# NOTE: node_fea_dim = 128, add k_adj_features. 2022.08.27, run again\n",
    "# NOTE: try gcn/gin with k_adj_features.\n",
    "# NOTE: mean pool\n",
    "\n",
    "\n",
    "models_trained = train_cc_degree(cc_dataloaders, feature_dim=128, lr=0.001, gnn_name='sparse_gin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: node_fea_dim = 128, add k_adj_features. 2022.08.23\n",
    "# NOTE: mean pool\n",
    "\n",
    "# NOTE: node feature mean: 0.2\n",
    "plot_cc_degree_prediction(models_trained, cc_dataloaders)\n",
    "\n",
    "# NOTE: !!!!!!!! mask is useless !!!!!!!!!!!!!\n",
    "# NOTE: !!!!!!!! cycles are useless too !!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: only all one features\n",
    "\n",
    "cc_dataloaders = []\n",
    "cc_dataloaders.append(assemble_dataloader(\n",
    "    *construct_dataset(cc_dataset,\n",
    "                        (allone_train, allone_test), y_torch_type=torch.FloatTensor, sparse=True), cuda=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: only all one features, try larger lr, 2022.08.28\n",
    "models_trained = train_cc_degree(cc_dataloaders, feature_dim=1, lr=0.0005, gnn_name='sparse_gin', epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: only all one features\n",
    "plot_cc_degree_prediction(models_trained, cc_dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: only all one and index features\n",
    "index_fea_train,  index_fea_test =  generate_node_feature([(cc_train_adjs, None, cc_test_adjs, None)] ,node_index_feature,\n",
    "                                                   sparse=True)\n",
    "\n",
    "com_fea_train = composite_node_features(allone_train, index_fea_train[0], padding=False)\n",
    "com_fea_test = composite_node_features(allone_test, index_fea_test[0], padding=False)\n",
    "\n",
    "cc_dataloaders = []\n",
    "cc_dataloaders.append(assemble_dataloader(\n",
    "    *construct_dataset(cc_dataset,\n",
    "                        (com_fea_train, com_fea_test), y_torch_type=torch.FloatTensor, sparse=True), cuda=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models_trained = train_cc_degree(cc_dataloaders, feature_dim=2, lr=0.001, gnn_name='sparse_gin', epochs=300)\n",
    "\n",
    "# with \n",
    "\n",
    "# NOTE: only all one features and index features.\n",
    "plot_cc_degree_prediction(models_trained, cc_dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: only all one and degree features\n",
    "degree_fea_train,  degree_fea_test =  generate_node_feature([(cc_train_adjs, None, cc_test_adjs, None)] ,node_degree_feature,\n",
    "                                                   sparse=True)\n",
    "\n",
    "com_fea_train = composite_node_features(allone_train, degree_fea_train[0], padding=False)\n",
    "com_fea_test = composite_node_features(allone_test, degree_fea_test[0], padding=False)\n",
    "\n",
    "cc_dataloaders = []\n",
    "cc_dataloaders.append(assemble_dataloader(\n",
    "    *construct_dataset(cc_dataset,\n",
    "                        (com_fea_train, com_fea_test), y_torch_type=torch.FloatTensor, sparse=True), cuda=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models_trained = train_cc_degree(cc_dataloaders, feature_dim=2, lr=0.001, gnn_name='sparse_gin', epochs=300)\n",
    "\n",
    "# with \n",
    "\n",
    "# NOTE: only all one features and degree features\n",
    "plot_cc_degree_prediction(models_trained, cc_dataloaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Compare the CCA of the features of these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check the correctness of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check the density?\n",
    "\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(3, 6, figsize=(8, 4), dpi=300)\n",
    "\n",
    "train_dl, _ = dataloaders[5]\n",
    "x_samples = train_dl.dataset.x\n",
    "print(type(x_samples[0]))\n",
    "\n",
    "for i in range(axes.shape[0]):\n",
    "    for j in range(axes.shape[1]):\n",
    "        index =  int(each_class_num) * j + i\n",
    "        print('index of sample:', index)\n",
    "        sns.heatmap(x_samples[index].A.cpu().numpy(), cbar=False, linecolor='indigo', square=True, linewidths=0.3,\n",
    "                         cmap=ListedColormap(['white', 'purple']), ax=axes[i, j])\n",
    "        axes[i, j].set_xticks([])\n",
    "        axes[i, j].set_yticks([])\n",
    "\n",
    "plt.axis('off')\n",
    "# ax.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot f1 curves:\n",
    "\n",
    "def get_f1s(evls):\n",
    "    mi_f1 = []\n",
    "    ma_f1 = []\n",
    "    w_f1 = []\n",
    "\n",
    "    for evl in evls:\n",
    "        mi_f1.append(evl.total_metrics['micro_f1'])\n",
    "        ma_f1.append(evl.total_metrics['macro_f1'])\n",
    "        w_f1.append(evl.total_metrics['weighted_f1'])\n",
    "    return mi_f1, ma_f1, w_f1\n",
    "\n",
    "    \n",
    "    \n",
    "def plot_f1_curves(mi_f1, ma_f1, w_f1):\n",
    "    plt.figure(figsize=(4, 3), dpi=150)\n",
    "\n",
    "\n",
    "    x = np.linspace(0, 1, 24)\n",
    "    plt.plot(x, mi_f1,  marker=\"8\")\n",
    "    plt.plot(x, ma_f1,  marker=11)\n",
    "    ax = plt.axes()\n",
    "  \n",
    "# Setting the background color of the plot \n",
    "# using set_facecolor() method\n",
    "    ax.set_facecolor(\"snow\")\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot performance of WS classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_f1, ma_f1, w_f1 = get_f1s(ws_gnn_evls)\n",
    "\n",
    "print(mi_f1[0])\n",
    "plot_f1_curves(mi_f1, ma_f1, w_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine wanted node features\n",
    "\n",
    "# case: 1 only use index feas.\n",
    "\n",
    "# case: 2, add allone vector can detect structural information:\n",
    "\n",
    "train_combined_feature = composite_node_features(train_node_index_feas, train_node_allone_feas, train_node_std_feas)\n",
    "test_combined_feature = composite_node_features(test_node_index_feas, test_node_allone_feas, test_node_std_feas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pos_en_dim = 10\n",
    "\n",
    "dataloaders = []\n",
    "for i, d in enumerate(data_sim):\n",
    "    dataloaders.append(assemble_dataloader(\n",
    "        *construct_dataset(d,\n",
    "                           (train_combined_feature[i], test_combined_feature[i]), \n",
    "                                                              lap_encode=True,\n",
    "                                                              lap_en_dim=pos_en_dim)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: set the feature dimensions:\n",
    "\n",
    "node_feature_dim = train_combined_feature.shape[-1]\n",
    "graph_feature_dim = 1\n",
    "\n",
    "print('node fea dim:', node_feature_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try again TODO: plot the performance curve.\n",
    "\n",
    "# 2. train by each method:\n",
    "\n",
    "# MLP:\n",
    "\n",
    "mlp_evls = []\n",
    "\n",
    "for train_dataloader, test_dataloader in dataloaders:\n",
    "    _, test_evl = train_mlp(train_dataloader, test_dataloader, epoch=100)\n",
    "    mlp_evls.append(test_evl)\n",
    "\n",
    "\n",
    "mi_f1, ma_f1, w_f1 = get_f1s(mlp_evls)\n",
    "\n",
    "plot_f1_curves(mi_f1, ma_f1, w_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train LSPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "\n",
    "\n",
    "def train_lspe(train_loader, test_loader, node_fea_dim, epoch=1, plot=False, model_name='lspe', graph_fea_dim=1):\n",
    "# use CNN to trian:\n",
    "    args = base_args()\n",
    "    lspe_model = choose_model(model_name, node_fea_dim=node_fea_dim, graph_fea_dim=graph_fea_dim)\n",
    "    # opt = optim.Adam(mlp_model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    opt = optim.SGD(lspe_model.parameters(), lr=0.0001)\n",
    "    ce_loss_cal = CELossCal()\n",
    "\n",
    "    trainer = utils.Trainer(lspe_model, optimizer=opt, loss_cal=ce_loss_cal)\n",
    "    train_sim_evl= SimpleEvaluator(args)\n",
    "    test_sim_evl= SimpleEvaluator(args)\n",
    "\n",
    "    training(epoch, trainer, train_sim_evl, test_sim_evl,train_loader,test_loader)\n",
    "    if plot:\n",
    "        train_sim_evl.plot_metrics()\n",
    "        test_sim_evl.plot_metrics()\n",
    "        \n",
    "    return train_sim_evl, test_sim_evl\n",
    "    \n",
    "# t_dl, v_dl = dataloaders[0]\n",
    "# _, _ = train_lspe(t_dl, v_dl, epoch=100, plot=True)\n",
    "# train_cnn(inva_train_dataloader, inva_test_dataloader, True)\n",
    "\n",
    "# node feature dim=3, added std feature\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train LSPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: LSPE:\n",
    "# NOTE: 6 classification\n",
    "\n",
    "val_lspe_evls = []\n",
    "train_lspe_evls = []\n",
    "for train_dataloader, test_dataloader in dataloaders:\n",
    "    train_evl, test_evl = train_lspe(train_dataloader, test_dataloader, node_fea_dim=node_feature_dim, model_name='lspe', epoch=100)\n",
    "    val_lspe_evls.append(test_evl)\n",
    "    train_lspe_evls.append(train_evl)\n",
    "    \n",
    "mi_f1, ma_f1, w_f1 = get_f1s(val_lspe_evls)\n",
    "\n",
    "plot_f1_curves(mi_f1, ma_f1, w_f1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(12).reshape(3, 4)\n",
    "a.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_evls = []\n",
    "for train_dataloader, test_dataloader in dataloaders:\n",
    "    train_evl, test_evl = train_cnn(train_dataloader, test_dataloader, epoch=100, graph_fea_dim=graph_feature_dim)\n",
    "    cnn_evls.append((train_evl,test_evl))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node feature dim=3, added std feature\n",
    "\n",
    "train_cnn_evls = [e[0] for e in cnn_evls]\n",
    "test_cnn_evls = [e[1] for e in cnn_evls]\n",
    "\n",
    "mi_f1, ma_f1, w_f1 = get_f1s(train_cnn_evls)\n",
    "\n",
    "plot_f1_curves(mi_f1, ma_f1, w_f1)\n",
    "\n",
    "\n",
    "mi_f1, ma_f1, w_f1 = get_f1s(test_cnn_evls)\n",
    "\n",
    "plot_f1_curves(mi_f1, ma_f1, w_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node feature dim=2\n",
    "\n",
    "train_cnn_evls = [e[0] for e in cnn_evls]\n",
    "test_cnn_evls = [e[1] for e in cnn_evls]\n",
    "\n",
    "mi_f1, ma_f1, w_f1 = get_f1s(train_cnn_evls)\n",
    "\n",
    "plot_f1_curves(mi_f1, ma_f1, w_f1)\n",
    "\n",
    "\n",
    "mi_f1, ma_f1, w_f1 = get_f1s(test_cnn_evls)\n",
    "\n",
    "plot_f1_curves(mi_f1, ma_f1, w_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion:\n",
    "# visualize the train loader:\n",
    "\n",
    "# node feature dim=2\n",
    "\n",
    "\n",
    "train_cnn_evls[4].plot_metrics()\n",
    "\n",
    "\n",
    "train_dl, _ = dataloaders[4]\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(8, 6), dpi=150)\n",
    "\n",
    "x_samples = train_dl.dataset.x\n",
    "print(x_samples[0][1].shape)\n",
    "\n",
    "for i in range(axes.shape[0]):\n",
    "    for j in range(axes.shape[1]):\n",
    "        index =  4*i+j\n",
    "        print('index of sample:', index)\n",
    "        sns.heatmap(x_samples[index][1].cpu().numpy(), cbar=False, linecolor='indigo', square=True, linewidths=0.3,\n",
    "                         cmap=ListedColormap(['white', 'purple']), ax=axes[i, j])\n",
    "        axes[i, j].set_xticks([])\n",
    "        axes[i, j].set_yticks([])\n",
    "\n",
    "plt.axis('off')\n",
    "# ax.grid()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train GIN with lspe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use degree node features:\n",
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "args = utils.get_common_args()\n",
    "\n",
    "args = args.parse_args({})\n",
    "args.pos_en = 'lap_pe'\n",
    "args.pos_en_dim = pos_en_dim\n",
    "\n",
    "gnn_evls = []\n",
    "for train_dataloader, test_dataloader in dataloaders:\n",
    "    train_evl, test_evl = train_gnn(args, train_dataloader, test_dataloader, gnn_name='gin', epoch=200, node_fea_dim=node_feature_dim)\n",
    "    gnn_evls.append(test_evl)\n",
    "    \n",
    "# node feature dim=3, added std feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mi_f1, ma_f1, w_f1 = get_f1s(gnn_evls)\n",
    "\n",
    "plot_f1_curves(mi_f1, ma_f1, w_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train GNN without laspe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use degree node features:\n",
    "\n",
    "gnn_evls = []\n",
    "for train_dataloader, test_dataloader in dataloaders:\n",
    "    train_evl, test_evl = train_gnn(train_dataloader, test_dataloader, gnn_name='gnn', epoch=200, node_fea_dim=node_feature_dim)\n",
    "    gnn_evls.append(test_evl)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node feature dim=3, added std feature\n",
    "\n",
    "mi_f1, ma_f1, w_f1 = get_f1s(gnn_evls)\n",
    "\n",
    "plot_f1_curves(mi_f1, ma_f1, w_f1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node feature dim=2\n",
    "\n",
    "mi_f1, ma_f1, w_f1 = get_f1s(gnn_evls)\n",
    "\n",
    "plot_f1_curves(mi_f1, ma_f1, w_f1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion:\n",
    "# plot loss function:\n",
    "\n",
    "evl = gnn_evls[10]\n",
    "evl.plot_metrics()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP gin:\n",
    "# NOTE: 6 classification\n",
    "\n",
    "gin_evls = []\n",
    "train_gin_evls = []\n",
    "for train_dataloader, test_dataloader in dataloaders:\n",
    "    train_evl, test_evl = train_gnn(train_dataloader, test_dataloader,\n",
    "                                    node_fea_dim=node_feature_dim, gnn_name='gin', epoch=100)\n",
    "    gin_evls.append(test_evl)\n",
    "    train_gin_evls.append(train_evl)\n",
    "    \n",
    "mi_f1, ma_f1, w_f1 = get_f1s(gin_evls)\n",
    "\n",
    "plot_f1_curves(mi_f1, ma_f1, w_f1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion:\n",
    "# plot loss function:\n",
    "\n",
    "evl = gin_evls[15]\n",
    "evl.plot_metrics()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP gin:\n",
    "# NOTE: 6 classification, \n",
    "\n",
    "# NOTE node feature dim=3, added std feature\n",
    "\n",
    "gin_evls = []\n",
    "train_gin_evls = []\n",
    "for train_dataloader, test_dataloader in dataloaders:\n",
    "    train_evl, test_evl = train_gnn(train_dataloader, test_dataloader, \n",
    "                                    node_fea_dim=node_feature_dim, gnn_name='gin', epoch=100)\n",
    "    gin_evls.append(test_evl)\n",
    "    train_gin_evls.append(train_evl)\n",
    "    \n",
    "mi_f1, ma_f1, w_f1 = get_f1s(gin_evls)\n",
    "\n",
    "plot_f1_curves(mi_f1, ma_f1, w_f1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Di GIN\n",
    "# NOTE:use both direction:\n",
    "# NOTE: 6 classification, 3 features, direct\n",
    "# NOTE: added lap_en:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use both direction:\n",
    "# NOTE: 6 classification, 3 features, direct\n",
    "# NOTE: added lap_en:\n",
    " \n",
    "\n",
    "args = utils.get_common_args()\n",
    "\n",
    "args = args.parse_args({})\n",
    "args.pos_en = 'lap_pe'\n",
    "args.pos_en_dim = pos_en_dim\n",
    "\n",
    "gin_evls = []\n",
    "train_evls = []\n",
    "for train_dataloader, test_dataloader in dataloaders:\n",
    "    train_evl, test_evl = train_gnn(args, train_dataloader, test_dataloader,\n",
    "                                    gnn_name='gin_direc', epoch=100, node_fea_dim=node_feature_dim)\n",
    "    gin_evls.append(test_evl)\n",
    "    train_evls.append(train_evl)\n",
    "    \n",
    "mi_f1, ma_f1, w_f1 = get_f1s(gin_evls)\n",
    "\n",
    "plot_f1_curves(mi_f1, ma_f1, w_f1)\n",
    "\n",
    "\n",
    "evl = gin_evls[1]\n",
    "evl.plot_metrics()\n",
    "\n",
    "evl = gin_evls[10]\n",
    "evl.plot_metrics()\n",
    "\n",
    "evl = gin_evls[15]\n",
    "evl.plot_metrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use both direction:\n",
    "# NOTE: 6 classification\n",
    "gin_evls = []\n",
    "train_evls = []\n",
    "for train_dataloader, test_dataloader in dataloaders:\n",
    "    train_evl, test_evl = train_gnn(train_dataloader, test_dataloader,gnn_name='gin_direc', epoch=100, node_fea_dim=node_feature_dim)\n",
    "    gin_evls.append(test_evl)\n",
    "    train_evls.append(train_evl)\n",
    "    \n",
    "mi_f1, ma_f1, w_f1 = get_f1s(gin_evls)\n",
    "\n",
    "plot_f1_curves(mi_f1, ma_f1, w_f1)\n",
    "\n",
    "\n",
    "evl = gin_evls[1]\n",
    "evl.plot_metrics()\n",
    "\n",
    "evl = gin_evls[10]\n",
    "evl.plot_metrics()\n",
    "\n",
    "evl = gin_evls[15]\n",
    "evl.plot_metrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use both direction:\n",
    "# NOTE: 6 classification\n",
    "# NOTE: 3 node features !!\n",
    "\n",
    "gin_evls = []\n",
    "train_evls = []\n",
    "for train_dataloader, test_dataloader in dataloaders:\n",
    "    train_evl, test_evl = train_gnn(train_dataloader, test_dataloader,gnn_name='gin_direc', epoch=100, node_fea_dim=node_feature_dim)\n",
    "    gin_evls.append(test_evl)\n",
    "    train_evls.append(train_evl)\n",
    "    \n",
    "mi_f1, ma_f1, w_f1 = get_f1s(gin_evls)\n",
    "\n",
    "plot_f1_curves(mi_f1, ma_f1, w_f1)\n",
    "\n",
    "\n",
    "evl = gin_evls[1]\n",
    "evl.plot_metrics()\n",
    "\n",
    "evl = gin_evls[10]\n",
    "evl.plot_metrics()\n",
    "\n",
    "evl = gin_evls[15]\n",
    "evl.plot_metrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: concate the allone features and index features, not together training.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try laplacian\n",
    "# construct new features:\n",
    "\n",
    "\n",
    "import utils\n",
    "importlib.reload(utils)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_kernel_cnn_evls = []\n",
    "for train_dataloader, test_dataloader in dataloaders:\n",
    "    _, test_evl = train_cnn(train_dataloader, test_dataloader, epoch=200, cnn_name='cnn_big')\n",
    "    big_kernel_cnn_evls.append(test_evl)\n",
    "    \n",
    "mi_f1, ma_f1, w_f1 = get_f1s(big_kernel_cnn_evls)\n",
    "\n",
    "plot_f1_curves(mi_f1, ma_f1, w_f1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: 6 class:\n",
    "\n",
    "big_kernel_cnn_evls = []\n",
    "cnn_train_evls = []\n",
    "for train_dataloader, test_dataloader in dataloaders:\n",
    "    train_evl, test_evl = train_cnn(train_dataloader, test_dataloader, epoch=200, cnn_name='cnn_big')\n",
    "    big_kernel_cnn_evls.append(test_evl)\n",
    "    cnn_train_evls.append(train_evl)\n",
    "    \n",
    "mi_f1, ma_f1, w_f1 = get_f1s(big_kernel_cnn_evls)\n",
    "\n",
    "plot_f1_curves(mi_f1, ma_f1, w_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Real-world dataset from PyG generic datasets\n",
    "website: `https://pytorch-geometric.readthedocs.io/en/latest/notes/data_cheatsheet.html`\n",
    "\n",
    "* graph classification:\n",
    "    * TUDataset\n",
    "    * ZINC\n",
    "\n",
    "## TODO: \n",
    "1. use. # due: 6.30.\n",
    "2. profile. # due: 7.1.\n",
    "\n",
    "## TODO (2022.09.24):\n",
    "* 1. load from gnn-comparison module.\n",
    "* 2. check the avg.CC avg.Degree and performance associations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gnn_comparison.PrepareDatasets import DATASETS\n",
    "\n",
    "print(DATASETS.keys())\n",
    "\n",
    "datasets_obj = {}\n",
    "for k, v in DATASETS.items():\n",
    "    print('dataset name:', k)\n",
    "    dat = v()\n",
    "    datasets_obj[k] = dat\n",
    "    print(type(dat.dataset.get_data()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gnn_comparison\n",
    "importlib.reload(gnn_comparison)\n",
    "import gnn_comparison.PrepareDatasets as predata\n",
    "importlib.reload(predata)\n",
    "\n",
    "def add_obj(name):\n",
    "    cls = predata.DATASETS[name]\n",
    "    dat = cls()\n",
    "    datasets_obj[name] = dat\n",
    "    \n",
    "# add_obj('COLLAB')\n",
    "# add_obj('NCI1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_pyg_dataset_stats(pyg_data):\n",
    "    adjs = []\n",
    "    # TODO: transform into networkx.\n",
    "    labels = []\n",
    "    for graph in pyg_data.dataset.get_data():\n",
    "        row = graph.edge_index[0]\n",
    "        col = graph.edge_index[1]\n",
    "        graph.y\n",
    "        N = graph.x.shape[0]\n",
    "        dense_A = torch.zeros((N, N))\n",
    "        dense_A[row, col] = 1\n",
    "        A = dense_A.detach().numpy()\n",
    "        adjs.append(A)\n",
    "        labels.append(graph.y.item())\n",
    "    return graphs_statistics(adjs, labels)\n",
    "\n",
    "\n",
    "# print(node_feature_utils.node_cc_avg_feature(adj=dense_A.detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot on one figure:\n",
    "datasets_stats = {}\n",
    "for k, v in datasets_obj.items():\n",
    "    datasets_stats[k] = get_pyg_dataset_stats(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils)\n",
    "import pandas as pd\n",
    "\n",
    "data_stats = {}\n",
    "K = 11\n",
    "tuple_label = ['avg_cc', 'avg_degree', 'tris']\n",
    "[tuple_label.append(f'cyc<={k}') for k in range(4, K)]\n",
    "tuple_label.append('y_label')\n",
    "\n",
    "\n",
    "print('tuple_label:', tuple_label)\n",
    "\n",
    "normed_stats = {}\n",
    "for name, st in datasets_stats.items():\n",
    "    print(f'dataset {name}, len: {len(st)}')    \n",
    "    stats_tuples = []\n",
    "    \n",
    "    for i in range(len(st)):\n",
    "        cc = st[i][0]\n",
    "        degree = st[i][1]\n",
    "        tris= st[i][2]\n",
    "        y = st[i][-1]\n",
    "        cycles = st[i][-2]\n",
    "        \n",
    "        counter_cur = defaultdict(int)\n",
    "        for c in cycles:\n",
    "            if len(c) < K:\n",
    "                counter_cur[len(c)] += 1\n",
    "        cycle_num = [0]\n",
    "        for k in range(3, K):\n",
    "            if k in counter_cur:\n",
    "                cycle_num.append(cycle_num[k-3]+counter_cur[k])\n",
    "            else:\n",
    "                cycle_num.append(cycle_num[k-3])\n",
    "                \n",
    "        cycle_num.pop(0)\n",
    "        cycle_num = tuple(cycle_num)\n",
    "        \n",
    "        stats_tuples.append((cc, degree, tris, *cycle_num, y))\n",
    "    \n",
    "    data_array = pd.DataFrame(stats_tuples).values\n",
    "    # TODO: normalize:\n",
    "    data_array = utils.normalize(data_array, along_axis=-1)\n",
    "    \n",
    "    normed_stats[name] = data_array\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stats(cur_data_stats:dict, show_idx=None):\n",
    "    total = len(cur_data_stats.keys())\n",
    "    nrows = int(total/3) + (0 if total%3 == 0 else 1)\n",
    "    print('nrows:', nrows, 'total', total)\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows, 3, figsize=(10, 8), dpi=300)\n",
    "    for idx, (name, normed_stat) in enumerate(cur_data_stats.items()):\n",
    "        plot_graphs_statistics(name, normed_stat, tuple_label, axe=axes[int(idx/3)][idx%3],\n",
    "                               sort_idx=1, show_idx=show_idx)\n",
    "    \n",
    "    if total%3 > 0:\n",
    "        for d in range(3-total%3):\n",
    "            fig.delaxes(axes[-1][-d-1])\n",
    "            \n",
    "    \n",
    "    handlers, labels = axes[0][0].get_legend_handles_labels()\n",
    "    \n",
    "    fig.legend(handlers, labels, loc='lower right', prop={'size':16})\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils)\n",
    "\n",
    "\n",
    "plot_stats(normed_stats, show_idx=[1, 3, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stats(datasets_stats) # K=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs_statistics(stats_obj) # K=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "N = 4\n",
    "dense_A = torch.zeros((N, N))\n",
    "row = torch.tensor(data=[0, 1])\n",
    "col = torch.tensor(data=[1, 2])\n",
    "row.to\n",
    "dense_A[row, col] = 1\n",
    "print(dense_A.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 1. how to use them.\n",
    "# classTUDataset\n",
    "\n",
    "\n",
    "import torch_geometric.datasets as pygdataset\n",
    "\n",
    "\n",
    "tudataset = pygdataset.tu_dataset.TUDataset(root='/li_zhengdao/github/GenerativeGNN/dataset/', name='AIDS')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "import importlib\n",
    "from models import BaseGraphUtils\n",
    "importlib.reload(models)\n",
    "\n",
    "\n",
    "refresh_import()\n",
    "\n",
    "tu_base_graphs = []\n",
    "for a in tudataset:\n",
    "    tu_base_graphs.append(BaseGraphUtils.from_pyg_graph(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tu_y = []\n",
    "for g in tu_base_graphs:\n",
    "    tu_y.append(g.label)\n",
    "tu_y = torch.stack(tu_y, dim=0).squeeze()\n",
    "\n",
    "train_tu_dataset = GraphDataset(x=tu_base_graphs, y=torch.LongTensor(tu_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import importlib\n",
    "import models\n",
    "from models import GraphDataset\n",
    "\n",
    "importlib.reload(utils)\n",
    "importlib.reload(models)\n",
    "\n",
    "\n",
    "train_x, train_y, test_x, test_y = utils.random_split_dataset(train_tu_dataset, [0.8, 0.2])\n",
    "\n",
    "print(len(train_x))\n",
    "from collections import Counter\n",
    "tr_y = Counter(train_y.numpy())\n",
    "print(tr_y)\n",
    "\n",
    "tu_train_dataset = GraphDataset(x=train_x, y=train_y)\n",
    "tu_test_dataset = GraphDataset(x=test_x, y=test_y)\n",
    "\n",
    "tu_train_dataloader, tu_test_dataloader = assemble_dataloader(tu_train_dataset, tu_test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg =train_tu_dataset.x[0]\n",
    "print(gg.graph_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: torch.LongTensor(0) will always fail !!!!!!!!!!!!!!!!!\n",
    "a = torch.from_numpy(np.array([0.0])).long()\n",
    "d = torch.LongTensor(1).repeat(10).reshape(10, 1)\n",
    "b = np.repeat(np.array([0]), 11)\n",
    "c =torch.from_numpy(b).long().reshape(11, 1)\n",
    "\n",
    "print(d.squeeze().shape)\n",
    "print(c.squeeze().shape)\n",
    "print(torch.cat([d, c], dim=0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in tu_train_dataloader:\n",
    "    print(x.batch_num)\n",
    "    print(y.shape)\n",
    "    print(x.adj_type)\n",
    "    print(x.pyg_graph.num_nodes)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profiling\n",
    "\n",
    "# TUDataset.\n",
    "# NOTE node feature dim=3, added std feature\n",
    "\n",
    "\n",
    "args = utils.get_common_args()\n",
    "\n",
    "args = args.parse_args({})\n",
    "args.pos_en = 'lap_pe'\n",
    "args.pos_en_dim = pos_en_dim\n",
    "\n",
    "\n",
    "gin_evls = []\n",
    "train_gin_evls = []\n",
    "train_evl, test_evl = train_gnn(args, tu_train_dataloader, tu_test_dataloader, \n",
    "                                    node_fea_dim=node_feature_dim, gnn_name='lsd_gin', epoch=100)\n",
    "gin_evls.append(test_evl)\n",
    "train_gin_evls.append(train_evl)\n",
    "    \n",
    "mi_f1, ma_f1, w_f1 = get_f1s(gin_evls)\n",
    "\n",
    "print(mi_f1[0])\n",
    "print(ma_f1[0])\n",
    "print(w_f1[0])\n",
    "plot_f1_curves(mi_f1, ma_f1, w_f1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try GCN + GNN. involve more spectral information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# incorporate the spectral methods:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "# in future, add node feature, distinguish topological feature\n",
    "* add temporal information to node features.\n",
    "* add non-tological related node features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to train a probabilistic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare graph similarity in spectra domain"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
