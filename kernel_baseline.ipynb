{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load local dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The OGB package is out of date. Your version is 1.3.2, while the latest version is 1.3.5.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'my_utils' from '/li_zhengdao/github/GenerativeGNN/my_utils.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import random\n",
    "import argparse\n",
    "import configparser\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_sparse\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch_geometric.utils import negative_sampling, to_networkx\n",
    "from typing import Union, Tuple\n",
    "from torch_geometric.typing import OptPairTensor, Adj, OptTensor, Size\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "\n",
    "from ogb.linkproppred import PygLinkPropPredDataset, Evaluator\n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy\n",
    "import math\n",
    "\n",
    "\n",
    "from dataset_utils import node_feature_utils\n",
    "from dataset_utils.node_feature_utils import *\n",
    "import my_utils as utils\n",
    "\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['REDDIT-BINARY', 'REDDIT-MULTI-5K', 'COLLAB', 'IMDB-BINARY', 'IMDB-MULTI', 'NCI1', 'ENZYMES', 'PROTEINS', 'DD', 'MUTAG', 'CSL'])\n",
      "loaded dataset, name: MUTAG\n",
      "processed_dir:  gnn_comparison/DATA/MUTAG/processed\n",
      "load dataset !\n",
      "dataset len:  188\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Load specific dataset:\n",
    "\n",
    "import sys,os\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "\n",
    "from PrepareDatasets import DATASETS\n",
    "import my_utils\n",
    "import dataset_utils\n",
    "\n",
    "\n",
    "print(DATASETS.keys())\n",
    "\"\"\"\n",
    "    'REDDIT-BINARY': RedditBinary,\n",
    "    'REDDIT-MULTI-5K': Reddit5K,\n",
    "    'COLLAB': Collab,\n",
    "    'IMDB-BINARY': IMDBBinary,\n",
    "    'IMDB-MULTI': IMDBMulti,\n",
    "    'NCI1': NCI1,\n",
    "    'ENZYMES': Enzymes,\n",
    "    'PROTEINS': Proteins,\n",
    "    'DD': DD,\n",
    "    \"MUTAG\": Mutag,\n",
    "    'CSL': CSL\n",
    "\"\"\"\n",
    "\n",
    "data_names = ['MUTAG']\n",
    "datasets_obj = {}\n",
    "for k, v in DATASETS.items():\n",
    "    if k not in data_names:\n",
    "        continue\n",
    "    \n",
    "    print('loaded dataset, name:', k)\n",
    "    dat = v(use_node_attrs=True)\n",
    "    datasets_obj[k] = dat\n",
    "    print(type(dat.dataset.get_data()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to grakel format or just load shuffle index??\n",
    "\n",
    "\n",
    "mutag_data = datasets_obj['MUTAG'].get_test_fold(1, batch_size=1, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "fold_subset = mutag_data.dataset.get_subset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_each_folder(fold_id, batch_size=1):\n",
    "    \n",
    "    fold_test = datasets_obj['MUTAG'].get_test_fold(fold_id, batch_size=batch_size, shuffle=True)\n",
    "    fold_train, fold_val = datasets_obj['MUTAG'].get_model_selection_fold(fold_id, inner_idx=None, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    train_y = []\n",
    "    \n",
    "    G_train = []\n",
    "    train_y = []\n",
    "    \n",
    "    G_test = []\n",
    "    test_y = []\n",
    "    \n",
    "    for d in fold_train:\n",
    "        d = d.dataset\n",
    "        \n",
    "        G_train.append((d.to_numpy_array(), d.x))\n",
    "        train_y.append(d.y.item())\n",
    "\n",
    "    for d in fold_test:\n",
    "        d = d.dataset\n",
    "        \n",
    "        G_test.append((d.to_numpy_array(), d.x))\n",
    "        test_y.append(d.y.item())\n",
    "        \n",
    "    return G_train, G_test, train_y, test_y\n",
    "    # do not use val for kernel methods.\n",
    "#     for d in fold.dataset.get_subset():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    'REDDIT-BINARY': RedditBinary,\n",
      "    'REDDIT-MULTI-5K': Reddit5K,\n",
      "    'COLLAB': Collab,\n",
      "    'IMDB-BINARY': IMDBBinary,\n",
      "    'IMDB-MULTI': IMDBMulti,\n",
      "    'NCI1': NCI1,\n",
      "    'ENZYMES': Enzymes,\n",
      "    'PROTEINS': Proteins,\n",
      "    'DD': DD,\n",
      "    \"MUTAG\": Mutag,\n",
      "    'CSL': CSL\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Batch' object has no attribute 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-7b6acbdc5c69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mG_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_each_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;31m# G_train, G_test, y_train, y_test = train_test_split(G, y, test_size=0.1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-7056938fefac>\u001b[0m in \u001b[0;36mget_each_folder\u001b[0;34m(fold_id, batch_size)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfold_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mG_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Batch' object has no attribute 'dataset'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from grakel.datasets import fetch_dataset\n",
    "from grakel.kernels import ShortestPath\n",
    "\n",
    "# Loads the MUTAG dataset\n",
    "MUTAG = fetch_dataset(\"MUTAG\", verbose=False)\n",
    "G, y = MUTAG.data, MUTAG.target\n",
    "\n",
    "# Splits the dataset into a training and a test set\n",
    "res=[]\n",
    "for i in range(1):\n",
    "    G_train, G_test, y_train, y_test = get_each_folder(i)\n",
    "    # G_train, G_test, y_train, y_test = train_test_split(G, y, test_size=0.1)\n",
    "    print(len(G_train))\n",
    "    print(G_train[1])\n",
    "    # Uses the shortest path kernel to generate the kernel matrices\n",
    "    gk = ShortestPath(normalize=True)\n",
    "    K_train = gk.fit_transform(G_train)\n",
    "    K_test = gk.transform(G_test)\n",
    "\n",
    "    # Uses the SVM classifier to perform classification\n",
    "    clf = SVC(kernel=\"precomputed\")\n",
    "    clf.fit(K_train, y_train)\n",
    "    y_pred = clf.predict(K_test)\n",
    "\n",
    "    # Computes and prints the classification accuracy\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    res.append(acc)\n",
    "    print(\"Accuracy:\", str(round(acc*100, 2)) + \"%\")\n",
    "    \n",
    "    \n",
    "res = np.array(res)\n",
    "print(f'Acc, mean: {np.mean(res)}, std: {np.std(res)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len: 188\n",
      "[{(15, 13), (10, 11), (5, 6), (9, 8), (2, 1), (14, 13), (8, 9), (15, 16), (1, 6), (14, 9), (17, 15), (1, 2), (11, 10), (13, 12), (12, 13), (16, 15), (3, 4), (10, 9), (4, 10), (3, 2), (5, 4), (9, 14), (10, 4), (4, 5), (13, 15), (9, 10), (7, 5), (2, 3), (8, 7), (12, 11), (11, 12), (6, 5), (15, 17), (13, 14), (6, 1), (5, 7), (4, 3), (7, 8)}, {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 1, 16: 2, 17: 2}, {(2, 1): 0, (1, 2): 0, (3, 2): 0, (2, 3): 0, (4, 3): 0, (3, 4): 0, (5, 4): 0, (4, 5): 0, (6, 5): 0, (5, 6): 0, (6, 1): 0, (1, 6): 0, (7, 5): 0, (5, 7): 0, (8, 7): 0, (7, 8): 0, (9, 8): 0, (8, 9): 0, (10, 9): 0, (9, 10): 0, (10, 4): 0, (4, 10): 0, (11, 10): 0, (10, 11): 0, (12, 11): 0, (11, 12): 0, (13, 12): 0, (12, 13): 0, (14, 13): 0, (13, 14): 0, (14, 9): 0, (9, 14): 0, (15, 13): 1, (13, 15): 1, (16, 15): 2, (15, 16): 2, (17, 15): 1, (15, 17): 1}]\n"
     ]
    }
   ],
   "source": [
    "print('len:', len(G))\n",
    "print(G[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
