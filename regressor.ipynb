{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressor\n",
    "1. load datasets\n",
    "1. sample from datasets\n",
    "1. construct features\n",
    "1. construct labels (load from logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets:\n",
    "\n",
    "import importlib\n",
    "import random\n",
    "import argparse\n",
    "import configparser\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_sparse\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch_geometric.utils import negative_sampling, to_networkx\n",
    "from typing import Union, Tuple\n",
    "from torch_geometric.typing import OptPairTensor, Adj, OptTensor, Size\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "\n",
    "from ogb.linkproppred import PygLinkPropPredDataset, Evaluator\n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy\n",
    "import math\n",
    "\n",
    "\n",
    "from dataset_utils import node_feature_utils\n",
    "from dataset_utils.node_feature_utils import *\n",
    "import my_utils as utils\n",
    "import sys,os\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load specific dataset:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from PrepareDatasets import DATASETS\n",
    "import my_utils\n",
    "import dataset_utils\n",
    "\n",
    "\n",
    "print(DATASETS.keys())\n",
    "\"\"\"\n",
    "    'REDDIT-BINARY': RedditBinary,\n",
    "    'REDDIT-MULTI-5K': Reddit5K,\n",
    "    'COLLAB': Collab,\n",
    "    'IMDB-BINARY': IMDBBinary,\n",
    "    'IMDB-MULTI': IMDBMulti,\n",
    "    'ENZYMES': Enzymes,\n",
    "    'PROTEINS': Proteins,\n",
    "    'NCI1': NCI1,\n",
    "    'DD': DD,\n",
    "    \"MUTAG\": Mutag,\n",
    "    'CSL': CSL\n",
    "\"\"\"\n",
    "\n",
    "data_names = ['PROTEINS']\n",
    "data_names = ['DD']\n",
    "data_names = ['ENZYMES']\n",
    "data_names = ['NCI1']\n",
    "data_names = ['IMDB-MULTI']\n",
    "data_names = ['REDDIT-BINARY']\n",
    "data_names = ['CIFAR10']\n",
    "data_names = ['ogbg_molhiv']\n",
    "\n",
    "\n",
    "# NOTE:new kernel:\n",
    "data_names = ['DD', 'PROTEINS', 'ENZYMES']\n",
    "\n",
    "data_names = ['ogbg_moltox21','ogbg-molbace']\n",
    "\n",
    "data_names = ['MUTAG']\n",
    "data_names = []\n",
    "datasets_obj = {}\n",
    "for k, v in DATASETS.items():\n",
    "    if k not in data_names:\n",
    "        continue\n",
    "    print('loaded dataset, name:', k)\n",
    "    dat = v(use_node_attrs=True)\n",
    "    datasets_obj[k] = dat\n",
    "    # print(type(dat.dataset.get_data()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features: mean, std of: avg, pooling, cc, tri_num, cycle4_num, ..., kernel features\n",
    "\n",
    "import dataset_utils.node_feature_utils as nfu\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "\n",
    "def extract_features(adjs, labels):\n",
    "    \n",
    "    def get_mean_std_corr(features, labels):\n",
    "        \n",
    "        mean = np.array(np.mean(features))\n",
    "        std = np.array(np.std(features))\n",
    "        x = np.array(features).reshape(-1)\n",
    "        y = np.array(labels)\n",
    "        corr , _ = pearsonr(x, y)\n",
    "        \n",
    "        if np.isnan(corr):\n",
    "            corr = np.array([0])\n",
    "        \n",
    "        if not isinstance(corr, np.ndarray):\n",
    "            corr = np.array([corr])\n",
    "            \n",
    "        return np.array([mean.item(), std.item(), corr.item()])\n",
    "\n",
    "    \n",
    "    # F1: avgD:\n",
    "    avg_d = [nfu.graph_avg_degree(adj=adj) for adj in adjs]\n",
    "    f_avgD = get_mean_std_corr(avg_d, labels)\n",
    "    # F2: avgCC:\n",
    "    avg_cc = [nfu.node_cc_avg_feature(adj=adj) for adj in adjs]\n",
    "    f_avgCC = get_mean_std_corr(avg_cc, labels)\n",
    "    \n",
    "    # F3: avgD/N:\n",
    "    avg_DN = [nfu.graph_avgDN_feature(adj=adj) for adj in adjs]\n",
    "    f_avgDN = get_mean_std_corr(avg_DN, labels)\n",
    "    \n",
    "    # F4: node num N:\n",
    "    avg_N = [adj.shape[0] for adj in adjs]\n",
    "    f_avgN = get_mean_std_corr(avg_N, labels)\n",
    "    \n",
    "    # F5: labels\n",
    "    f_Y = get_mean_std_corr(labels, labels)[:2]\n",
    "    \n",
    "    # F6: cycles:\n",
    "    avg_cyc = [nfu.graph_cycle_feature(adj=adj,k='4-5-6-7') for adj in adjs]\n",
    "    f_cyc4 = get_mean_std_corr([c[0] for c in avg_cyc], labels)\n",
    "    f_cyc5 = get_mean_std_corr([c[1] for c in avg_cyc], labels)\n",
    "    f_cyc6 = get_mean_std_corr([c[2] for c in avg_cyc], labels)\n",
    "    f_cyc7 = get_mean_std_corr([c[3] for c in avg_cyc], labels)\n",
    "    \n",
    "        \n",
    "    feas = np.concatenate([f_avgD, f_avgCC, f_avgDN, f_avgN, f_Y, f_cyc4, f_cyc5, f_cyc6, f_cyc7], axis=0)\n",
    "    return feas\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets_obj['MUTAG']\n",
    "adjs = dataset.get_dense_adjs(dataset.dataset)\n",
    "labels = [d.y for d in dataset.dataset]\n",
    "\n",
    "feas = extract_features(adjs=adjs, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load each fold logs to construct y:\n",
    "import json\n",
    "\n",
    "# MUTAG:\n",
    "# GIN_degree_log_path: [result_GIN_0403_GIN_degree_MUTAG]\n",
    "# MLP_log_path: [result_GIN_0403_GIN_degree_MUTAG]\n",
    "# GIN_degree_log_path: [result_GIN_0403_GIN_degree_MUTAG]\n",
    "\"\"\"\n",
    "{\"best_config\": {\"config\": {\"model\": \"GIN\", \"device\": \"cuda:1\", \"batch_size\": 64, \"learning_rate\": 0.0001, \"classifier_epochs\": 200, \"hidden_units\": [64, 64, 64, 64], \"layer_num\": 5, \"optimizer\": \"Adam\", \"scheduler\": {\"class\": \"StepLR\", \"args\": {\"step_size\": 50, \"gamma\": 0.5}}, \"loss\": \"MulticlassClassificationLoss\", \"train_eps\": false, \"l2\": 0.0, \"aggregation\": \"sum\", \"gradient_clipping\": null, \"dropout\": 0.5, \"early_stopper\": {\"class\": \"Patience\", \"args\": {\"patience\": 50, \"use_loss\": false}}, \"shuffle\": true, \"resume\": false, \"additional_features\": \"degree\", \"node_attribute\": false, \"shuffle_feature\": false, \"roc_auc\": false, \"mol_split\": false, \"dataset\": \"syn_cc\", \"config_file\": \"gnn_comparison/config_GIN_lzd_degree.yml\", \"experiment\": \"endtoend\", \"result_folder\": \"results/result_0422_GIN_lzd_degree_syn_cc_0.1\", \"dataset_name\": \"syn_cc\", \"dataset_para\": \"0.1\", \"outer_folds\": 10, \"outer_processes\": 2, \"inner_folds\": 5, \"inner_processes\": 1, \"debug\": true, \"ogb_evl\": false}, \"TR_score\": 16.183574925298277, \"VL_score\": 21.505376272304083, \"TR_roc_auc\": -1, \"VL_roc_auc\": -1}, \"OUTER_TR\": 14.774557204254199, \"OUTER_TS\": 11.003236511378612, \"OUTER_TR_ROCAUC\": -1, \"OUTER_TE_ROCAUC\": -1}\n",
    "\"\"\"\n",
    "\n",
    "_OUTER_RESULTS_FILENAME = 'outer_results.json'\n",
    "\n",
    "def get_test_acc(data_root_path, fold=10):\n",
    "    if data_root_path is None:\n",
    "        return [None for _ in range(fold)]\n",
    "    \n",
    "    outer_TR_scores,outer_TS_scores,outer_TR_ROCAUC,outer_TE_ROCAUC = [],[],[],[]\n",
    "    for i in range(1, fold+1):\n",
    "        config_filename = os.path.join(data_root_path, f'OUTER_FOLD_{i}', _OUTER_RESULTS_FILENAME)\n",
    "\n",
    "        with open(config_filename, 'r') as fp:\n",
    "            outer_fold_scores = json.load(fp)\n",
    "\n",
    "            outer_TR_scores.append(outer_fold_scores['OUTER_TR'])\n",
    "            outer_TS_scores.append(outer_fold_scores['OUTER_TS'])\n",
    "            \n",
    "            if 'OUTER_TR_ROCAUC' in outer_fold_scores:\n",
    "                outer_TR_ROCAUC.append(outer_fold_scores['OUTER_TR_ROCAUC'])\n",
    "                outer_TE_ROCAUC.append(outer_fold_scores['OUTER_TE_ROCAUC'])\n",
    "\n",
    "    return outer_TS_scores\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load splits and logs for each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# syn_cc datasets:\n",
    "\n",
    "syn_cc_GNN_test_acc = []\n",
    "\n",
    "for i in range(1, 10):\n",
    "    data_root_path = f'./results/result_0422_GIN_lzd_degree_syn_cc_{i/10}/GIN_syn_cc_assessment/10_NESTED_CV'\n",
    "    syn_cc_GNN_test_acc.append(get_test_acc(data_root_path))\n",
    "    \n",
    "\n",
    "syn_cc_MLP_test_acc = []\n",
    "\n",
    "for i in range(1, 10):\n",
    "    data_root_path = f'./results/result_0422_Baseline_lzd_mlp_syn_cc_{i/10}/MolecularGraphMLP_syn_cc_assessment/10_NESTED_CV'\n",
    "    syn_cc_MLP_test_acc.append(get_test_acc(data_root_path))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct E of each fold, and plot\n",
    "\n",
    "# Effectiveness \n",
    "\n",
    "def get_E(Acc_MLP_avg_degree, Acc_GNN_degree, Acc_MLP_attr, Acc_GNN_attr):\n",
    "    factor = 0.5\n",
    "    if Acc_MLP_avg_degree is None:\n",
    "        E_struct = 0\n",
    "        factor = 1\n",
    "    else:\n",
    "        E_struct = (abs(Acc_GNN_degree - Acc_MLP_avg_degree) / Acc_MLP_avg_degree) * (100 - min(Acc_GNN_degree, Acc_MLP_avg_degree))\n",
    "    \n",
    "    if Acc_MLP_attr is None:\n",
    "        E_attribute = 0\n",
    "        factor = 1\n",
    "    else:\n",
    "        E_attribute = (abs(Acc_GNN_attr - Acc_MLP_attr) / Acc_MLP_attr) * (100 - min(Acc_GNN_attr, Acc_MLP_attr))\n",
    "    return (E_struct+E_attribute) * factor\n",
    "\n",
    "# NOTE: get E for each dataset:\n",
    "\n",
    "def plot_E(es, ax=None):\n",
    "    e_res = sorted(es, key=lambda x:x[0])\n",
    "    labels = [e[1] for e in e_res]\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(dpi=100)\n",
    "        \n",
    "    for e in e_res:\n",
    "        bars = ax.bar(e[1], e[0], label=e[1], hatch='\\\\', edgecolor='black', linewidth=0.5)\n",
    "\n",
    "    ax.set_xticks(range(len(labels)))\n",
    "    ax.set_xticklabels(labels, rotation=45, ha='center')\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.grid(linestyle='dashed',zorder=0)\n",
    "    ax.set_title('E=(E_struct+E_attr)/2')\n",
    "\n",
    "# ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sy_es = []\n",
    "for i in range(len(syn_cc_GNN_test_acc)):\n",
    "    # cc_MLP_avg_degree, Acc_GNN_degree, Acc_MLP_attr, Acc_GNN_att\n",
    "    sy_es.append((get_E(np.mean(syn_cc_MLP_test_acc[i]), np.mean(syn_cc_GNN_test_acc[i]),\n",
    "                       None, None), f'cc_corr:{(i+1)/10}'))\n",
    "# \n",
    "plot_E(sy_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Check mutag\n",
    "# construct each E with each fold and\n",
    "\n",
    "data_root_path = f'./results/result_GIN_0404_GIN_attr_MUTAG/GIN_MUTAG_assessment/10_NESTED_CV'\n",
    "mutag_GNN_attr_test_acc=get_test_acc(data_root_path)\n",
    "\n",
    "\n",
    "data_root_path = f'./results/result_GIN_0327_finger_mlp_attr_multicrossen_MUTAG/MolecularFingerprint_MUTAG_assessment/10_NESTED_CV'\n",
    "mutag_MLP_attr_test_acc = get_test_acc(data_root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check mutag\n",
    "# construct each E with each fold and\n",
    "\n",
    "data_root_path = f'./results/result_GIN_0404_GIN_attr_MUTAG/GIN_MUTAG_assessment/10_NESTED_CV'\n",
    "mutag_GNN_attr_test_acc=get_test_acc(data_root_path)\n",
    "\n",
    "\n",
    "data_root_path = f'./results/result_GIN_0327_finger_mlp_attr_multicrossen_MUTAG/MolecularFingerprint_MUTAG_assessment/10_NESTED_CV'\n",
    "mutag_MLP_attr_test_acc = get_test_acc(data_root_path)\n",
    "\n",
    "\n",
    "\n",
    "mutag_es = []\n",
    "for i in range(len(mutag_GNN_attr_test_acc)):\n",
    "    # cc_MLP_avg_degree, Acc_GNN_degree, Acc_MLP_attr, Acc_GNN_att\n",
    "    mutag_es.append((get_E(None, None,\n",
    "                       mutag_MLP_attr_test_acc[i], mutag_GNN_attr_test_acc[i]), f'mutag_fold:{(i+1)/10}'))\n",
    "\n",
    "# plot_E(mutag_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load splits as datasets:\n",
    "import torch_geometric.utils as torch_utils\n",
    "    \n",
    "\"\"\"\n",
    "the ixj th sample : (D_i_split_j, E_i_j)\n",
    "\"\"\"\n",
    "\n",
    "# Check mutag\n",
    "# construct each E with each fold and\n",
    "def is_pyg_dataset(d_name:str):\n",
    "    return d_name.startswith('ogb') or d_name.startswith('syn')\n",
    "\n",
    "def E_datasets(dataset, \n",
    "               MLP_log_path_struct=None, GNN_log_path_struct=None,\n",
    "               MLP_log_path_attr=None, GNN_log_path_attr=None, fold=10):\n",
    "    \n",
    "    \n",
    "    GNN_test_acc_struct = get_test_acc(GNN_log_path_struct, fold=fold)\n",
    "    MLP_test_acc_struct = get_test_acc(MLP_log_path_struct, fold=fold)\n",
    "    GNN_test_acc_attr = get_test_acc(GNN_log_path_attr, fold=fold)\n",
    "    MLP_test_acc_attr = get_test_acc(MLP_log_path_attr, fold=fold)\n",
    "    \n",
    "    def get_dense_adjs(dataset):\n",
    "        adjs = []\n",
    "\n",
    "        if is_pyg_dataset(dataset.name):\n",
    "            for d in dataset:\n",
    "                if d.edge_index.numel() < 1:\n",
    "                    N = d.x.shape[0]\n",
    "                    adj = np.ones(shape=(N, N))\n",
    "                else:\n",
    "                    adj = torch_utils.to_dense_adj(d.edge_index).numpy()[0]\n",
    "                adjs.append(adj)\n",
    "        else:\n",
    "            adjs = [d.to_numpy_array() for d in dataset.data]\n",
    "            \n",
    "        return adjs\n",
    "    \n",
    "    mutag_splits = []\n",
    "    for i in range(fold):\n",
    "        train_loader, val_loader = dataset.get_model_selection_fold(outer_idx=i, inner_idx=0, batch_size=1, shuffle=False)\n",
    "        adjs = get_dense_adjs(train_loader.dataset) + get_dense_adjs(val_loader.dataset)\n",
    "        labels = [d.y for d in train_loader.dataset] + [d.y for d in val_loader.dataset]\n",
    "        feas = extract_features(adjs=adjs, labels=labels)\n",
    "        e = get_E(MLP_test_acc_struct[i], GNN_test_acc_struct[i],  MLP_test_acc_attr[i], GNN_test_acc_attr[i])\n",
    "            \n",
    "        mutag_splits.append((feas, e))\n",
    "        \n",
    "    return mutag_splits\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MUTAG\n",
    "GNN_attr_log_path = f'./results/result_GIN_0404_GIN_attr_MUTAG/GIN_MUTAG_assessment/10_NESTED_CV'\n",
    "MLP_attr_log_path = f'./results/result_GIN_0327_finger_mlp_attr_multicrossen_MUTAG/MolecularFingerprint_MUTAG_assessment/10_NESTED_CV'\n",
    "dataset = datasets_obj['MUTAG']\n",
    "mutag_datasets = E_datasets(dataset, MLP_attr_log_path, GNN_attr_log_path, None, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del datasets_obj\n",
    "\n",
    "# DD:\n",
    "\n",
    "data_names = ['DD']\n",
    "              \n",
    "datasets_obj = {}\n",
    "for k, v in DATASETS.items():\n",
    "    if k not in data_names:\n",
    "        continue\n",
    "    print('loaded dataset, name:', k)\n",
    "    dat = v(use_node_attrs=True)\n",
    "    datasets_obj[k] = dat\n",
    "    # print(type(dat.dataset.get_data()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DD\n",
    "\"\"\"\n",
    "result_GIN_0308_DD_degree_attribute\n",
    "result_GIN_0309_DD_both_degree_attribute\n",
    "result_GIN_0310_DD_mix\n",
    "result_GIN_0311_DD_mix\n",
    "result_GIN_0312_DD_mix\n",
    "result_GIN_0318_decouple_attr_degree_DD\n",
    "result_GIN_0319_new_alpha_decouple_attr_degree_DD\n",
    "result_GIN_0327_finger_mlp_attr_crossen_DD\n",
    "result_GIN_0327_finger_mlp_attr_multicrossen_DD\n",
    "\"\"\"\n",
    "\n",
    "GNN_attr_log_path = f'./results/result_GIN_0308_DD_degree_attribute/Adapter_DD_assessment/10_NESTED_CV'\n",
    "MLP_attr_log_path = f'./results/result_GIN_0327_finger_mlp_attr_multicrossen_DD/MolecularFingerprint_DD_assessment/10_NESTED_CV'\n",
    "dataset = datasets_obj['DD']\n",
    "dd_datasets = E_datasets(dataset, None, None, GNN_attr_log_path, MLP_attr_log_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save datasets\n",
    "import pickle as pk\n",
    "\n",
    "def save_datasets(datasets, file_name):\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pk.dump(datasets, f)\n",
    "\n",
    "\n",
    "# save_datasets(mutag_datasets, 'mutag_datasets.pkl')\n",
    "# save_datasets(dd_datasets, 'dd_datasets.pkl')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del datasets_obj\n",
    "\n",
    "# DD:\n",
    "\n",
    "data_names = ['CIFAR10']\n",
    "              \n",
    "datasets_obj = {}\n",
    "for k, v in DATASETS.items():\n",
    "    if k not in data_names:\n",
    "        continue\n",
    "    print('loaded dataset, name:', k)\n",
    "    dat = v(use_node_attrs=True)\n",
    "    datasets_obj[k] = dat\n",
    "    # print(type(dat.dataset.get_data()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR10 only has on fold\n",
    "\"\"\"\n",
    "result_GIN_0317_decouple_degree_attr_CIFAR10\n",
    "result_GIN_0317_mix_degree_attr_CIFAR10\n",
    "result_GIN_0317_only_attr_CIFAR10\n",
    "result_GIN_0317_only_degree_CIFAR10\n",
    "result_GIN_0318_decouple_degree_attr_CIFAR10\n",
    "result_GIN_0327_finger_mlp_attr_multicrossen_CIFAR10\n",
    "result_GIN_0401_GIN_degree_CIFAR10\n",
    "result_GIN_0403_GIN_degree_CIFAR10\n",
    "\"\"\"\n",
    "\n",
    "MLP_log_path_degree = f'./results/result_GIN_0401_graph_mlp_avgDegree_CIFAR10/MolecularGraphMLP_CIFAR10_assessment/1_NESTED_CV'\n",
    "GNN_log_path_degree = f'./results/result_GIN_0317_only_degree_CIFAR10/GIN_CIFAR10_assessment/1_NESTED_CV'\n",
    "\n",
    "MLP_log_path_attr = f'./results/result_GIN_0327_finger_mlp_attr_multicrossen_CIFAR10/MolecularFingerprint_CIFAR10_assessment/10_NESTED_CV'\n",
    "GNN_log_path_attr = f'./results/result_GIN_0317_only_attr_CIFAR10/GIN_CIFAR10_assessment/1_NESTED_CV'\n",
    "\n",
    "dataset = datasets_obj['CIFAR10']\n",
    "cifar10_datasets = E_datasets(dataset, MLP_log_path_degree, GNN_log_path_degree, MLP_log_path_attr, GNN_log_path_attr, fold=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_datasets(dd_datasets, 'cifar10_datasets.pkl')\n",
    "\n",
    "del datasets_obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get NCI1\n",
    "\n",
    "# 'NCI1', 'IMDB-MULTI', 'REDDIT-BINARY', 'CIFAR10',\n",
    "# #               'ogbg_molhiv', 'ogbg_moltox21', 'ogbg-molbace', 'MUTAG']\n",
    "\n",
    "\"\"\"\n",
    "result_GIN_0312_attr_degree_NCI1\n",
    "result_GIN_0312_decouple_attr_degree_NCI1\n",
    "result_GIN_0312_only_attribute_NCI\n",
    "result_GIN_0312_only_degree_NCI\n",
    "result_GIN_0313_degree_shuffle_NCI1\n",
    "result_GIN_0313_pagerank_NCI1\n",
    "result_GIN_0318_decouple_attr_degree_NCI1\n",
    "result_GIN_0319_new_alpha_decouple_attr_degree_NCI1\n",
    "result_GIN_0327_finger_mlp_attr_crossen_NCI1\n",
    "result_GIN_0327_finger_mlp_attr_multicrossen_NCI1\n",
    "result_GIN_0403_GIN_attr_NCI1\n",
    "result_GIN_0403_GIN_degree_NCI1\n",
    "result_GIN_0404_GIN_attr_NCI1\n",
    "\"\"\"\n",
    "del datasets_obj\n",
    "\n",
    "data_names = ['NCI1']\n",
    "              \n",
    "datasets_obj = {}\n",
    "for k, v in DATASETS.items():\n",
    "    if k not in data_names:\n",
    "        continue\n",
    "    print('loaded dataset, name:', k)\n",
    "    dat = v(use_node_attrs=True)\n",
    "    datasets_obj[k] = dat\n",
    "\n",
    "MLP_log_path_degree = f'./results/result_GIN_0401_graph_mlp_avgDegree_CIFAR10/MolecularGraphMLP_CIFAR10_assessment/1_NESTED_CV'\n",
    "GNN_log_path_degree = f'./results/result_GIN_0317_only_degree_CIFAR10/GIN_CIFAR10_assessment/1_NESTED_CV'\n",
    "\n",
    "MLP_log_path_attr = f'./results/result_GIN_0327_finger_mlp_attr_multicrossen_NCI1/MolecularFingerprint_NCI1_assessment/10_NESTED_CV'\n",
    "GNN_log_path_attr = f'./results/result_GIN_0404_GIN_attr_NCI1/GIN_NCI1_assessment/1_NESTED_CV'\n",
    "\n",
    "dataset = datasets_obj['NCI1']\n",
    "cifar10_datasets = E_datasets(dataset, MLP_log_path_degree, GNN_log_path_degree, MLP_log_path_attr, GNN_log_path_attr, fold=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load regressor datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all pickle datasets\n",
    "import pickle as pk\n",
    "\n",
    "def load_datasets(file_name):\n",
    "    with open(file_name, 'rb') as f:\n",
    "        datasets = pk.load(f)\n",
    "    return datasets\n",
    "\n",
    "d1 = load_datasets('mutag_datasets.pkl')\n",
    "d2 = load_datasets('dd_datasets.pkl')\n",
    "d3 = load_datasets('cifar10_datasets.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SynCC\n",
    "\n",
    "data_names = ['syn_cc']\n",
    "              \n",
    "syn_datasets = []\n",
    "for i in range(1, 10):\n",
    "    v = DATASETS['syn_cc']\n",
    "    config = {'dataset_para':f'{i/10}'}\n",
    "    dat = v(use_node_attrs=True, config=config)\n",
    "\n",
    "    gnn_data_root_path = f'./results/result_0422_GIN_lzd_degree_syn_cc_{i/10}/GIN_syn_cc_assessment/10_NESTED_CV'\n",
    "    mlp_data_root_path = f'./results/result_0422_Baseline_lzd_mlp_syn_cc_{i/10}/MolecularGraphMLP_syn_cc_assessment/10_NESTED_CV'\n",
    "    e_dataset_10_folds = E_datasets(dat, mlp_data_root_path, gnn_data_root_path, None, None)\n",
    "    syn_datasets.extend(e_dataset_10_folds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_datasets(syn_datasets, 'syn_datasets.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_datasets = load_datasets('syn_datasets.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all datasets:\n",
    "\n",
    "\n",
    "all_datasets = []\n",
    "\n",
    "\n",
    "for d in syn_datasets:\n",
    "    all_datasets.append(d)\n",
    "    \n",
    "all_datasets.extend(d1+d2+d3)\n",
    "\n",
    "store_each_len = []\n",
    "\n",
    "for d in all_datasets:\n",
    "    store_each_len.append(len(d))\n",
    "    \n",
    "def belong_to_which_datasets(idx, store_each_len):\n",
    "    sum_len = 0\n",
    "    for i, l in enumerate(store_each_len):\n",
    "        sum_len += l\n",
    "        if idx < sum_len:\n",
    "            return i\n",
    "    return -1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# construct regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def mean_norm(x):\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(x), scaler\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.X = torch.tensor([t[0] for t in data], dtype=torch.float32)\n",
    "        self.Y = torch.tensor([t[1] for t in data], dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_data(num_samples):\n",
    "    features = np.random.rand(num_samples, 26)\n",
    "    labels = np.random.rand(num_samples, 1)\n",
    "    return [(features[i], labels[i]) for i in range(num_samples)]\n",
    "\n",
    "\n",
    "\n",
    "X = np.array([t[0] for t in all_datasets])\n",
    "Y = np.array([t[1] for t in all_datasets])\n",
    "\n",
    "normalized_x, scaler_x = mean_norm(X)\n",
    "normalized_y, scaler_y = mean_norm(Y.reshape(-1, 1))\n",
    "\n",
    "print(normalized_x.shape, normalized_y.shape)\n",
    "\n",
    "print(len(normalized_x))\n",
    "normed_combined_data = [(normalized_x[i], normalized_y[i]) for i in range(normalized_x.shape[0])]\n",
    "\n",
    "dataset = CustomDataset(normed_combined_data)\n",
    "\n",
    "train_size = int(0.6 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_data, val_data, test_data = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "class MLPRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPRegressor, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(26, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "def evaluate(loader, model,):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    preds = []\n",
    "    Y = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.view(-1, 1))\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            total_samples += inputs.size(0)\n",
    "            preds.append(outputs.cpu().numpy())\n",
    "            Y.append(labels.cpu().numpy())\n",
    "            \n",
    "    \n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    Y = np.concatenate(Y, axis=0).ravel()\n",
    "    \n",
    "    mae = mean_absolute_error(Y, preds)\n",
    "    mse = mean_squared_error(Y, preds)\n",
    "    rmse = math.sqrt(mse)\n",
    "    r2 = r2_score(Y, preds)\n",
    "    \n",
    "    results = (mae, mse, rmse, r2)\n",
    "    return mse, results, preds, Y\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "model = MLPRegressor().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 500\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss, _, _, _ = evaluate(train_loader, model)\n",
    "    val_loss, _,_,_ = evaluate(val_loader, model)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "test_loss, test_results,_, _ = evaluate(test_loader, model)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f'MLPs: {round(test_results[0], 2)} & {round(test_results[1], 2)} & {round(test_results[2], 2)} & {round(test_results[3], 2)}')\n",
    "\n",
    "# Plot train and validation loss curves\n",
    "plt.figure()\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Train and Validation Loss Curves\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def train_regressor(loader, regressor):\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for inputs, labels in loader:\n",
    "        X.extend(inputs.cpu().numpy())\n",
    "        Y.extend(labels.cpu().numpy())\n",
    "\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y).ravel()\n",
    "\n",
    "    regressor.fit(X, Y)\n",
    "    \n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import math\n",
    "\n",
    "\n",
    "def regressor_evaluate(loader, regressor):\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for inputs, labels in loader:\n",
    "        X.extend(inputs.cpu().numpy())\n",
    "        Y.extend(labels.cpu().numpy())\n",
    "\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y).ravel()\n",
    "\n",
    "    preds = regressor.predict(X)\n",
    "    mae = mean_absolute_error(Y, preds)\n",
    "    mse = mean_squared_error(Y, preds)\n",
    "    rmse = math.sqrt(mse)\n",
    "    r2 = r2_score(Y, preds)\n",
    "    \n",
    "    results = (mae, mse, rmse, r2)\n",
    "    return mse, results, preds, Y\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baseline regressors:\n",
    "- random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_regressor(regressor, regressor_name):\n",
    "        \n",
    "    # Choose a regressor\n",
    "    # regressor = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "    train_regressor(train_loader, regressor)\n",
    "\n",
    "    train_loss, train_results, train_preds, train_Y = regressor_evaluate(train_loader, regressor)\n",
    "    val_loss, val_results, val_preds, val_Y = regressor_evaluate(val_loader, regressor)\n",
    "    test_loss, test_reuslts, test_preds, test_Y = regressor_evaluate(test_loader, regressor)\n",
    "\n",
    "    # print(f\"regressor_name: {regressor_name}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Test Loss: {test_loss:.4f} \\\n",
    "    #       (mse, rmse, r^2): {test_reuslts}\")\n",
    "    \n",
    "    print(f'{regressor_name}: {round(test_reuslts[0], 2)} & {round(test_reuslts[1], 2)} & {round(test_reuslts[2], 2)} & {round(test_reuslts[3], 2)}')\n",
    "    plt.figure()\n",
    "    plt.plot(scaler_y.inverse_transform(test_preds), label=\"Predictions\")\n",
    "    plt.plot(scaler_y.inverse_transform(test_Y), label=\"True Labels\")\n",
    "    plt.title(regressor_name)\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Choose a regressor\n",
    "regressors = {'SVR': SVR(),'Ridge': Ridge(), \"RandomForestRegressor\": RandomForestRegressor(),  \n",
    "              \"LinearRegression\": LinearRegression()}\n",
    "\n",
    "for k, regressor in regressors.items():\n",
    "    run_regressor(regressor, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "regressors = {'XG boost': xgb.XGBRegressor(objective ='reg:squarederror'),\n",
    "              'LGBMRegressor': lgb.LGBMRegressor()}\n",
    "\n",
    "for k, regressor in regressors.items():\n",
    "    run_regressor(regressor, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use test dataset to evaluate the model\n",
    "\n",
    "# average each datasets:\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "test_dataset_idx = [belong_to_which_datasets(i, store_each_len) for i in test_data.indices]\n",
    "print(Counter(test_dataset_idx))\n",
    "\n",
    "# Get predictions and true labels from the test dataset\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        predictions.extend(outputs.squeeze().cpu().numpy())\n",
    "        true_labels.extend(labels.squeeze().cpu().numpy())\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(scaler_y.inverse_transform(predictions), label=\"Predictions\")\n",
    "plt.plot(scaler_y.inverse_transform(true_labels), label=\"True Labels\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# # Inverse normalization of true_labels and predictions\n",
    "# combined_labels = np.array([t[1] for t in combined_data])\n",
    "# label_norm = np.linalg.norm(combined_labels)\n",
    "# inverse_true_labels = np.multiply(true_labels, label_norm)\n",
    "# inverse_predictions = np.multiply(predictions, label_norm)\n",
    "\n",
    "# # Plot predicted vs true labels\n",
    "# plt.figure()\n",
    "# plt.scatter(true_labels, inverse_predictions, alpha=0.5)\n",
    "# plt.xlabel(\"True Labels\")\n",
    "# plt.ylabel(\"Predicted Labels\")\n",
    "# plt.title(\"Predicted vs True Labels\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
