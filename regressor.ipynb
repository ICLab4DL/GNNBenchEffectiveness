{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressor\n",
    "1. load datasets\n",
    "1. sample from datasets\n",
    "1. construct features\n",
    "1. construct labels (load from logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets:\n",
    "\n",
    "import importlib\n",
    "import random\n",
    "import argparse\n",
    "import configparser\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_sparse\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch_geometric.utils import negative_sampling, to_networkx\n",
    "from typing import Union, Tuple\n",
    "from torch_geometric.typing import OptPairTensor, Adj, OptTensor, Size\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "\n",
    "from ogb.linkproppred import PygLinkPropPredDataset, Evaluator\n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy\n",
    "import math\n",
    "\n",
    "\n",
    "from dataset_utils import node_feature_utils\n",
    "from dataset_utils.node_feature_utils import *\n",
    "import my_utils as utils\n",
    "import sys,os\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "\n",
    "importlib.reload(utils)\n",
    "\n",
    "# save datasets\n",
    "import pickle as pk\n",
    "\n",
    "def save_datasets(datasets, file_name):\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pk.dump(datasets, f)\n",
    "\n",
    "def load_datasets(file_name):\n",
    "    with open(file_name, 'rb') as f:\n",
    "        datasets = pk.load(f)\n",
    "    return datasets\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load regressor datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- small: MUTAG, NCI1, DD, CIFAR10, MNIST, SYN_CC\n",
    "- middle: ogbg-molhiv, ogbg-molbace, ogbg-moltox21\n",
    "\"\"\"\n",
    "\n",
    "# small scale: < 10k\n",
    "small_datasets = ['mutag', 'nci1', 'dd', 'cifar10', 'mnist', 'syn_cc']\n",
    "d1 = load_datasets('mutag_datasets.pkl')\n",
    "d2 = load_datasets('dd_datasets.pkl')\n",
    "d3 = load_datasets('cifar10_datasets.pkl')\n",
    "d4 = load_datasets('cifar10_datasets.pkl')\n",
    "\n",
    "# SynCC\n",
    "syn_cc_datasets = load_datasets('syn_datasets.pkl')\n",
    "\n",
    "# middle scale: > 10k\n",
    "\n",
    "\n",
    "# large scale: > 100k\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all datasets:\n",
    "\n",
    "\n",
    "all_datasets = []\n",
    "\n",
    "for d in syn_cc_datasets:\n",
    "    all_datasets.append(d)\n",
    "    \n",
    "all_datasets.extend(d1+d2+d3)\n",
    "\n",
    "store_each_len = []\n",
    "\n",
    "for d in all_datasets:\n",
    "    store_each_len.append(len(d))\n",
    "    \n",
    "def belong_to_which_datasets(idx, store_each_len):\n",
    "    sum_len = 0\n",
    "    for i, l in enumerate(store_each_len):\n",
    "        sum_len += l\n",
    "        if idx < sum_len:\n",
    "            return i\n",
    "    return -1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# construct regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def mean_norm(x):\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(x), scaler\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.X = torch.tensor([t[0] for t in data], dtype=torch.float32)\n",
    "        self.Y = torch.tensor([t[1] for t in data], dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_data(num_samples):\n",
    "    features = np.random.rand(num_samples, 26)\n",
    "    labels = np.random.rand(num_samples, 1)\n",
    "    return [(features[i], labels[i]) for i in range(num_samples)]\n",
    "\n",
    "\n",
    "\n",
    "X = np.array([t[0] for t in all_datasets])\n",
    "Y = np.array([t[1] for t in all_datasets])\n",
    "\n",
    "normalized_x, scaler_x = mean_norm(X)\n",
    "normalized_y, scaler_y = mean_norm(Y.reshape(-1, 1))\n",
    "\n",
    "print(normalized_x.shape, normalized_y.shape)\n",
    "\n",
    "print(len(normalized_x))\n",
    "normed_combined_data = [(normalized_x[i], normalized_y[i]) for i in range(normalized_x.shape[0])]\n",
    "\n",
    "dataset = CustomDataset(normed_combined_data)\n",
    "\n",
    "train_size = int(0.6 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_data, val_data, test_data = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "class MLPRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPRegressor, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(26, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "def evaluate(loader, model,):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    preds = []\n",
    "    Y = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.view(-1, 1))\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            total_samples += inputs.size(0)\n",
    "            preds.append(outputs.cpu().numpy())\n",
    "            Y.append(labels.cpu().numpy())\n",
    "            \n",
    "    \n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    Y = np.concatenate(Y, axis=0).ravel()\n",
    "    \n",
    "    mae = mean_absolute_error(Y, preds)\n",
    "    mse = mean_squared_error(Y, preds)\n",
    "    rmse = math.sqrt(mse)\n",
    "    r2 = r2_score(Y, preds)\n",
    "    \n",
    "    results = (mae, mse, rmse, r2)\n",
    "    return mse, results, preds, Y\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "model = MLPRegressor().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 500\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss, _, _, _ = evaluate(train_loader, model)\n",
    "    val_loss, _,_,_ = evaluate(val_loader, model)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "test_loss, test_results,_, _ = evaluate(test_loader, model)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f'MLPs: {round(test_results[0], 2)} & {round(test_results[1], 2)} & {round(test_results[2], 2)} & {round(test_results[3], 2)}')\n",
    "\n",
    "# Plot train and validation loss curves\n",
    "plt.figure()\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Train and Validation Loss Curves\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def train_regressor(loader, regressor):\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for inputs, labels in loader:\n",
    "        X.extend(inputs.cpu().numpy())\n",
    "        Y.extend(labels.cpu().numpy())\n",
    "\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y).ravel()\n",
    "\n",
    "    regressor.fit(X, Y)\n",
    "    \n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import math\n",
    "\n",
    "\n",
    "def regressor_evaluate(loader, regressor):\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for inputs, labels in loader:\n",
    "        X.extend(inputs.cpu().numpy())\n",
    "        Y.extend(labels.cpu().numpy())\n",
    "\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y).ravel()\n",
    "\n",
    "    preds = regressor.predict(X)\n",
    "    mae = mean_absolute_error(Y, preds)\n",
    "    mse = mean_squared_error(Y, preds)\n",
    "    rmse = math.sqrt(mse)\n",
    "    r2 = r2_score(Y, preds)\n",
    "    \n",
    "    results = (mae, mse, rmse, r2)\n",
    "    return mse, results, preds, Y\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baseline regressors:\n",
    "- random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_regressor(regressor, regressor_name):\n",
    "        \n",
    "    # Choose a regressor\n",
    "    # regressor = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "    train_regressor(train_loader, regressor)\n",
    "\n",
    "    train_loss, train_results, train_preds, train_Y = regressor_evaluate(train_loader, regressor)\n",
    "    val_loss, val_results, val_preds, val_Y = regressor_evaluate(val_loader, regressor)\n",
    "    test_loss, test_reuslts, test_preds, test_Y = regressor_evaluate(test_loader, regressor)\n",
    "\n",
    "    # print(f\"regressor_name: {regressor_name}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Test Loss: {test_loss:.4f} \\\n",
    "    #       (mse, rmse, r^2): {test_reuslts}\")\n",
    "    \n",
    "    print(f'{regressor_name}: {round(test_reuslts[0], 2)} & {round(test_reuslts[1], 2)} & {round(test_reuslts[2], 2)} & {round(test_reuslts[3], 2)}')\n",
    "    plt.figure()\n",
    "    plt.plot(scaler_y.inverse_transform(test_preds), label=\"Predictions\")\n",
    "    plt.plot(scaler_y.inverse_transform(test_Y), label=\"True Labels\")\n",
    "    plt.title(regressor_name)\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Choose a regressor\n",
    "regressors = {'SVR': SVR(),'Ridge': Ridge(), \"RandomForestRegressor\": RandomForestRegressor(),  \n",
    "              \"LinearRegression\": LinearRegression()}\n",
    "\n",
    "for k, regressor in regressors.items():\n",
    "    run_regressor(regressor, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "regressors = {'XG boost': xgb.XGBRegressor(objective ='reg:squarederror'),\n",
    "              'LGBMRegressor': lgb.LGBMRegressor()}\n",
    "\n",
    "for k, regressor in regressors.items():\n",
    "    run_regressor(regressor, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use test dataset to evaluate the model\n",
    "\n",
    "# average each datasets:\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "test_dataset_idx = [belong_to_which_datasets(i, store_each_len) for i in test_data.indices]\n",
    "print(Counter(test_dataset_idx))\n",
    "\n",
    "# Get predictions and true labels from the test dataset\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        predictions.extend(outputs.squeeze().cpu().numpy())\n",
    "        true_labels.extend(labels.squeeze().cpu().numpy())\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(scaler_y.inverse_transform(predictions), label=\"Predictions\")\n",
    "plt.plot(scaler_y.inverse_transform(true_labels), label=\"True Labels\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot predictions of other real-world datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR10 only has on fold\n",
    "\"\"\"\n",
    "result_GIN_0317_decouple_degree_attr_CIFAR10\n",
    "result_GIN_0317_mix_degree_attr_CIFAR10\n",
    "result_GIN_0317_only_attr_CIFAR10\n",
    "result_GIN_0317_only_degree_CIFAR10\n",
    "result_GIN_0318_decouple_degree_attr_CIFAR10\n",
    "result_GIN_0327_finger_mlp_attr_multicrossen_CIFAR10\n",
    "result_GIN_0401_GIN_degree_CIFAR10\n",
    "result_GIN_0403_GIN_degree_CIFAR10\n",
    "\"\"\"\n",
    "\n",
    "MLP_log_path_degree = f'./results/result_GIN_0401_graph_mlp_avgDegree_CIFAR10/MolecularGraphMLP_CIFAR10_assessment/1_NESTED_CV'\n",
    "GNN_log_path_degree = f'./results/result_GIN_0317_only_degree_CIFAR10/GIN_CIFAR10_assessment/1_NESTED_CV'\n",
    "\n",
    "MLP_log_path_attr = f'./results/result_GIN_0327_finger_mlp_attr_multicrossen_CIFAR10/MolecularFingerprint_CIFAR10_assessment/10_NESTED_CV'\n",
    "GNN_log_path_attr = f'./results/result_GIN_0317_only_attr_CIFAR10/GIN_CIFAR10_assessment/1_NESTED_CV'\n",
    "\n",
    "dataset = datasets_obj['CIFAR10']\n",
    "cifar10_datasets = E_datasets(dataset, MLP_log_path_degree, GNN_log_path_degree, MLP_log_path_attr, GNN_log_path_attr, fold=1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
