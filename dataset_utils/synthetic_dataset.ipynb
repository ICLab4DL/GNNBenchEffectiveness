{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Given correlation coefficient r of X and Y, and given X, now generate Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap\n",
    "import networkx as nx\n",
    "\n",
    "# here..\n",
    "cmaps = {}\n",
    "\n",
    "gradient = np.linspace(0, 1, 256)\n",
    "gradient = np.vstack((gradient, gradient))\n",
    "\n",
    "x_y_label_font = 20\n",
    "x_y_legend_font = 20\n",
    "\n",
    "plt.rc('font', family='Times New Roman')\n",
    "fig_dpi = 220\n",
    "fig_shape_squre = (6, 5)\n",
    "\n",
    "def plot_color_gradients(category, cmap_list):\n",
    "    # Create figure and adjust figure height to number of colormaps\n",
    "    nrows = len(cmap_list)\n",
    "    figh = 0.35 + 0.15 + (nrows + (nrows - 1) * 0.1) * 0.22\n",
    "    fig, axs = plt.subplots(nrows=nrows + 1, figsize=(6.4, figh), dpi=100)\n",
    "    fig.subplots_adjust(top=1 - 0.35 / figh, bottom=0.15 / figh,\n",
    "                        left=0.2, right=0.99)\n",
    "    axs[0].set_title(f'{category} colormaps', fontsize=14)\n",
    "\n",
    "    for ax, name in zip(axs, cmap_list):\n",
    "        ax.imshow(gradient, aspect='auto', cmap=plt.get_cmap(name))\n",
    "        ax.text(-0.01, 0.5, name, va='center', ha='right', fontsize=10,\n",
    "                transform=ax.transAxes)\n",
    "\n",
    "    # Turn off *all* ticks & spines, not just the ones with colormaps.\n",
    "    for ax in axs:\n",
    "        ax.set_axis_off()\n",
    "\n",
    "    # Save colormap list for later.\n",
    "    cmaps[category] = cmap_list\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "class MyColor(object):\n",
    "    def __init__(self, cmap_name, skip_idx=5, backup_name='Set1', \n",
    "                 backup_color=3, add_red=False, pre_defined=False):\n",
    "        if pre_defined:\n",
    "            # colors = ['#3682be','#45a776','#f05326','#eed777','#334f65','#b3974e','#38cb7d','#ddae33','#844bb3','#93c555','#5f6694','#df3881']\n",
    "            colors = '00a8e1 - 99cc00 - e30039 - 800080 - 00994e - ff6600 - 808000 - db00c2 - 008080 - 0000ff - c8cc00'\n",
    "            colors = [\"#\"+c.strip() for c in colors.split('-')]\n",
    "            print(colors)\n",
    "            cmap = ListedColormap(colors, name = 'mycmap')\n",
    "            self.color_set = list(cmap.colors)\n",
    "        else:\n",
    "            if isinstance(cmap_name, list):\n",
    "                #NOTE: combine all cmaps:\n",
    "                self.color_set = []\n",
    "                for cname in cmap_name:\n",
    "                    self.color_set.extend(list(plt.get_cmap(cname).colors))\n",
    "                print('color_set: ', self.color_set[15])\n",
    "            else:\n",
    "                self.color_set = list(plt.get_cmap(cmap_name).colors)\n",
    "                \n",
    "        # NOTE: always ignore light yellow in Set1\n",
    "\n",
    "        if add_red:\n",
    "            self.light_set = list(plt.get_cmap('Set1').colors)\n",
    "            self.color_set  = [v for i, v in enumerate(list(plt.get_cmap(cmap_name).colors)[:-1]) if i!=5]\n",
    "            self.color_set.extend([self.light_set[0]])\n",
    "            self.color_set.extend([self.light_set[4]])\n",
    "\n",
    "        self.backup_set = plt.get_cmap(backup_name).colors\n",
    "        self.backup_color = backup_color\n",
    "        self.skip_idx=skip_idx\n",
    "        self.idx = 0\n",
    "        self.color_len = len(self.color_set)\n",
    "        \n",
    "    def get_color(self):\n",
    "        if self.idx == self.color_len - 1:\n",
    "            self.idx = 0\n",
    "        if self.idx == self.skip_idx:\n",
    "            self.idx += 1\n",
    "            return self.backup_set[self.backup_color]\n",
    "        color = self.color_set[self.idx]\n",
    "        self.idx += 1\n",
    "        return color\n",
    "    \n",
    "    def shuffle(self):\n",
    "        np.random.shuffle(self.color_set)\n",
    "    \n",
    "\n",
    "plot_color_gradients('Qualitative',\n",
    "                     ['Pastel1', 'Pastel2', 'Paired', 'Accent', 'Dark2',\n",
    "                      'Set1', 'Set2', 'Set3', 'tab10', 'tab20', 'tab20b',\n",
    "                      'tab20c'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "def z_norm(x_data):\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    return min_max_scaler.fit_transform(x_data)\n",
    "\n",
    "def mean_norm(x_data):\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    return scaler.fit_transform(x_data)\n",
    "\n",
    "\n",
    "def generate_Y_from_X(X1, X2, X3, r1, r2, r3):\n",
    "    \"\"\"\n",
    "    Generates values for variable Y from {0, 1, 2} based on the given independent variables X1, X2, X3 and their\n",
    "    corresponding correlation coefficients r1, r2, r3.\n",
    "    \n",
    "    Args:\n",
    "        X1 (ndarray): Sample of variable X1 from normal distribution.\n",
    "        X2 (ndarray): Sample of variable X2 from normal distribution.\n",
    "        X3 (ndarray): Sample of variable X3 from normal distribution.\n",
    "        r1 (float): Correlation coefficient between X1 and Y.\n",
    "        r2 (float): Correlation coefficient between X2 and Y.\n",
    "        r3 (float): Correlation coefficient between X3 and Y.\n",
    "        \n",
    "    Returns:\n",
    "        ndarray: Generated values for variable Y.\n",
    "    \"\"\"\n",
    "    # Calculate the mean and standard deviation of X1, X2, X3\n",
    "    X1= X1.squeeze()\n",
    "    X2= X2.squeeze()\n",
    "    X3= X3.squeeze()\n",
    "    \n",
    "    mean_X1 = np.mean(X1)\n",
    "    mean_X2 = np.mean(X2)\n",
    "    mean_X3 = np.mean(X3)\n",
    "    \n",
    "    print('mean_X1: ', mean_X1)\n",
    "    print('mean_X2: ', mean_X2)\n",
    "    print('mean_X3: ', mean_X3)\n",
    "    \n",
    "    std_X1 = np.std(X1)\n",
    "    std_X2 = np.std(X2)\n",
    "    std_X3 = np.std(X3)\n",
    "    \n",
    "    # Calculate the standard deviation of Y\n",
    "    std_Y = np.sqrt((std_X1**2 * r1**2 + std_X2**2 * r2**2 + std_X3**2 * r3**2) / (r1**2 + r2**2 + r3**2 + 2*r1*r2*r3))\n",
    "    \n",
    "    # Generate values for Y from normal distribution with mean 0 and calculated standard deviation\n",
    "    # Y = np.random.normal(loc=0, scale=std_Y, size=len(X1))\n",
    "    \n",
    "    # Add the means of X1, X2, X3 and the calculated standard deviation of Y to the generated values to get the final values for Y\n",
    "    # Y = mean_X1 + mean_X2 + mean_X3 + r1 * ((X1 - mean_X1) * std_Y / std_X1) + r2 * ((X2 - mean_X2) * std_Y / std_X2) + r3 * ((X3 - mean_X3) * std_Y / std_X3)\n",
    "    Y = r1 * ((X1 - mean_X1) * std_Y / std_X1) + r2 * ((X2 - mean_X2) * std_Y / std_X2) + r3 * ((X3 - mean_X3) * std_Y / std_X3)\n",
    "    \n",
    "    # Round the generated values of Y to the nearest integer and clip them to be within {0, 1, 2}\n",
    "    # Y = np.round(Y * 49 + 50).clip(0, 2).astype(int)\n",
    "    Y = z_norm(Y.reshape(-1, 1))\n",
    "    Y = Y.squeeze()\n",
    "    Y = np.round(Y * 10).astype(int)\n",
    "    \n",
    "    return Y\n",
    "\n",
    "# Generate samples of X1, X2, X3 with sample size 100 from normal distribution\n",
    "\n",
    "normal_dist = True\n",
    "sample_size=  10000\n",
    "\n",
    "if not normal_dist:\n",
    "    X1 = np.random.uniform(10, 100, size=sample_size)\n",
    "    X2 = np.random.uniform(20, 50, size=sample_size)\n",
    "    X3 = np.random.uniform(0, 1, size=sample_size)\n",
    "else:\n",
    "    X1 = np.random.normal(loc=1, scale=1, size=sample_size)\n",
    "    X2 = np.random.normal(loc=2, scale=1, size=sample_size)\n",
    "    X3 = np.random.normal(loc=3, scale=1, size=sample_size)\n",
    "\n",
    "# Specify the correlation coefficients between X1, X2, X3, and Y\n",
    "r1 = -0.3\n",
    "r2 = 0.5\n",
    "r3 = 0.2\n",
    "\n",
    "use_z_norm = False\n",
    "\n",
    "norm = z_norm if use_z_norm else mean_norm\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(X1)\n",
    "plt.show()\n",
    "X1 = norm(X1.reshape(-1, 1))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(X1)\n",
    "plt.show()\n",
    "\n",
    "X2 = norm(X2.reshape(-1, 1))\n",
    "X3 = norm(X3.reshape(-1, 1))\n",
    "\n",
    "\n",
    "# Generate values for Y based on X1, X2, X3, and their correlation coefficients\n",
    "Y = generate_Y_from_X(X1, X2, X3, r1, r2, r3)\n",
    "\n",
    "# Check the Pearson correlation between X1 and Y, X2 and Y, X3 and Y\n",
    "corr_X1_Y, _ = pearsonr(X1.squeeze(), Y)\n",
    "corr_X2_Y, _ = pearsonr(X2.squeeze(), Y)\n",
    "corr_X3_Y, _ = pearsonr(X3.squeeze(), Y)\n",
    "\n",
    "# Plot the linear correlation figures for X1 vs Y, X2 vs Y, X3 vs Y in a single figure with different colors\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "ax[0].scatter(X1, Y, color='r', label=f'Corr: {corr_X1_Y:.2f}')\n",
    "ax[0].set_xlabel('X1')\n",
    "ax[0].set_ylabel('Y')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].scatter(X2, Y, color='g', label=f'Corr: {corr_X2_Y:.2f}')\n",
    "ax[1].set_xlabel('X2')\n",
    "ax[1].set_ylabel('Y')\n",
    "ax[1].legend()\n",
    "\n",
    "ax[2].scatter(X3, Y, color='b', label=f'Corr: {corr_X3_Y:.2f}')\n",
    "ax[2].set_xlabel('X3')\n",
    "ax[2].set_ylabel('Y')\n",
    "ax[2].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn import preprocessing\n",
    "from functools import reduce\n",
    "\n",
    "def z_norm(x_data):\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    return min_max_scaler.fit_transform(x_data), min_max_scaler\n",
    "\n",
    "def mean_norm(x_data):\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    return scaler.fit_transform(x_data)\n",
    "\n",
    "\n",
    "\n",
    "sigmas = [1, 2, 3]\n",
    "\n",
    "def get_XY(uniform=True, mix=False):\n",
    "        \n",
    "    sample_size =  10000\n",
    "\n",
    "    \n",
    "    if not uniform:\n",
    "        ns = [np.random.normal(loc=0, scale=1, size=sample_size) for _ in range(len(sigmas))]\n",
    "    else:\n",
    "        ns = [np.random.uniform(0, 1, size=sample_size) for _ in range(len(sigmas))]\n",
    "\n",
    "    if mix:\n",
    "        ns[-1] =  np.random.normal(loc=0, scale=1, size=sample_size) if uniform else np.random.uniform(-np.sqrt(3), np.sqrt(3), size=sample_size)\n",
    "    # Specify the correlation coefficients between X1, X2, X3, and Y\n",
    "\n",
    "    rs = [-0.7, 0.1, 0.7]\n",
    "    \n",
    "    sum_rs = np.sum([r**2 for r in rs])\n",
    "    print('sum_rs:', sum_rs)\n",
    "\n",
    "    scale = np.sqrt(12) if uniform else 1\n",
    "    scale = 1\n",
    "\n",
    "    sigma_y = 1\n",
    "\n",
    "    r5 = np.sqrt(1 - np.sum([r**2 for r in rs]))\n",
    "    rs.extend([r5])\n",
    "\n",
    "    Xs = [1*ns[i] for i in range(len(sigmas))]\n",
    "    # Xs = [sigmas[i]*ns[i] for i in range(len(sigmas))]\n",
    "    \n",
    "    Y = scale * sigma_y * (reduce(lambda x, y: x+y, map(lambda x: x[0]*x[1], zip(rs, ns))))\n",
    "\n",
    "    Y, _ = z_norm(Y.reshape(-1, 1))\n",
    "    \n",
    "    Y = Y.squeeze()\n",
    "    Y = np.round(Y * 10).astype(int)\n",
    "    \n",
    "    return Xs, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # ROUND:\n",
    "\n",
    "colors = MyColor(cmap_name=\"Set2\")\n",
    "\n",
    "def plot_xy(Xs, Y, ax, x_label):\n",
    "    # Check the Pearson correlation between X1 and Y, X2 and Y, X3 and Y\n",
    "    for i in range(len(ax)):\n",
    "        # corr_Xi_Y, _ = pearsonr(Xs[i].squeeze(), Y[int(i/3)])\n",
    "        corr_Xi_Y, _ = pearsonr(Xs[i].squeeze(), Y)\n",
    "        ax[i].scatter(Xs[i], Y, color=colors.get_color(), label=f'Corr: {corr_Xi_Y:.2f}')\n",
    "        ax[i].set_xlabel(x_label[i],fontsize=16)\n",
    "        ax[i].set_ylabel('Y', fontsize=16,  rotation = 0)\n",
    "        ax[i].set_title(f'Corr: {corr_Xi_Y:.2f}', fontsize=16)\n",
    "        ax[i].set_xticklabels(ax[i].get_xticks(), fontsize=12)\n",
    "        ax[i].set_yticklabels(ax[i].get_yticks(), fontsize=12)\n",
    "# Y = np.round(Y).astype(int)\n",
    "from collections import Counter\n",
    "\n",
    "Xs_uni, Y_uni = get_XY(uniform=True)\n",
    "print('count Y_norm: ', Counter(Y_uni))\n",
    "\n",
    "Xs_norm, Y_norm = get_XY(uniform=False, mix=False)\n",
    "print('count Y_uni: ', Counter(Y_norm))\n",
    "\n",
    "# Plot the linear correlation figures for X1 vs Y, X2 vs Y, X3 vs Y in a single figure with different colors\n",
    "fig, axes = plt.subplots(1, 6, figsize=(18, 3), dpi=100)\n",
    "\n",
    "# Xs_uni.extend(Xs_norm)\n",
    "\n",
    "plot_xy(Xs_uni, Y_uni, axes[:3], ['P1~U(0, 1)', 'P2~U(0, 3)', 'P3~U(0, 4)'])\n",
    "plot_xy(Xs_norm, Y_norm, axes[3:], ['P1~N(0, 1)', 'P2~N(0, 4)', 'P3~N(0, 9)'])\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOW, generate graphs with - $\\color{red}{given\\ properties}$\n",
    "- $\\color{red}{NOTE}$ that, the class is imbalanced.\n",
    "- $\\color{red}{LIST}$ all properties with fixed N: average degree, CC, density, triangles, 4-cycles, 6-cycles, 8-cycles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from scipy.sparse import csr_matrix\n",
    "import random\n",
    "\n",
    "def rewire_given_cc(desired_cc, G, node_num):\n",
    "    # Calculate the number of triangles needed to achieve the desired clustering coefficient\n",
    "    adj = nx.adjacency_matrix(G)\n",
    "    degrees = [i-1 for i in np.sum(adj, axis=1) if i > 1]\n",
    "    total_triads = reduce(lambda x, y:x+y, map(lambda x: x*(x-1)/2, degrees))\n",
    "    desired_triads = int(desired_cc * total_triads)\n",
    "    actual_triads = sum(nx.triangles(G).values()) // 3\n",
    "    triads_to_add = desired_triads - actual_triads\n",
    "    # Add triangles to the graph\n",
    "    while triads_to_add > 0:\n",
    "        u, v = random.sample(G.nodes(), k=2)\n",
    "        if not G.has_edge(u, v) and not G.has_edge(v, u) and not u == v:\n",
    "            neighbors_u = set(G.neighbors(u))\n",
    "            neighbors_v = set(G.neighbors(v))\n",
    "            common_neighbors = neighbors_u.intersection(neighbors_v)\n",
    "            if len(common_neighbors) > 0:\n",
    "                w = random.choice(list(common_neighbors))\n",
    "                G.add_edge(u, v)\n",
    "                G.add_edge(u, w)\n",
    "                G.add_edge(v, w)\n",
    "                triads_to_add -= 1\n",
    "            \n",
    "    return G\n",
    "\n",
    "\n",
    "def add_triangles(G, target_cc, tris_num):\n",
    "    \n",
    "    er_nodes = list(G.nodes)\n",
    "    node_num = len(er_nodes)\n",
    "    \n",
    "    tris = [nx.complete_graph(3) for _ in range(tris_num)]\n",
    "    label_id = node_num\n",
    "    for i, tr in enumerate(tris):\n",
    "        tris[i] = nx.relabel_nodes(tr, {0: label_id, 1: label_id+1, 2: label_id+2})\n",
    "        label_id+=3\n",
    "\n",
    "    for tr in tris:\n",
    "        tr_nodes = list(tr.nodes)\n",
    "        G.add_edge(random.choice(er_nodes), random.choice(tr_nodes))\n",
    "\n",
    "    for tr in tris:\n",
    "        G = nx.compose(G, tr)\n",
    "    \n",
    "    return G\n",
    "\n",
    "\n",
    "def get_Y(ns,  rs = None, is_uniform=True):\n",
    "    \n",
    "    sum_rs = np.sum([r**2 for r in rs])\n",
    "    print('sum_rs:', sum_rs)\n",
    "\n",
    "    scale = np.sqrt(12) if is_uniform else 1\n",
    "    sigma_y = 1\n",
    "\n",
    "    r_y = np.sqrt(1 - np.sum([r**2 for r in rs]))\n",
    "    rs.extend([r_y])\n",
    "\n",
    "        \n",
    "    Y = scale * sigma_y * (reduce(lambda x, y: x+y, map(lambda x: x[0]*x[1], zip(rs, ns))))\n",
    "\n",
    "    Y, _ = z_norm(Y.reshape(-1, 1))\n",
    "    Y = Y.squeeze()\n",
    "    Y = np.round(Y * 10).astype(int)\n",
    "    \n",
    "    return Y\n",
    "\n",
    "# tris_num = 10\n",
    "# tris_num = N*(target_cc - ACC_ER) / (1 - target_cc)\n",
    "\n",
    "er_nodes = 100\n",
    "\n",
    "tris_num = np.arange(1, int(er_nodes/6)) # 15\n",
    "# print('tris_num:  ', tris_num)\n",
    "\n",
    "samples = []\n",
    "for tr_num in tris_num:\n",
    "    G = nx.erdos_renyi_graph(er_nodes - tr_num*3, 0.1 + tr_num*0.01)\n",
    "    samples.append(add_triangles(G, 1, tr_num))\n",
    "\n",
    "avg_degrees = [sum(dict(G.degree()).values()) / len(G) for G in samples]\n",
    "avg_CCs = [nx.average_clustering(G) for G in samples]\n",
    "Ns = [len(G) for G in samples]\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3), dpi=150)\n",
    "axes[0].plot(avg_degrees)\n",
    "axes[0].set_title('avg degree')\n",
    "axes[1].plot(avg_CCs)\n",
    "axes[1].set_title('avg CC')\n",
    "axes[2].plot(Ns)\n",
    "axes[2].set_title('N')\n",
    "\n",
    "# axes[1].plot(check_avg_degree)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from functools import reduce\n",
    "import pickle as pk\n",
    "import os\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset, Data\n",
    "from torch_geometric.utils import from_networkx\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "# fixed node num, fixed average degree, CC ~ U(0.1, 0.5)\n",
    "def z_norm(x_data):\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    return min_max_scaler.fit_transform(x_data), min_max_scaler\n",
    "\n",
    "def mean_norm(x_data):\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    return scaler.fit_transform(x_data)\n",
    "\n",
    "\n",
    "def connect_graphs(g1, g2):\n",
    "    n1 = list(g1.nodes)\n",
    "    n2 = list(g2.nodes)\n",
    "    e1 = random.choices(n1, k=1)[0]\n",
    "    e2 = random.choices(n2, k=1)[0]\n",
    "    g_cur = nx.compose(g1, g2)\n",
    "    g_cur.add_edge(e1, e2)\n",
    "    return g_cur\n",
    "\n",
    "\n",
    "def random_connect_graph(graph_list:list):\n",
    "    # NOTE: relabeling the nodes.\n",
    "    \n",
    "    new_graphs = []\n",
    "    np.random.shuffle(graph_list)\n",
    "    node_idx = 0\n",
    "    for g in graph_list:\n",
    "        len_nodes = len(list(g.nodes))\n",
    "        mapping = {}\n",
    "        for i in range(len_nodes):\n",
    "            mapping[i] = i+node_idx\n",
    "        new_g = nx.relabel_nodes(g, mapping)\n",
    "        new_graphs.append(new_g)\n",
    "        node_idx += len_nodes\n",
    "        \n",
    "    g_all = reduce(connect_graphs, new_graphs)\n",
    "    return g_all\n",
    "\n",
    "\n",
    "def add_square(G, sq_num):\n",
    "     \n",
    "    er_nodes = list(G.nodes)\n",
    "    node_num = len(er_nodes)\n",
    "    \n",
    "    added = [nx.cycle_graph(4) for _ in range(sq_num)]\n",
    "    \n",
    "    # line graphs:\n",
    "    \n",
    "    label_id = node_num\n",
    "    for i, tr in enumerate(added):\n",
    "        added[i] = nx.relabel_nodes(tr, {0: label_id, 1: label_id+1, 2: label_id+2, 3:label_id+3})\n",
    "        label_id+=4\n",
    "\n",
    "    for tr in added:\n",
    "        tr_nodes = list(tr.nodes)\n",
    "        G.add_edge(random.choice(er_nodes), random.choice(tr_nodes))\n",
    "        er_nodes = list(G.nodes)\n",
    "\n",
    "    for tr in added:\n",
    "        G = nx.compose(G, tr)\n",
    "    \n",
    "    return G\n",
    "\n",
    "\n",
    "def add_triangles(G, tris_num, tri=None):\n",
    "    \n",
    "    er_nodes = list(G.nodes)\n",
    "    node_num = len(er_nodes)\n",
    "    \n",
    "    if tri is None:\n",
    "        tris = [nx.complete_graph(3) for _ in range(tris_num)]\n",
    "    else:\n",
    "        # copy tri:\n",
    "        tris = [tri.copy() for _ in range(tris_num)]\n",
    "        \n",
    "    label_id = node_num\n",
    "    for i, tr in enumerate(tris):\n",
    "        tris[i] = nx.relabel_nodes(tr, {0: label_id, 1: label_id+1, 2: label_id+2})\n",
    "        label_id+=3\n",
    "\n",
    "    for tr in tris:\n",
    "        tr_nodes = list(tr.nodes)\n",
    "        G.add_edge(random.choice(er_nodes), random.choice(tr_nodes))\n",
    "\n",
    "    for tr in tris:\n",
    "        G = nx.compose(G, tr)\n",
    "    \n",
    "    return G\n",
    "\n",
    "\n",
    "def get_Y(ns, class_num, rs = None, is_uniform=True):\n",
    "    \n",
    "    sum_rs = np.sum([r**2 for r in rs])\n",
    "    print('sum_rs:', sum_rs)\n",
    "\n",
    "    scale = np.sqrt(12) if is_uniform else 1\n",
    "    sigma_y = 1\n",
    "\n",
    "    r_y = np.sqrt(1 - np.sum([r**2 for r in rs]))\n",
    "    rs.extend([r_y])\n",
    "        \n",
    "    Y = scale * sigma_y * (reduce(lambda x, y: x+y, map(lambda x: x[0]*x[1], zip(rs, ns))))\n",
    "\n",
    "    Y, _ = z_norm(Y.reshape(-1, 1))\n",
    "    Y = Y.squeeze()\n",
    "    Y = np.round(Y * (class_num-1)).astype(int)\n",
    "    \n",
    "    return Y\n",
    "\n",
    "\n",
    "def convert_to_torch_geometric_data(graphs, Y):\n",
    "    data_list = []\n",
    "    for i, graph in enumerate(graphs):\n",
    "        \n",
    "        nx.set_node_attributes(graph, torch.ones(1), 'x')\n",
    "        # nx.set_edge_attributes(graph, torch.randn(graph.number_of_edges(), 1), 'edge_attr')\n",
    "        data = from_networkx(graph)\n",
    "        data.y = torch.tensor([Y[i]], dtype=torch.long)\n",
    "        data_list.append(data)\n",
    "    return data_list\n",
    "\n",
    "\n",
    "class SynDataset(InMemoryDataset):\n",
    "    def __init__(self, data=None, name=None, root=None, transform=None, pre_transform=None):\n",
    "        super(SynDataset, self).__init__(root, transform, pre_transform)\n",
    "        if data is None:\n",
    "            data_path = os.path.join(root, f'syn_{name}.pkl')\n",
    "            with open(data_path, 'rb') as f:\n",
    "                data = pk.load(f)\n",
    "                \n",
    "        self.num_tasks = len({int(i.y.item()) for i in data})\n",
    "        self.data, self.slices = self.collate(data)\n",
    "        self.name = name\n",
    "        self.root = root\n",
    "\n",
    "    def _download(self):\n",
    "        pass\n",
    "\n",
    "    def _process(self):\n",
    "        pass\n",
    "\n",
    "def graph_avg_degree(adj):\n",
    "    if not isinstance(adj, np.ndarray):\n",
    "        adj = adj.todense()\n",
    "    degrees = np.sum(adj, axis=1).reshape(adj.shape[0], 1)\n",
    "    mean_D = np.mean(degrees).astype(np.float32).reshape(1)\n",
    "    return mean_D\n",
    "\n",
    "\n",
    "\n",
    "def dump_ER_graphs_by_CC(sample_num, class_num, rs, er_nodes=100, min_CC=0.1, max_CC=0.5, name=None):\n",
    "    \"\"\"\n",
    "    return labels and graphs.\n",
    "    \"\"\"\n",
    "    ori_cc = np.random.uniform(min_CC, max_CC, size=(sample_num,1))\n",
    "    normed_cc, scaler = z_norm(ori_cc)\n",
    "    \n",
    "    n_cc = np.random.uniform(0, 1, size=(sample_num, 1))\n",
    "    # n_cc = np.random.normal(loc=0, scale=1, size=(sample_num, 1))\n",
    "    \n",
    "    cc = scaler.inverse_transform(n_cc)\n",
    "    \n",
    "    # # NOTE: plot the distribution of CC:\n",
    "    # sort_idx = np.argsort(cc.squeeze())\n",
    "    # plt.figure()\n",
    "    # plt.plot(ori_cc[sort_idx])\n",
    "    # plt.figure()\n",
    "    # plt.plot(cc[sort_idx])\n",
    "    \n",
    "    tris = cc * 30\n",
    "    \n",
    "    samples = []\n",
    "    rand_p = np.random.normal(loc=0, scale=0.05, size=(sample_num,))\n",
    "    rand_node = np.random.normal(loc=0, scale=3, size=(sample_num,))\n",
    "    \n",
    "    cc_cur = []\n",
    "    for i in range(sample_num):\n",
    "        tr_num = int(tris[i])\n",
    "        G = nx.erdos_renyi_graph(er_nodes - tr_num * 3 + abs(int(rand_node[i])), abs(rand_p[i]) - tr_num * 0.01)\n",
    "        new_G = add_triangles(G, tr_num)\n",
    "        cc_cur.append(nx.average_clustering(new_G))\n",
    "        samples.append(new_G)\n",
    "        \n",
    "    # NOTE: check labels:\n",
    "    # new_cc = np.array([nx.average_clustering(g) for g in samples])\n",
    "    # plt.figure()\n",
    "    # plt.plot(new_cc[sort_idx])\n",
    "    \n",
    "    n_y = np.random.uniform(0, 1, size=(sample_num, 1))\n",
    "    Y = get_Y([n_cc, n_y], class_num, rs=rs, is_uniform=True)\n",
    "    print(Y.shape)\n",
    "    print('len samples:', len(samples))\n",
    "    \n",
    "    # if abs(corr_degree) > 0.1, regenerate the samples.\n",
    "    \n",
    "    avg_degree = np.array([graph_avg_degree(nx.to_numpy_matrix(g)) for g in samples])\n",
    "    \n",
    "    \n",
    "    corr_degree, _ = pearsonr(avg_degree.reshape(-1, 1).squeeze(), Y.squeeze())\n",
    "    corr_cc, _ = pearsonr(np.array(cc_cur), Y.squeeze())\n",
    "    # corr_degree = np.corrcoef(Y, avg_degree)[0, 1]\n",
    "    print('corr_cc', corr_cc)\n",
    "    \n",
    "    if abs(corr_degree) >= 0.1:\n",
    "        print('no ok:', corr_degree)\n",
    "        return False\n",
    "    else:\n",
    "        print('corr:', corr_degree)\n",
    "    \n",
    "    # calculate the pearson correlation coefficient of Y and avg_degree:\n",
    "    # Calculate the average degree using the average_degree_connectivity() function\n",
    "    # avg_degree_fix = 2 * np.ones((100,)) + np.random.normal(loc=0, scale=0.1, size=(100,))\n",
    "    # corr_degree_fix = np.corrcoef(Y, avg_degree_fix)[0, 1]\n",
    "    # print('corr_degree: ', corr_degree, corr_degree_fix)\n",
    "    # print('corr new_cc:', np.corrcoef(Y, new_cc)[0, 1])\n",
    "    # plt.figure()\n",
    "    # plt.plot(avg_degree[sort_idx])\n",
    "    # plt.plot(Y[sort_idx])\n",
    "    # plt.title('avg_degree')\n",
    "    \n",
    "    \n",
    "    pyg_data = convert_to_torch_geometric_data(samples, Y)\n",
    "    \n",
    "    root = 'DATA'\n",
    "    name = f'cc_{rs[0]}' if name is None else name\n",
    "    if not os.path.exists(root):\n",
    "        os.mkdir(root)\n",
    "    data_path = os.path.join(root, f'syn_{name}.pkl')\n",
    "    with open(data_path, 'wb') as f:\n",
    "        pk.dump(pyg_data, f)\n",
    "    return True\n",
    "\n",
    "# save datasets\n",
    "import pickle as pk\n",
    "\n",
    "def save_datasets(datasets, file_name):\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pk.dump(datasets, f)\n",
    "\n",
    "def load_datasets(file_name):\n",
    "    with open(file_name, 'rb') as f:\n",
    "        datasets = pk.load(f)\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok = dump_ER_graphs_by_CC(1000, 10, rs=[0.8], er_nodes=100, name='syn_test_cc_0.8')\n",
    "while not ok:\n",
    "    ok = dump_ER_graphs_by_CC(1000, 10, rs=[0.8], er_nodes=100, name='syn_test_cc_0.8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_cc = load_datasets('DATA/syn_test_cc.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $syn_{cc}\\{correlation\\}$: generate datasets: correlation from [0.1, 0.2, ..., 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 10):\n",
    "    correlation = i/10\n",
    "    print(correlation)\n",
    "    ok = dump_ER_graphs_by_CC(4096, 10, rs=[correlation])\n",
    "    while not ok:\n",
    "        ok = dump_ER_graphs_by_CC(4096, 10, rs=[correlation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and check the correlation:\n",
    "\n",
    "d8 = load_datasets('DATA/syn_cc_0.8.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the correaltion of average degree and Y:\n",
    "# transform the pyg data to networkx graph:\n",
    "# import torch_utils\n",
    "\n",
    "import torch_geometric.utils as torch_utils\n",
    "\n",
    "adjs = []\n",
    "\n",
    "for d in d8:\n",
    "    if d.edge_index.numel() < 1:\n",
    "        N = d.x.shape[0]\n",
    "        adj = np.ones(shape=(N, N))\n",
    "    else:\n",
    "        adj = torch_utils.to_dense_adj(d.edge_index).numpy()[0]\n",
    "    adjs.append(adj)\n",
    "\n",
    "\n",
    "\n",
    "avg_d = [graph_avg_degree(adj=adj) for adj in adjs]\n",
    "Y = [d.y.numpy().squeeze() for d in d8]\n",
    "\n",
    "corr_avg_degree, _ = pearsonr(np.array(avg_d).squeeze(), np.array([d.y.numpy().squeeze() for d in d8]).squeeze())\n",
    "                           \n",
    "print(abs(corr_avg_degree)>=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "root = 'DATA'\n",
    "name = 'cc_0.8'\n",
    "dataset = SynDataset(root=root, name=name)\n",
    "print(dataset)\n",
    "\n",
    "# save:\n",
    "\n",
    "# dataset = cc_dataset\n",
    "# train_size = int(0.8 * len(dataset))\n",
    "# val_size = int(0.1 * len(dataset))\n",
    "# test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_bipartite_graph(num_nodes_set1, num_nodes_set2, num_edges):\n",
    "    G = nx.Graph()\n",
    "    nodes_set1 = range(num_nodes_set1)\n",
    "    nodes_set2 = range(num_nodes_set1, num_nodes_set1 + num_nodes_set2)\n",
    "\n",
    "    G.add_nodes_from(nodes_set1, bipartite=0)\n",
    "    G.add_nodes_from(nodes_set2, bipartite=1)\n",
    "\n",
    "    while G.number_of_edges() < num_edges:\n",
    "        u = random.choice(nodes_set1)\n",
    "        v = random.choice(nodes_set2)\n",
    "        if not G.has_edge(u, v):\n",
    "            G.add_edge(u, v)\n",
    "\n",
    "    return G\n",
    "\n",
    "def draw_graph(G):\n",
    "    pos = nx.bipartite_layout(G, [n for n, d in G.nodes(data=True) if d['bipartite'] == 0])\n",
    "    nx.draw(G, pos, with_labels=True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate same CC and different average degree datasets\n",
    "\n",
    "- use bipartite graph to achive high average degree and low 0 CC\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_two_metrics(m1, m2, need_sort=False):\n",
    "    if need_sort:\n",
    "        sorted_xy = sorted(zip(m1, m2), key=lambda x: x[0])\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 3), dpi=100)\n",
    "        axes[0].plot([i[0] for i in sorted_xy])\n",
    "        axes[1].plot([i[1] for i in sorted_xy])\n",
    "        plt.show()\n",
    "    else:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 3), dpi=100) \n",
    "        axes[0].plot(m1)\n",
    "        axes[1].plot(m2)\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "def ER_graphs_by_Degree(sample_num, class_num, node_num=100, min_Degree=3, max_Degree=20):\n",
    "    \"\"\"\n",
    "    return labels and graphs.\n",
    "    \"\"\"\n",
    "    er_nodes = 100\n",
    "    ori_degree = np.random.uniform(min_Degree, max_Degree, size=(sample_num,1))\n",
    "    _, scaler = z_norm(ori_degree)\n",
    "    \n",
    "    n_degree = np.random.uniform(0, 1, size=(sample_num, 1))\n",
    "    dd = scaler.inverse_transform(n_degree)\n",
    "    \n",
    "    samples = []\n",
    "    ori_avg_degrees, ori_cc = [], []\n",
    "    new_avg_degrees, new_cc = [], []\n",
    "    \n",
    "    dd = np.linspace(2, 20, sample_num)\n",
    "    dd = [int(i) for i in dd]\n",
    "    \n",
    "    for i in range(sample_num):\n",
    "        sq_num = int(dd[i])\n",
    "        G = nx.erdos_renyi_graph(er_nodes-2*sq_num, sq_num/(er_nodes-2*sq_num))\n",
    "        \n",
    "        ori_avg_degrees.append(sum(dict(G.degree()).values()) / len(G))\n",
    "        ori_cc.append(nx.average_clustering(G))\n",
    "        \n",
    "        # add bipartite graphs:\n",
    "        \n",
    "        G = generate_bipartite_graph(10, 10, 90)\n",
    "\n",
    "        new_G = add_square(G, int(sq_num/3))\n",
    "        \n",
    "        new_avg_degrees.append(sum(dict(new_G.degree()).values()) / len(new_G))\n",
    "        new_cc.append(nx.average_clustering(new_G))\n",
    "        \n",
    "        # samples.append(new_G)\n",
    "    \n",
    "    # labels:\n",
    "    plot_two_metrics(ori_avg_degrees, ori_cc, need_sort=False)\n",
    "    plot_two_metrics(new_avg_degrees, new_cc, need_sort=False)\n",
    "    \n",
    "    # labels:\n",
    "    \n",
    "    # n_y = np.random.uniform(0, 1, size=(sample_num, 1))\n",
    "    # Y = get_Y([n_cc, n_y], class_num, rs=[0.9], is_uniform=True)\n",
    "    # print(Y.shape)\n",
    "    # print('len samples:', len(samples))\n",
    "    # pyg_data = convert_to_torch_geometric_data(samples, Y)\n",
    "    # dataset = SynDataset(pyg_data, name=\"CC\")\n",
    "    # print(dataset)\n",
    "    \n",
    "    \n",
    "    # plt.figure()\n",
    "    # plt.plot([i[0] for i in sorted_xy])\n",
    "    \n",
    "    # plt.figure()\n",
    "    # plt.plot([i[1] for i in sorted_xy])\n",
    "    # plt.title('CC', fontsize=16)\n",
    "    \n",
    "    # return dataset\n",
    "datasets = ER_graphs_by_Degree(100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_two_metrics(m1, m2, need_sort=False):\n",
    "    if need_sort:\n",
    "        sorted_xy = sorted(zip(m1, m2), key=lambda x: x[0])\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 3), dpi=100)\n",
    "        axes[0].plot([i[0] for i in sorted_xy])\n",
    "        axes[1].plot([i[1] for i in sorted_xy])\n",
    "        plt.show()\n",
    "    else:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 3), dpi=100) \n",
    "        axes[0].plot(m1)\n",
    "        axes[1].plot(m2)\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "def ER_graphs_by_Degree(sample_num, class_num, node_num=100, min_Degree=3, max_Degree=20):\n",
    "    \"\"\"\n",
    "    return labels and graphs.\n",
    "    \"\"\"\n",
    "    er_nodes = node_num\n",
    "    ori_degree = np.random.uniform(min_Degree, max_Degree, size=(sample_num,1))\n",
    "    _, scaler = z_norm(ori_degree)\n",
    "    \n",
    "    n_degree = np.random.uniform(0, 1, size=(sample_num, 1))\n",
    "    dd = scaler.inverse_transform(n_degree)\n",
    "    \n",
    "    ori_avg_degrees, ori_cc = [], []\n",
    "    new_avg_degrees, new_cc = [], []\n",
    "    \n",
    "    p  = np.linspace(0.1, 0.5, sample_num)\n",
    "    \n",
    "    num_bi = np.linspace(0, 3, sample_num)\n",
    "    num_bi_linear = np.linspace(0, 20, sample_num) + np.random.randint(0, 6, size=(sample_num, ))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # make num_bi exponentially increase:\n",
    "    num_bi = np.exp(num_bi)\n",
    "    \n",
    "    bi_G = generate_bipartite_graph(10, 10, 100)\n",
    "    tri = nx.complete_graph(3)\n",
    "    print(num_bi[-1])\n",
    "    \n",
    "    for i in range(sample_num):\n",
    "        G = nx.erdos_renyi_graph(er_nodes - 3 * int(num_bi_linear[sample_num - i-1]), p[i])\n",
    "        \n",
    "        new_G = add_triangles(G, int(num_bi_linear[-i]), tri)\n",
    "        \n",
    "        ori_avg_degrees.append(np.mean(np.sum(nx.to_numpy_matrix(G), axis=1)))\n",
    "        ori_cc.append(nx.average_clustering(G))\n",
    "        \n",
    "        new_avg_degrees.append(np.mean(np.sum(nx.to_numpy_matrix(new_G), axis=1)))\n",
    "        new_cc.append(nx.average_clustering(new_G))\n",
    "        \n",
    "    # labels:\n",
    "    corr, _ = pearsonr(ori_avg_degrees, ori_cc)\n",
    "    corr2, _ = pearsonr(new_avg_degrees, new_cc)\n",
    "    print('correlation of avg_degree and cc:', corr, corr2)\n",
    "    \n",
    "    plot_two_metrics(ori_avg_degrees, ori_cc, need_sort=False)\n",
    "    plot_two_metrics(new_avg_degrees, new_cc, need_sort=False)\n",
    "    \n",
    "datasets = ER_graphs_by_Degree(100, 10, node_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Formally generate correlated graphs:\n",
    "\n",
    "def ER_graphs_by_Degree(sample_num, class_num, rs, er_nodes=100, min_p=0.1, max_p=0.4, name=None):\n",
    "    ori_p = np.random.uniform(min_p, max_p, size=(sample_num, 1))\n",
    "    _, scaler = z_norm(ori_p)\n",
    "    \n",
    "    n_p = np.random.uniform(0, 1, size=(sample_num, 1))\n",
    "    pp = scaler.inverse_transform(n_p)\n",
    "    \n",
    "    ori_avg_degrees, ori_cc = [], []\n",
    "    new_avg_degrees, new_cc = [], []\n",
    "    \n",
    "    num_bi_linear = np.linspace(0, 20, sample_num) + np.random.randint(0, 6, size=(sample_num, ))\n",
    "    \n",
    "    tri = nx.complete_graph(3)\n",
    "    samples = []\n",
    "    for i in range(sample_num):\n",
    "        G = nx.erdos_renyi_graph(er_nodes - 3 * int(num_bi_linear[sample_num - i-1]), pp[i])\n",
    "        \n",
    "        new_G = add_triangles(G, int(num_bi_linear[-i]), tri)\n",
    "        \n",
    "        ori_avg_degrees.append(np.mean(np.sum(nx.to_numpy_matrix(G), axis=1)))\n",
    "        ori_cc.append(nx.average_clustering(G))\n",
    "        \n",
    "        new_avg_degrees.append(np.mean(np.sum(nx.to_numpy_matrix(new_G), axis=1)))\n",
    "        new_cc.append(nx.average_clustering(new_G))\n",
    "        samples.append(new_G)\n",
    "        \n",
    "    # use numpy argsort new_avg_degrees:\n",
    "    sort_idx = np.argsort\n",
    "    # labels:\n",
    "    corr, _ = pearsonr(ori_avg_degrees, ori_cc)\n",
    "    corr2, _ = pearsonr(new_avg_degrees, new_cc)\n",
    "    print('correlation of avg_degree and cc:', corr, corr2)\n",
    "    \n",
    "    plot_two_metrics(ori_avg_degrees, ori_cc, need_sort=False)\n",
    "    plot_two_metrics(new_avg_degrees, new_cc, need_sort=False)\n",
    "    \n",
    "    n_y = np.random.uniform(0, 1, size=(sample_num, 1))\n",
    "    \n",
    "    Y = get_Y([n_p, n_y], class_num, rs=rs, is_uniform=True)\n",
    "    \n",
    "    \n",
    "    print(Y.shape)\n",
    "    print('len samples:', len(samples))\n",
    "    \n",
    "    # if abs(corr_degree) > 0.1, regenerate the samples.\n",
    "    \n",
    "    avg_degree = np.array([graph_avg_degree(nx.to_numpy_matrix(g)) for g in samples])\n",
    "    \n",
    "    \n",
    "    print(avg_degree.shape, Y.squeeze().shape)\n",
    "    \n",
    "    \n",
    "    corr_degree, _ = pearsonr(avg_degree.reshape(-1, 1).squeeze(), Y.squeeze())\n",
    "    corr_cc, _ = pearsonr(np.array(new_cc), Y.squeeze())\n",
    "    \n",
    "    # corr_degree = np.corrcoef(Y, avg_degree)[0, 1]\n",
    "    print('corr_cc', corr_cc)\n",
    "    print('corr_degree', corr_degree)\n",
    "    \n",
    "    # if abs(corr_degree) >= 0.1:\n",
    "    #     print('no ok:', corr_degree)\n",
    "    #     return False\n",
    "    # else:\n",
    "    #     print('corr:', corr_degree)\n",
    "    \n",
    "    \n",
    "    pyg_data = convert_to_torch_geometric_data(samples, Y)\n",
    "    return pyg_data\n",
    "    root = 'DATA'\n",
    "    name = f'cc_{rs[0]}' if name is None else name\n",
    "    # if not os.path.exists(root):\n",
    "    #     os.mkdir(root)\n",
    "    # data_path = os.path.join(root, f'syn_{name}.pkl')\n",
    "    # with open(data_path, 'wb') as f:\n",
    "    #     pk.dump(pyg_data, f)\n",
    "    return pyg_data\n",
    "\n",
    "\n",
    "degree_datasets = ER_graphs_by_Degree(100, 10, rs=[0.8], er_nodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_p = np.random.uniform(0.1, 0.5, size=(100, 1))\n",
    "_, scaler = z_norm(ori_p)\n",
    "\n",
    "n_p = np.random.uniform(0, 1, size=(100, 1))\n",
    "pp = scaler.inverse_transform(n_p)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(ori_p)\n",
    "plt.plot(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_uniform = np.random.normal(1, 10, size=1000)\n",
    "\n",
    "std =  np.std(cc_uniform)\n",
    "print('std1: ', std)\n",
    "print(\"cc_uniform: \", cc_uniform.shape)\n",
    "\n",
    "cc, scaler = z_norm(cc_uniform.reshape(-1, 1))\n",
    "cc = cc*15\n",
    "\n",
    "new_cc = np.random.normal(0, 1, size=1000)\n",
    "# std=  np.std(cc)\n",
    "\n",
    "old_cc = new_cc * std\n",
    "# old_cc = scaler.inverse_transform(new_cc.reshape(-1, 1))\n",
    "\n",
    "# NOTE: for normal and uniform, the recover approach is not different !!!!!!\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(cc_uniform)\n",
    "plt.figure()\n",
    "plt.plot(old_cc)\n",
    "plt.legend(\"me\", loc=\"upper center\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import beta\n",
    "\n",
    "# Beta distribution parameters\n",
    "params = [\n",
    "    (1, 1),  # Uniform distribution\n",
    "    (2, 2),  # Symmetric U-shaped distribution\n",
    "    (3, 3),  # Symmetric bell-shaped distribution\n",
    "    (2, 5),  # Right-skewed distribution\n",
    "    (5, 2),  # Left-skewed distribution\n",
    "    (0.5, 0.5),  # Bimodal distribution\n",
    "    (10, 1),  # Distribution concentrated near 1\n",
    "    (1, 10)  # Distribution concentrated near 0\n",
    "]\n",
    "\n",
    "# Number of samples\n",
    "n_samples = 1000\n",
    "\n",
    "# Create a 4x2 grid of subplots\n",
    "fig, axes = plt.subplots(4, 2, figsize=(12, 16))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (a, b) in enumerate(params):\n",
    "    # Generate samples from the Beta distribution\n",
    "    samples = beta.rvs(a, b, size=n_samples)\n",
    "\n",
    "    # Plot histogram\n",
    "    axes[i].hist(samples, bins=50, density=True, alpha=0.7)\n",
    "    axes[i].set_title(f'α={a}, β={b}')\n",
    "    axes[i].set_xlabel('x')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
